,label,text,text_clean,Badre AN,Ramamurthy K,John BE,Sahoo RK,Moreira JE,Huo M,Saint-Paul R,Bateman S,McCalla G,Pechenizkiy M,Rozinat A,Raghunathan V,Aalst WM,Perimal-Lewis L,Hatala M,Oliveira TC,e Abreu FB,Azevedo LG,Camargo V,Krogen ET,Yang S,Sun L,Xiong H,Marsic I,Heinimann H,Wiesmann D,Wei CP,Faber NR,Wombacher A,Ghannouchi SA,Lamine E,Montani S,Chebel-Morello B,Tax N,Wu B,Luo B,Epure EV,Espen Ingvaldsen J,Hayase T,Laga N,Masse P,Mendy G,Hicheur Cairns A,Saoud NB,Gunther CW,Wang J,Czarnecki K,Arora C,Esfahani FS,Sulaiman MN,Udzir NI,Renard M,Beillard B,Lalande M,Ren F,Wijesekera D,Rosen D,Wickboldt JA,Gaspary LP,Trastour D,Kramer W,Cho M,Krathu W,Hug C,Deneckére R,Brinkkemper S,Ghose A,Narendra NC,Yan J,Han JS,Song YJ,publication_date_2010,publication_date_2015,publication_date_2016
0,0,"Automatic Chunk Detection in Human-computer Interaction This paper describes an algorithm to detect user's mental chunks by analysis of pause lengths in goal-directed human-computer interaction. Identifying and characterizing users' chunks can help in gauging the users' level of expertise. The algorithm described in this paper works with information collected by an automatic logging mechanism. Therefore, it is applicable to situations in which no human intervention is required to perform the analysis, such as adaptive interfaces. An empirical study was conducted to validate the algorithm, showing that mental chunks and their characteristics can indeed be inferred from analysis of human-computer interaction logs. Users performing a variety of goal-directed tasks were monitored. Using an automated logging tool, every command invoked, every operation performed with the input devices, as well as all system responses were recorded. Analysis of the interaction logs was performed by a program that implements a chunk detection algorithm that looks at command sequences and timings. The results support the hypothesis that a significant number of user mental chunks can be detected by our algorithm. chunk detection, chunking, event logging, human-computer interaction, models of the user, novice expert differences, user study",automatic chunk detection in human computer interaction this paper describes an algorithm to detect user s mental chunks by analysis of pause lengths in goal directed human computer interaction identifying and characterizing users chunks can help in gauging the users level of expertise the algorithm described in this paper works with information collected by an automatic logging mechanism therefore it is applicable to situations in which no human intervention is required to perform the analysis such as adaptive interfaces an empirical study was conducted to validate the algorithm showing that mental chunks and their characteristics can indeed be inferred from analysis of human computer interaction logs users performing a variety of goal directed tasks were monitored using an automated logging tool every command invoked every operation performed with the input devices as well as all system responses were recorded analysis of the interaction logs was performed by a program that implements a chunk detection algorithm that looks at command sequences and timings the results support the hypothesis that a significant number of user mental chunks can be detected by our algorithm chunk detection chunking event logging human computer interaction models of the user novice expert differences user study,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
1,0,"The Design of RPM: An FPGA-based Multiprocessor Emulator Recent advances in Field-Programmable Gate Arrays (FPGA) and programmable interconnects have made it possible to build efficient hardware emulation engines. In addition, improvements in Computer-Aided Design (CAD) tools, mainly in synthesis tools, greatly simplify the design of large circuits. The RPM (Rapid Prototype Engine for Multiprocessors) Project leverages these two technological advances. Its goal is to develop a common hardware platform for the emulation of multiprocessor systems with different architectures. For cost reasons, the use of FPGAs in RPM is limited to the memory controllers, while the rest of the emulator, including the processors, memories and interconnect, is built with off-the-shelf components. A flexible non-intrusive event logging mechanism is included at all levels of the memory hierarchy, making it possible to monitor the emulation in very fine detail. This paper presents the hardware design of RPM. field-programmable gate arrays, logic emulation, message-passing multicomputers, rapid prototyping, shared-memory multiprocessors",the design of rpm an fpga based multiprocessor emulator recent advances in field programmable gate arrays fpga and programmable interconnects have made it possible to build efficient hardware emulation engines in addition improvements in computer aided design cad tools mainly in synthesis tools greatly simplify the design of large circuits the rpm rapid prototype engine for multiprocessors project leverages these two technological advances its goal is to develop a common hardware platform for the emulation of multiprocessor systems with different architectures for cost reasons the use of fpgas in rpm is limited to the memory controllers while the rest of the emulator including the processors memories and interconnect is built with off the shelf components a flexible non intrusive event logging mechanism is included at all levels of the memory hierarchy making it possible to monitor the emulation in very fine detail this paper presents the hardware design of rpm field programmable gate arrays logic emulation message passing multicomputers rapid prototyping shared memory multiprocessors,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
2,0,"A Tool for Creating Predictive Performance Models from User Interface Demonstrations A central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality. In the last 15 years significant advances in this regard have been achieved. However, the evaluation portion of the iterative design process has received relatively little support from tools. Even though advances have also been made in usability evaluation methods, nearly all evaluation is still done ``by hand,'' making it more expensive and difficult than it might be. This paper considers a partial implementation of the CRITIQUE usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks. This paper will consider techniques used by the system to produce predictive models (keystroke level models and simplified GOMS models) from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods. A preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions (within 2%). Further, because they are automated, these models promise to be less subject to human error and less affected by the skill of the modeler. GOMS, event logs, task modeling, tool support for evaluation, toolkits",a tool for creating predictive performance models from user interface demonstrations a central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality in the last 15 years significant advances in this regard have been achieved however the evaluation portion of the iterative design process has received relatively little support from tools even though advances have also been made in usability evaluation methods nearly all evaluation is still done by hand making it more expensive and difficult than it might be this paper considers a partial implementation of the critique usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks this paper will consider techniques used by the system to produce predictive models keystroke level models and simplified goms models from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods a preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions within 2 further because they are automated these models promise to be less subject to human error and less affected by the skill of the modeler goms event logs task modeling tool support for evaluation toolkits,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
3,0,"Critical Event Prediction for Proactive Management in Large-scale Computer Clusters As the complexity of distributed computing systems increases, systems management tasks require significantly higher levels of automation; examples include diagnosis and prediction based on real-time streams of computer events, setting alarms, and performing continuous monitoring. The core of autonomic computing, a recently proposed initiative towards next-generation IT-systems capable of 'self-healing', is the ability to analyze data in real-time and to predict potential problems. The goal is to avoid catastrophic failures through prompt execution of remedial actions.This paper describes an attempt to build a proactive prediction and control system for large clusters. We collected event logs containing various system reliability, availability and serviceability (RAS) events, and system activity reports (SARs) from a 350-node cluster system for a period of one year. The 'raw' system health measurements contain a great deal of redundant event data, which is either repetitive in nature or misaligned with respect to time. We applied a filtering technique and modeled the data into a set of primary and derived variables. These variables used probabilistic networks for establishing event correlations through prediction algorithms. We also evaluated the role of time-series methods, rule-based classification algorithms and Bayesian network models in event prediction.Based on historical data, our results suggest that it is feasible to predict system performance parameters (SARs) with a high degree of accuracy using time-series models. Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70% accuracy. critical event prediction, large-scale clusters, system event log",critical event prediction for proactive management in large scale computer clusters as the complexity of distributed computing systems increases systems management tasks require significantly higher levels of automation examples include diagnosis and prediction based on real time streams of computer events setting alarms and performing continuous monitoring the core of autonomic computing a recently proposed initiative towards next generation it systems capable of self healing is the ability to analyze data in real time and to predict potential problems the goal is to avoid catastrophic failures through prompt execution of remedial actions this paper describes an attempt to build a proactive prediction and control system for large clusters we collected event logs containing various system reliability availability and serviceability ras events and system activity reports sars from a 350 node cluster system for a period of one year the raw system health measurements contain a great deal of redundant event data which is either repetitive in nature or misaligned with respect to time we applied a filtering technique and modeled the data into a set of primary and derived variables these variables used probabilistic networks for establishing event correlations through prediction algorithms we also evaluated the role of time series methods rule based classification algorithms and bayesian network models in event prediction based on historical data our results suggest that it is feasible to predict system performance parameters sars with a high degree of accuracy using time series models rule based classification techniques can be used to extract machine event signatures to predict critical events with up to 70 accuracy critical event prediction large scale clusters system event log,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
4,0,"IP Fault Localization via Risk Modeling Automated, rapid, and effective fault management is a central goal of large operational IP networks. Today's networks suffer from a wide and volatile set of failure modes, where the underlying fault proves difficult to detect and localize, thereby delaying repair. One of the main challenges stems from operational reality: IP routing and the underlying optical fiber plant are typically described by disparate data models and housed in distinct network management systems. We introduce a fault-localization methodology based on the use of risk models and an associated troubleshooting system, SCORE (Spatial Correlation Engine), which automatically identifies likely root causes across layers. In particular, we apply SCORE to the problem of localizing link failures in IP and optical networks. In experiments conducted on a tier-1 ISP backbone, SCORE proved remarkably effective at localizing optical link failures using only IP-layer event logs. Moreover, SCORE was often able to automatically uncover inconsistencies in the databases that maintain the critical associations between the IP and optical networks. []",ip fault localization via risk modeling automated rapid and effective fault management is a central goal of large operational ip networks today s networks suffer from a wide and volatile set of failure modes where the underlying fault proves difficult to detect and localize thereby delaying repair one of the main challenges stems from operational reality ip routing and the underlying optical fiber plant are typically described by disparate data models and housed in distinct network management systems we introduce a fault localization methodology based on the use of risk models and an associated troubleshooting system score spatial correlation engine which automatically identifies likely root causes across layers in particular we apply score to the problem of localizing link failures in ip and optical networks in experiments conducted on a tier 1 isp backbone score proved remarkably effective at localizing optical link failures using only ip layer event logs moreover score was often able to automatically uncover inconsistencies in the databases that maintain the critical associations between the ip and optical networks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
5,0,"An Exploratory Study of Process Enactment As Input to Software Process Improvement Software process improvement has been a focus of industry for many years. To assist the procedure and implementation of software process improvement we provide a software process recovery method based on mining project enactment data. The goal of process recovery is to improve the quality of a planned software process. We investigate the enactment of a planned software process from the view of understanding the appropriateness and fitness for purpose of the process model from the viewpoint of the project managers in the context of a small software development organization. We collected empirical data from this organization and then applied our method to a pilot case study. The main contribution of our work is to provide a methodology of software process model recovery which supports software process improvement. process mining, software process improvement",an exploratory study of process enactment as input to software process improvement software process improvement has been a focus of industry for many years to assist the procedure and implementation of software process improvement we provide a software process recovery method based on mining project enactment data the goal of process recovery is to improve the quality of a planned software process we investigate the enactment of a planned software process from the view of understanding the appropriateness and fitness for purpose of the process model from the viewpoint of the project managers in the context of a small software development organization we collected empirical data from this organization and then applied our method to a pilot case study the main contribution of our work is to provide a methodology of software process model recovery which supports software process improvement process mining software process improvement,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
6,0,"Enforcing Object Protocols by Combining Static and Runtime Analysis In this paper, we consider object protocols that constrain interactions between objects in a program. Several such protocols have been proposed in the literature. For many APIs (such as JDOM, JDBC), API designers constrain how API clients interact with API objects. In practice, API clients violate such constraints, as evidenced by postings in discussion forums for these APIs. Thus, it is important that API designers specify constraints using appropriate object protocols and enforce them. The goal of an object protocol is expressed as a protocol invariant. Fundamental properties such as ownership can be expressed as protocol invariants. We present a language, PROLANG, to specify object protocols along with their protocol invariants, and a tool, INVCOP++, to check if a program satisfies a protocol invariant. INVCOP++ separates the problem of checking if a protocol satisfies its protocol invariant (called protocol correctness), from the problem of checking if a program conforms to a protocol (called program conformance). The former is solved using static analysis, and the latter using runtime analysis. Due to this separation (1) errors made in protocol design are detected at a higher level of abstraction, independent of the program's source code, and (2) performance of conformance checking is improved as protocol correctness has been verified statically. We present theoretical guarantees about the way we combine static and runtime analysis, and empirical evidence that our tool INVCOP++ finds usage errors in widely used APIs. We also show that statically checking protocol correctness greatly optimizes the overhead of checking program conformance, thus enabling API clients to test whether their programs use the API as intended by the API designer. aspect oriented programming, invariants, program verification",enforcing object protocols by combining static and runtime analysis in this paper we consider object protocols that constrain interactions between objects in a program several such protocols have been proposed in the literature for many apis such as jdom jdbc api designers constrain how api clients interact with api objects in practice api clients violate such constraints as evidenced by postings in discussion forums for these apis thus it is important that api designers specify constraints using appropriate object protocols and enforce them the goal of an object protocol is expressed as a protocol invariant fundamental properties such as ownership can be expressed as protocol invariants we present a language prolang to specify object protocols along with their protocol invariants and a tool invcop to check if a program satisfies a protocol invariant invcop separates the problem of checking if a protocol satisfies its protocol invariant called protocol correctness from the problem of checking if a program conforms to a protocol called program conformance the former is solved using static analysis and the latter using runtime analysis due to this separation 1 errors made in protocol design are detected at a higher level of abstraction independent of the program s source code and 2 performance of conformance checking is improved as protocol correctness has been verified statically we present theoretical guarantees about the way we combine static and runtime analysis and empirical evidence that our tool invcop finds usage errors in widely used apis we also show that statically checking protocol correctness greatly optimizes the overhead of checking program conformance thus enabling api clients to test whether their programs use the api as intended by the api designer aspect oriented programming invariants program verification,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
7,0,"Process Spaceship: Discovering and Exploring Process Views from Event Logs in Data Spaces Business processes (BPs) are central to the operation of both public and private organizations. A business process is a set of coordinated tasks and activities to achieve a business objective or goal. Given the importance of BPs to overall efficiency and effectiveness, the competitiveness of organizations hinges on continuous BP improvement. In the nineties, the focus of BP improvement was on automation: workflow management systems (WfMSs) and other middleware technologies were used to reduce cost and improve efficiency by providing better system integration and automated enactment of operational business processes. Recently, the focus of business process has expanded to monitoring, analysis and understanding of business processes, and such techniques are incorporated in business process management systems (BPMSs). []",process spaceship discovering and exploring process views from event logs in data spaces business processes bps are central to the operation of both public and private organizations a business process is a set of coordinated tasks and activities to achieve a business objective or goal given the importance of bps to overall efficiency and effectiveness the competitiveness of organizations hinges on continuous bp improvement in the nineties the focus of bp improvement was on automation workflow management systems wfmss and other middleware technologies were used to reduce cost and improve efficiency by providing better system integration and automated enactment of operational business processes recently the focus of business process has expanded to monitoring analysis and understanding of business processes and such techniques are incorporated in business process management systems bpmss,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
8,0,"Interactive Usability Instrumentation Usage data logged from user interactions can be extremely valuable for evaluating software usability. However, instrumenting software to collect usage data is a time-intensive task that often requires technical expertise as well as an understanding of the usability issues to be explored. We have developed a new technique for software instrumentation that removes the need for programming. Interactive Usability Instrumentation (IUI) allows usability evaluators to work directly with a system's interface to specify what components and what events should be logged. Evaluators are able to create higher-level abstractions on the events they log and are provided with real-time feedback on how events are logged. As a proof of the IUI concept, we have created the UMARA system, an instrumentation system that is enabled by recent advances in aspect-oriented programming. UMARA allows users to instrument software without the need for additional coding, and provides tools for specification, data collection, and data analysis. We report on the use of UMARA in the instrumentation of two large open-source projects; our experiences show that IUI can substantially simplify the process of log-based usability evaluation. aspect-oriented programming, instrumentation, software logging, usability",interactive usability instrumentation usage data logged from user interactions can be extremely valuable for evaluating software usability however instrumenting software to collect usage data is a time intensive task that often requires technical expertise as well as an understanding of the usability issues to be explored we have developed a new technique for software instrumentation that removes the need for programming interactive usability instrumentation iui allows usability evaluators to work directly with a system s interface to specify what components and what events should be logged evaluators are able to create higher level abstractions on the events they log and are provided with real time feedback on how events are logged as a proof of the iui concept we have created the umara system an instrumentation system that is enabled by recent advances in aspect oriented programming umara allows users to instrument software without the need for additional coding and provides tools for specification data collection and data analysis we report on the use of umara in the instrumentation of two large open source projects our experiences show that iui can substantially simplify the process of log based usability evaluation aspect oriented programming instrumentation software logging usability,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
9,0,"Using Minimum Description Length for Process Mining In the field of process mining, the goal is to automatically extract process models from event logs. Recently, many algorithms have been proposed for this task. For comparing these models, different quality measures have been proposed. Most of these measures, however, have several disadvantages; they are model-dependent, assume that the model that generated the log is known, or need negative examples of event sequences. In this paper we propose a new measure, based on the minimal description length principle, to evaluate the quality of process models that does not have these disadvantages. To illustrate the properties of the new measure we conduct experiments and discuss the trade-off between model complexity and compression. []",using minimum description length for process mining in the field of process mining the goal is to automatically extract process models from event logs recently many algorithms have been proposed for this task for comparing these models different quality measures have been proposed most of these measures however have several disadvantages they are model dependent assume that the model that generated the log is known or need negative examples of event sequences in this paper we propose a new measure based on the minimal description length principle to evaluate the quality of process models that does not have these disadvantages to illustrate the properties of the new measure we conduct experiments and discuss the trade off between model complexity and compression,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
10,0,"A Data Lineage Model for Distributed Sub-image Processing An important challenge facing e-Science is the development of scalable systems and analysis techniques that allow client applications to locate data and services in increasingly large-scale distributed environments. e-Science Systems should achieve three main goals: (i) efficient and selective processing of data, (ii) support network collaboration without clogging distribution networks; and (iii) allow transparency of experiments through repeatability and verifiability of experiments. Several systems have addressed limited combinations of these properties, but we address all three in this work. We describe the architecture and implementation of such a framework in Astro-WISE, an astronomical approach to distributed data processing, discovery and retrieval of datasets that achieves scalability via dynamic linking (data lineage) maintained within the system. We show that lineage data collected during the processing and analysis of datasets can be reused to perform selective reprocessing(at sub-image level)ondatasets while the remainder of the dataset is untouched, a rather difficult process to automate without lineage. data lineage, data reduction, provenance, scientific computing, subimage processing, target processing",a data lineage model for distributed sub image processing an important challenge facing e science is the development of scalable systems and analysis techniques that allow client applications to locate data and services in increasingly large scale distributed environments e science systems should achieve three main goals i efficient and selective processing of data ii support network collaboration without clogging distribution networks and iii allow transparency of experiments through repeatability and verifiability of experiments several systems have addressed limited combinations of these properties but we address all three in this work we describe the architecture and implementation of such a framework in astro wise an astronomical approach to distributed data processing discovery and retrieval of datasets that achieves scalability via dynamic linking data lineage maintained within the system we show that lineage data collected during the processing and analysis of datasets can be reused to perform selective reprocessing at sub image level ondatasets while the remainder of the dataset is untouched a rather difficult process to automate without lineage data lineage data reduction provenance scientific computing subimage processing target processing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0,0.0,0.0
11,0,"Aveksha: A Hardware-software Approach for Non-intrusive Tracing and Profiling of Wireless Embedded Systems It is important to get an idea of the events occurring in an embedded wireless node when it is deployed in the field, away from the convenience of an interactive debugger. Such visibility can be useful for post-deployment testing, replay-based debugging, and for performance and energy profiling of various software components. Prior software-based solutions to address this problem have incurred high execution overhead and intrusiveness. The intrusiveness changes the intrinsic timing behavior of the application, thereby reducing the fidelity of the collected profile. Prior hardware-based solutions have involved the use of dedicated ASICs or other tightly coupled changes to the embedded node's processor, which significantly limits their applicability. In this paper, we present Aveksha, a hardware-software approach for achieving the above goals in a non-intrusive manner. Our approach is based on the key insight that most embedded processors have an on-chip debug module (which has traditionally been used for interactive debugging) that provides significant visibility into the internal state of the processor. We design a debug board that interfaces with the on-chip debug module of an embedded node's processor through the JTAG port and provides three modes of event logging and tracing: breakpoint, watchpoint, and program counter polling. Using expressive triggers that the on-chip debug module supports, Aveksha can watch for, and record, a variety of programmable events of interest. A key feature of Aveksha is that the target processor does not have to be stopped during event logging (in the last two of the three modes), subject to a limit on the rate at which logged events occur. Aveksha also performs power monitoring of the embedded wireless node and, importantly, enables power consumption data to be correlated to events of interest. Aveksha is an operating system-agnostic solution. We demonstrate its functionality and performance using three applications running on Telos motes; two in TinyOS and one in Contiki. We show that Aveksha can trace tasks and other generic events at the function and task-level granularity. We also describe how we used Aveksha to find a subtle bug in the TinyOS low power listening protocol. JTAG, debugging, tracing, wireless sensor network",aveksha a hardware software approach for non intrusive tracing and profiling of wireless embedded systems it is important to get an idea of the events occurring in an embedded wireless node when it is deployed in the field away from the convenience of an interactive debugger such visibility can be useful for post deployment testing replay based debugging and for performance and energy profiling of various software components prior software based solutions to address this problem have incurred high execution overhead and intrusiveness the intrusiveness changes the intrinsic timing behavior of the application thereby reducing the fidelity of the collected profile prior hardware based solutions have involved the use of dedicated asics or other tightly coupled changes to the embedded node s processor which significantly limits their applicability in this paper we present aveksha a hardware software approach for achieving the above goals in a non intrusive manner our approach is based on the key insight that most embedded processors have an on chip debug module which has traditionally been used for interactive debugging that provides significant visibility into the internal state of the processor we design a debug board that interfaces with the on chip debug module of an embedded node s processor through the jtag port and provides three modes of event logging and tracing breakpoint watchpoint and program counter polling using expressive triggers that the on chip debug module supports aveksha can watch for and record a variety of programmable events of interest a key feature of aveksha is that the target processor does not have to be stopped during event logging in the last two of the three modes subject to a limit on the rate at which logged events occur aveksha also performs power monitoring of the embedded wireless node and importantly enables power consumption data to be correlated to events of interest aveksha is an operating system agnostic solution we demonstrate its functionality and performance using three applications running on telos motes two in tinyos and one in contiki we show that aveksha can trace tasks and other generic events at the function and task level granularity we also describe how we used aveksha to find a subtle bug in the tinyos low power listening protocol jtag debugging tracing wireless sensor network,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
12,0,"Mining Service Integration Opportunities Towards Joined-up Government Service integration is central to joined-up government initiatives and requires information on the collaborators and the services they offer, roles of different actors, the resources required, and their goals (individual and shared). These information are largely available in unstructured forms on government portals, publications and other textural sources. This paper explores semantic text mining for extracting service-related information from such sources using Natural Language Processing techniques supported by Service-Oriented Process Ontologies. Our solution framework consists of the following steps: (1) creating domain and service-oriented process ontology, (2) extracting service-related information from textual sources based on the ontology, and finally (3) mining relationship among the services based on the extracted information in Step 2 linked with a pre-defined hierarchy of service delivery goals specifying the objective(s) to be achieved among the orchestrated services. We describe our approach to these tasks and discuss the progress of the work, our experiences and the challenges encountered so far. collaborative networks, data mining, information extraction, joined-up government, process mining, service integration, text mining, transformational government",mining service integration opportunities towards joined up government service integration is central to joined up government initiatives and requires information on the collaborators and the services they offer roles of different actors the resources required and their goals individual and shared these information are largely available in unstructured forms on government portals publications and other textural sources this paper explores semantic text mining for extracting service related information from such sources using natural language processing techniques supported by service oriented process ontologies our solution framework consists of the following steps 1 creating domain and service oriented process ontology 2 extracting service related information from textual sources based on the ontology and finally 3 mining relationship among the services based on the extracted information in step 2 linked with a pre defined hierarchy of service delivery goals specifying the objective s to be achieved among the orchestrated services we describe our approach to these tasks and discuss the progress of the work our experiences and the challenges encountered so far collaborative networks data mining information extraction joined up government process mining service integration text mining transformational government,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
13,0,"Performance Modeling in Mapreduce Environments: Challenges and Opportunities Unstructured data is the largest and fastest growing portion of most enterprise's assets, often representing 70% to 80% of online data. These steep increase in volume of information being produced often exceeds the capabilities of existing commercial databases. MapReduce and its open-source implementation Hadoop represent an economically compelling alternative that offers an efficient distributed computing platform for handling large volumes of data and mining petabytes of unstructured information. It is increasingly being used across the enterprise for advanced data analytics, business intelligence, and enabling new applications associated with data retention, regulatory compliance, e-discovery and litigation issues. However, setting up a dedicated Hadoop cluster requires a significant capital expenditure that can be difficult to justify. Cloud computing offers a compelling alternative and allows users to rent resources in a ""pay-as-you-go"" fashion. For example, a list of offered Amazon Web Services includes MapReduce environment for rent. It is an attractive and cost-efficient option for many users because acquiring and maintaining complex, large-scale infrastructures is a difficult and expensive decision. One of the open questions in such environments is the amount of resources that a user should lease from the service provider. Currently, there is no available methodology to easily answer this question, and the task of estimating required resources to meet application performance goals is the solely user's responsibility. The users need to perform adequate application testing, performance evaluation, capacity planning estimation, and then request appropriate amount of resources from the service provider. To address these problems we need to understand: ""What do we need to know about a MapReduce job for building an efficient and accurate modeling framework? Can we extract a representative job profile that reflects a set of critical performance characteristics of the underlying application during all job execution phases, i.e., map, shuffle, sort and reduce phases? What metrics should be included in the job profile?"" We discuss a profiling technique for MapReduce applications that aims to construct a compact job profile that is comprised of performance invariants which are independent of the amount of resources assigned to the job (i.e., the size of the Hadoop cluster) and the size of the input dataset. The challenge is how to accurately predict application performance in the large production environment and for processing large datasets from the application executions that being run in the smaller staging environment and that process smaller input datasets. One of the major Hadoop benefits is its ability of dealing with failures (disk, processes, node failures) and allowing the user job to complete. The performance implications of failures depend on their types, when do they happen, and whether a system can offer some spare resources instead of failed ones to the running jobs. We discuss how to enhance the MapReduce performance model for evaluating the failure impact on job completion time and predicting a potential performance degradation. Sharing a MapReduce cluster among multiple applications is a common practice in such environments. However, a key challenge in these shared environments is the ability to tailor and control resource allocations to different applications for achieving their performance goals and service level objectives (SLOs). Currently, there is no job scheduler for MapReduce environments that given a job completion deadline, could allocate the appropriate amount of resources to the job so that it meets the required SLO. In MapReduce environments, many production jobs are run periodically on new data. For example, Facebook, Yahoo!, and eBay process terabytes of data and event logs per day on their Hadoop clusters for spam detection, business intelligence and different types of optimization. For the production jobs that are routinely executed on the new datasets, can we build on-line job profiles that later are used for resource allocation and performance management by the job scheduler? Wediscuss opportunities and challenges for building the SLO-based Hadoop scheduler. The accuracy of new performance models might depend on the resource contention, especially, the network contention in the production Hadoop cluster. Typically, service providerstend to over provision network resources to avoid undesirable side effects of network contention. At the same time, it is an interesting modeling question whether such a network contention factor can be introduced, measured, and incorporated in theMapReduce performance model. Benchmarking Hadoop, optimizing cluster parameter settings, designing job schedulers with different performance objectives, and constructing intelligent workload management in shared Hadoop clusters create an exciting list of challenges and opportunities for the performance analysis and modeling in MapReduce environments. capacity planning, job scheduling, mapreduce, measurements, resource allocation, workload profiling",performance modeling in mapreduce environments challenges and opportunities unstructured data is the largest and fastest growing portion of most enterprise s assets often representing 70 to 80 of online data these steep increase in volume of information being produced often exceeds the capabilities of existing commercial databases mapreduce and its open source implementation hadoop represent an economically compelling alternative that offers an efficient distributed computing platform for handling large volumes of data and mining petabytes of unstructured information it is increasingly being used across the enterprise for advanced data analytics business intelligence and enabling new applications associated with data retention regulatory compliance e discovery and litigation issues however setting up a dedicated hadoop cluster requires a significant capital expenditure that can be difficult to justify cloud computing offers a compelling alternative and allows users to rent resources in a pay as you go fashion for example a list of offered amazon web services includes mapreduce environment for rent it is an attractive and cost efficient option for many users because acquiring and maintaining complex large scale infrastructures is a difficult and expensive decision one of the open questions in such environments is the amount of resources that a user should lease from the service provider currently there is no available methodology to easily answer this question and the task of estimating required resources to meet application performance goals is the solely user s responsibility the users need to perform adequate application testing performance evaluation capacity planning estimation and then request appropriate amount of resources from the service provider to address these problems we need to understand what do we need to know about a mapreduce job for building an efficient and accurate modeling framework can we extract a representative job profile that reflects a set of critical performance characteristics of the underlying application during all job execution phases i e map shuffle sort and reduce phases what metrics should be included in the job profile we discuss a profiling technique for mapreduce applications that aims to construct a compact job profile that is comprised of performance invariants which are independent of the amount of resources assigned to the job i e the size of the hadoop cluster and the size of the input dataset the challenge is how to accurately predict application performance in the large production environment and for processing large datasets from the application executions that being run in the smaller staging environment and that process smaller input datasets one of the major hadoop benefits is its ability of dealing with failures disk processes node failures and allowing the user job to complete the performance implications of failures depend on their types when do they happen and whether a system can offer some spare resources instead of failed ones to the running jobs we discuss how to enhance the mapreduce performance model for evaluating the failure impact on job completion time and predicting a potential performance degradation sharing a mapreduce cluster among multiple applications is a common practice in such environments however a key challenge in these shared environments is the ability to tailor and control resource allocations to different applications for achieving their performance goals and service level objectives slos currently there is no job scheduler for mapreduce environments that given a job completion deadline could allocate the appropriate amount of resources to the job so that it meets the required slo in mapreduce environments many production jobs are run periodically on new data for example facebook yahoo and ebay process terabytes of data and event logs per day on their hadoop clusters for spam detection business intelligence and different types of optimization for the production jobs that are routinely executed on the new datasets can we build on line job profiles that later are used for resource allocation and performance management by the job scheduler wediscuss opportunities and challenges for building the slo based hadoop scheduler the accuracy of new performance models might depend on the resource contention especially the network contention in the production hadoop cluster typically service providerstend to over provision network resources to avoid undesirable side effects of network contention at the same time it is an interesting modeling question whether such a network contention factor can be introduced measured and incorporated in themapreduce performance model benchmarking hadoop optimizing cluster parameter settings designing job schedulers with different performance objectives and constructing intelligent workload management in shared hadoop clusters create an exciting list of challenges and opportunities for the performance analysis and modeling in mapreduce environments capacity planning job scheduling mapreduce measurements resource allocation workload profiling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
14,0,"RanKloud: Scalable Multimedia and Social Media Retrieval and Analysis in the Cloud Today, multimedia data are produced in massive quantities, thanks to a diverse spectrum of applications including entertainment, surveillance, e-commerce, web, and social media. In particular, social media data have three challenging characteristics: data sizes are enormous, data are often multi-faceted, and data are dynamic. Tensors (multi-dimensional arrays) are widely used for representing such high-order dimensional data. Consequently, a system dealing with social media data needs to scale with the tensor volume and the number and diversity of the data facets. This necessitates highly parallelizable, and in many cases cloud-based, frameworks for scalable processing and efficient analysis of large media and social media collections. Most multimedia applications share a few core operations, including integration/fusion, classification, clustering, graph analysis, near-neighbor search, and similarity search. When performed naively, however, these core operations are often very costly, because the number of objects and object features that need to be considered can be prohibitive. Avoiding this cost requires that redundant work is avoided. Thus, for the next generation cloud-based massive media processing and analysis systems to have transformative impact, the fundamental principles that govern their design must include an awareness of the utilities of data and features to a particular analysis task. Recently, the observation that - while not all - a significant class of data processing applications can be expressed in terms of a small set of primitives that are, in many cases, easy to parallelize, has led to frameworks, such as MapReduce, which have been successfully applied in data processing, mining, and information retrieval domains. Yet, in many other domains (including many aggregation and join tasks that are hard to parallelize) they significantly lag behind traditional solutions. In particular, many multimedia and social media analysis tasks are in the category of applications that pose significant challenges. In this talk, I will present an overview of recent developments in the area of scalable multimedia and social media retrieval and analysis in the cloud and our own efforts [1, 2, 3, 4, 5, 6] to build a scalable data processing middleware, called RanKloud, specifically sensitive to the needs and requirements of multimedia and social media analysis applications. RanKloud avoids waste by intelligently partitioning the data and allocating it on available resources to minimize the data replication and indexing overheads and to prune superfluous low-utility processing. It also includes a tensor-based relational data model to support the complete lifecycle (from collection to analysis) of the data, involving various integration and other manipulation steps. RanKloud also addresses the computational cost of various multi-dimensional data analysis operations, including decomposition or structural change detection, by (a) leveraging a priori background knowledge (or metadata) about one or more domain dimensions and (b) by extending compressed sensing (CS) to tensor data to encode the observed tensor streams in the form of compact descriptors. RanKloud will extend the scope of cloud-based systems to the delivery of efficient and large scale analysis over data with variable utility and, thus, will enable new and efficient applications, tools, and systems for multimedia and social media retrieval and analysis. analysis, compressed sensing, data partitioning, mapreduce, multimedia, multiresolution, parallel processing, retrieval, social media, tensor decomposition",rankloud scalable multimedia and social media retrieval and analysis in the cloud today multimedia data are produced in massive quantities thanks to a diverse spectrum of applications including entertainment surveillance e commerce web and social media in particular social media data have three challenging characteristics data sizes are enormous data are often multi faceted and data are dynamic tensors multi dimensional arrays are widely used for representing such high order dimensional data consequently a system dealing with social media data needs to scale with the tensor volume and the number and diversity of the data facets this necessitates highly parallelizable and in many cases cloud based frameworks for scalable processing and efficient analysis of large media and social media collections most multimedia applications share a few core operations including integration fusion classification clustering graph analysis near neighbor search and similarity search when performed naively however these core operations are often very costly because the number of objects and object features that need to be considered can be prohibitive avoiding this cost requires that redundant work is avoided thus for the next generation cloud based massive media processing and analysis systems to have transformative impact the fundamental principles that govern their design must include an awareness of the utilities of data and features to a particular analysis task recently the observation that while not all a significant class of data processing applications can be expressed in terms of a small set of primitives that are in many cases easy to parallelize has led to frameworks such as mapreduce which have been successfully applied in data processing mining and information retrieval domains yet in many other domains including many aggregation and join tasks that are hard to parallelize they significantly lag behind traditional solutions in particular many multimedia and social media analysis tasks are in the category of applications that pose significant challenges in this talk i will present an overview of recent developments in the area of scalable multimedia and social media retrieval and analysis in the cloud and our own efforts 1 2 3 4 5 6 to build a scalable data processing middleware called rankloud specifically sensitive to the needs and requirements of multimedia and social media analysis applications rankloud avoids waste by intelligently partitioning the data and allocating it on available resources to minimize the data replication and indexing overheads and to prune superfluous low utility processing it also includes a tensor based relational data model to support the complete lifecycle from collection to analysis of the data involving various integration and other manipulation steps rankloud also addresses the computational cost of various multi dimensional data analysis operations including decomposition or structural change detection by a leveraging a priori background knowledge or metadata about one or more domain dimensions and b by extending compressed sensing cs to tensor data to encode the observed tensor streams in the form of compact descriptors rankloud will extend the scope of cloud based systems to the delivery of efficient and large scale analysis over data with variable utility and thus will enable new and efficient applications tools and systems for multimedia and social media retrieval and analysis analysis compressed sensing data partitioning mapreduce multimedia multiresolution parallel processing retrieval social media tensor decomposition,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
15,0,"Event Correlation for Process Discovery from Web Service Interaction Logs Understanding, analyzing, and ultimately improving business processes is a goal of enterprises today. These tasks are challenging as business processes in modern enterprises are implemented over several applications and Web services, and the information about process execution is scattered across several data sources. Understanding modern business processes entails identifying the correlation between events in data sources in the context of business processes (event correlation is the process of finding relationships between events that belong to the same process execution instance). In this paper, we investigate the problem of event correlation for business processes that are realized through the interactions of a set of Web services. We identify various ways in which process-related events could be correlated as well as investigate the problem of discovering event correlation (semi-) automatically from service interaction logs. We introduce the concept of process view to represent the process resulting from a certain way of event correlation and that of process space referring to the set of possible process views over process events. Event correlation is a challenging problem as there are various ways in which process events could be correlated, and in many cases, it is subjective. Exploring all the possibilities of correlations is computationally expensive, and only some of the correlated event sets result in process views that are interesting. We propose efficient algorithms and heuristics to identify correlated event sets that lead potentially to interesting process views. To account for its subjectivity, we have designed the event correlation discovery process to be interactive and enable users to guide it toward process views of their interest and organize the discovered process views into a process map that allows users to effectively navigate through the process space and identify the ones of interest. We report on experiments performed on both synthetic and real-world datasets that show the viability and efficiency of the approach. Business processes, Event correlation, Process spaces, Process views",event correlation for process discovery from web service interaction logs understanding analyzing and ultimately improving business processes is a goal of enterprises today these tasks are challenging as business processes in modern enterprises are implemented over several applications and web services and the information about process execution is scattered across several data sources understanding modern business processes entails identifying the correlation between events in data sources in the context of business processes event correlation is the process of finding relationships between events that belong to the same process execution instance in this paper we investigate the problem of event correlation for business processes that are realized through the interactions of a set of web services we identify various ways in which process related events could be correlated as well as investigate the problem of discovering event correlation semi automatically from service interaction logs we introduce the concept of process view to represent the process resulting from a certain way of event correlation and that of process space referring to the set of possible process views over process events event correlation is a challenging problem as there are various ways in which process events could be correlated and in many cases it is subjective exploring all the possibilities of correlations is computationally expensive and only some of the correlated event sets result in process views that are interesting we propose efficient algorithms and heuristics to identify correlated event sets that lead potentially to interesting process views to account for its subjectivity we have designed the event correlation discovery process to be interactive and enable users to guide it toward process views of their interest and organize the discovered process views into a process map that allows users to effectively navigate through the process space and identify the ones of interest we report on experiments performed on both synthetic and real world datasets that show the viability and efficiency of the approach business processes event correlation process spaces process views,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
16,0,"Agile Development with Software Process Mining Modern companies continue investing more and more in the creation, maintenance and change of software systems, but the proper specification and design of such systems continues to be a challenge. The majority of current approaches either ignore real user and system runtime behavior or consider it only informally. This leads to a rather prescriptive top-down approach to software development. In this paper, we propose a bottom-up approach, which takes event logs (e.g., trace data) of a software system for the analysis of the user and system runtime behavior and for improving the software. We use well-established methods from the area of process mining for this analysis. Moreover, we suggest embedding process mining into the agile development lifecycle. The goal of this position paper is to motivate the need for foundational research in the area of software process mining (applying process mining to software analysis) by showing the relevance and listing open challenges. Our proposal is based on our experiences with analyzing a big productive touristic system. This system was developed using agile methods and process mining could be effectively integrated into the development lifecycle. Agile Methods, Process Mining, Software Process",agile development with software process mining modern companies continue investing more and more in the creation maintenance and change of software systems but the proper specification and design of such systems continues to be a challenge the majority of current approaches either ignore real user and system runtime behavior or consider it only informally this leads to a rather prescriptive top down approach to software development in this paper we propose a bottom up approach which takes event logs e g trace data of a software system for the analysis of the user and system runtime behavior and for improving the software we use well established methods from the area of process mining for this analysis moreover we suggest embedding process mining into the agile development lifecycle the goal of this position paper is to motivate the need for foundational research in the area of software process mining applying process mining to software analysis by showing the relevance and listing open challenges our proposal is based on our experiences with analyzing a big productive touristic system this system was developed using agile methods and process mining could be effectively integrated into the development lifecycle agile methods process mining software process,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
17,0,"CHOReOSynt: Enforcing Choreography Realizability in the Future Internet Choreographies are an emergent Service Engineering (SE) approach to compose together and coordinate services in a distributed way. A choreography formalizes the way business participants coordinate their interactions. The focus is not on orchestrations of the work performed within them, but rather on the exchange of messages between these participants. The problems usually addressed when considering a choreography-based specification of the system to be realized are realizability check, and conformance check. In this paper we describe the CHOReOSynt tool, which has been conceived to deal with an additional problem, namely, automated choreography enforcement. That is, when the goal is to actually realize a service choreography by reusing third-party services, their uncontrolled (or wrongly coordinated) composite behavior may show undesired interactions that preclude the choreography realization. CHOReOSynt solves this problem by automatically synthesizing additional software entities that, when interposed among the services, allow for preventing undesired interactions. Choreography Synthesis, Distributed Coordination",choreosynt enforcing choreography realizability in the future internet choreographies are an emergent service engineering se approach to compose together and coordinate services in a distributed way a choreography formalizes the way business participants coordinate their interactions the focus is not on orchestrations of the work performed within them but rather on the exchange of messages between these participants the problems usually addressed when considering a choreography based specification of the system to be realized are realizability check and conformance check in this paper we describe the choreosynt tool which has been conceived to deal with an additional problem namely automated choreography enforcement that is when the goal is to actually realize a service choreography by reusing third party services their uncontrolled or wrongly coordinated composite behavior may show undesired interactions that preclude the choreography realization choreosynt solves this problem by automatically synthesizing additional software entities that when interposed among the services allow for preventing undesired interactions choreography synthesis distributed coordination,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
18,0,"Health Intelligence: Discovering the Process Model Using Process Mining by Constructing Start-to-end Patient Journeys Australian Public Hospitals are continually engaged in various process improvement activities to improve patient care and to improve hospital efficiency as the demand for service intensifies. As a consequence there are many initiatives within the health sector focusing on gaining insight into the underlying health processes which are assessed for compliance with specified Key Performance Indicators (KPIs). Process Mining is classified as a Business Intelligence (BI) tool. The aim of process mining activities is to gain insight into the underlying process or processes. The fundamental element needed for process mining is a historical event log of a process. Generally, these event logs are easily sourced from Process Aware Information Systems (PAIS). Simulation is widely used by hospitals as a tool to study the complex hospital setting and for prediction. Generally, simulation models are constructed by 'hand'. This paper presents a novel way of deriving event logs for health data in the absence of PAIS. The constructed event log is then used as an input for process mining activities taking advantage of existing process mining algorithms aiding the discovery of knowledge of the underlying processes which leads to Health Intelligence (HI). One such output of process mining activity, presented in this paper, is the discovery of process model for simulation using the derived event log as an input for process mining by constructing start-to-end patient journey. The study was undertaken using data from Flinders Medical Centre to gain insight into patient journeys from the point of admission to the Emergency Department (ED) until the patient is discharged from the hospital. emergency department (ED), event logs, general medicine (GM), hospital key performance indicators, inliers, outliers, patient journey, process mining, simulation model",health intelligence discovering the process model using process mining by constructing start to end patient journeys australian public hospitals are continually engaged in various process improvement activities to improve patient care and to improve hospital efficiency as the demand for service intensifies as a consequence there are many initiatives within the health sector focusing on gaining insight into the underlying health processes which are assessed for compliance with specified key performance indicators kpis process mining is classified as a business intelligence bi tool the aim of process mining activities is to gain insight into the underlying process or processes the fundamental element needed for process mining is a historical event log of a process generally these event logs are easily sourced from process aware information systems pais simulation is widely used by hospitals as a tool to study the complex hospital setting and for prediction generally simulation models are constructed by hand this paper presents a novel way of deriving event logs for health data in the absence of pais the constructed event log is then used as an input for process mining activities taking advantage of existing process mining algorithms aiding the discovery of knowledge of the underlying processes which leads to health intelligence hi one such output of process mining activity presented in this paper is the discovery of process model for simulation using the derived event log as an input for process mining by constructing start to end patient journey the study was undertaken using data from flinders medical centre to gain insight into patient journeys from the point of admission to the emergency department ed until the patient is discharged from the hospital emergency department ed event logs general medicine gm hospital key performance indicators inliers outliers patient journey process mining simulation model,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
19,0,"Model Based Test Case Generation with Metaheuristics for Networks of Timed Automata Model-Based Testing, the task of generating test inputs and oracles from a test model, has been successfully applied in the context of safety-critical real time systems. As these systems grow in complexity, test-models, designed to reflect the systems behaviour, will grow too. Currently testers face situations where test-models are too complex for present test generators. In this paper, we outline a software tool for the evaluation of the scalability of a combination of approaches for model-based test generation. We chose Networks of Timed Automata (NTA) as the modeling formalism because real-time properties can be specified and the semantics are well-defined. However, the tool input is given as a restricted UML statechart which is internally transformed. We expect this to increase industrial acceptance. The tool will provide the selection, parametrization and generation of a metaheuristic algorithm. The aim is to support test model specific generation algorithms. A simulator for NTAs will enable the metaheuristic to search for test goals in the model. For better performance, it will have an advanced parallelisation. Furthermore, input models will be used for search space reduction for even faster test case generation. The proposed approach allows the inclusion of an oracle generator that is able to provide expected outputs; this enables conformance checking between test models and systems under test. We plan to implement the outlined tool to enable test case generation even for models that are beyond the scope of currently available generators. metaheuristics, model based testing, search based testing, timed automata",model based test case generation with metaheuristics for networks of timed automata model based testing the task of generating test inputs and oracles from a test model has been successfully applied in the context of safety critical real time systems as these systems grow in complexity test models designed to reflect the systems behaviour will grow too currently testers face situations where test models are too complex for present test generators in this paper we outline a software tool for the evaluation of the scalability of a combination of approaches for model based test generation we chose networks of timed automata nta as the modeling formalism because real time properties can be specified and the semantics are well defined however the tool input is given as a restricted uml statechart which is internally transformed we expect this to increase industrial acceptance the tool will provide the selection parametrization and generation of a metaheuristic algorithm the aim is to support test model specific generation algorithms a simulator for ntas will enable the metaheuristic to search for test goals in the model for better performance it will have an advanced parallelisation furthermore input models will be used for search space reduction for even faster test case generation the proposed approach allows the inclusion of an oracle generator that is able to provide expected outputs this enables conformance checking between test models and systems under test we plan to implement the outlined tool to enable test case generation even for models that are beyond the scope of currently available generators metaheuristics model based testing search based testing timed automata,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
20,0,"Process Mining Can Be Applied to Software Too! Modern information systems produce tremendous amounts of event data. The area of process mining deals with extracting knowledge from this data. Real-life processes can be effectively discovered, analyzed and optimized with the help of mature process mining techniques. There is a variety of process mining case studies and experience reports from such business areas as healthcare, public, transportation and education. Although nowadays, these techniques are mostly used for discovering business processes. The goal of this industrial paper is to show that process mining can be applied to software too. Here we present and analyze our experiences on applying process mining in different productive software systems used in the touristic domain. Process models and user interface workflows underlie the functional specifications of the systems we experiment with. When the systems are utilized, user interaction is recorded in event logs. After applying process mining methods to these logs, process and user interface flow models are automatically derived. These resulting models provide insight regarding the real usage of the software, motivate the changes in the functional specifications, enable usability improvements and software redesign. Thus, with the help of our examples we demonstrate that process mining facilitates new forms of software analysis. The user interaction with almost every software system can be mined in order to improve the software and to monitor and measure its real usage. client technology, process mining, software process mining, user interface design",process mining can be applied to software too modern information systems produce tremendous amounts of event data the area of process mining deals with extracting knowledge from this data real life processes can be effectively discovered analyzed and optimized with the help of mature process mining techniques there is a variety of process mining case studies and experience reports from such business areas as healthcare public transportation and education although nowadays these techniques are mostly used for discovering business processes the goal of this industrial paper is to show that process mining can be applied to software too here we present and analyze our experiences on applying process mining in different productive software systems used in the touristic domain process models and user interface workflows underlie the functional specifications of the systems we experiment with when the systems are utilized user interaction is recorded in event logs after applying process mining methods to these logs process and user interface flow models are automatically derived these resulting models provide insight regarding the real usage of the software motivate the changes in the functional specifications enable usability improvements and software redesign thus with the help of our examples we demonstrate that process mining facilitates new forms of software analysis the user interaction with almost every software system can be mined in order to improve the software and to monitor and measure its real usage client technology process mining software process mining user interface design,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
21,0,"Process Mining Software Repositories from Student Projects in an Undergraduate Software Engineering Course An undergraduate level Software Engineering courses generally consists of a team-based semester long project and emphasizes on both technical and managerial skills. Software Engineering is a practice-oriented and applied discipline and hence there is an emphasis on hands-on development, process, usage of tools in addition to theory and basic concepts. We present an approach for mining the process data (process mining) from software repositories archiving data generated as a result of constructing software by student teams in an educational setting. We present an application of mining three software repositories: team wiki (used during requirement engineering), version control system (development and maintenance) and issue tracking system (corrective and adaptive maintenance) in the context of an undergraduate Software Engineering course. We propose visualizations, metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life-cycle. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the instructor serving as a feedback tool on development process and quality by students. We mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team, quality of commit messages, intensity and consistency of commit activities, bug fixing process trend and quality, component and developer entropy, process compliance and verification. We present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges, limitations and recommendations. Education Data Mining, Learning Analytic, Mining Software Repositories, Process Mining, Software Engineering Education",process mining software repositories from student projects in an undergraduate software engineering course an undergraduate level software engineering courses generally consists of a team based semester long project and emphasizes on both technical and managerial skills software engineering is a practice oriented and applied discipline and hence there is an emphasis on hands on development process usage of tools in addition to theory and basic concepts we present an approach for mining the process data process mining from software repositories archiving data generated as a result of constructing software by student teams in an educational setting we present an application of mining three software repositories team wiki used during requirement engineering version control system development and maintenance and issue tracking system corrective and adaptive maintenance in the context of an undergraduate software engineering course we propose visualizations metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life cycle the proposed visualizations and metrics learning analytics provide a multi faceted view to the instructor serving as a feedback tool on development process and quality by students we mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team quality of commit messages intensity and consistency of commit activities bug fixing process trend and quality component and developer entropy process compliance and verification we present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges limitations and recommendations education data mining learning analytic mining software repositories process mining software engineering education,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
22,0,"A Process Mining Approach to Linking the Study of Aptitude and Event Facets of Self-regulated Learning Research on self-regulated learning has taken main two paths: self-regulated learning as aptitudes and more recently, self-regulated learning as events. This paper proposes the use of the Fuzzy miner process mining technique to examine the relationship between students' self-reported aptitudes (i.e., achievement goal orientation and approaches to learning) and strategies followed in self-regulated learning. A pilot study is conducted to probe the method and the preliminary results are reported. clustering, learning patterns, process mining, self-regulated learning",a process mining approach to linking the study of aptitude and event facets of self regulated learning research on self regulated learning has taken main two paths self regulated learning as aptitudes and more recently self regulated learning as events this paper proposes the use of the fuzzy miner process mining technique to examine the relationship between students self reported aptitudes i e achievement goal orientation and approaches to learning and strategies followed in self regulated learning a pilot study is conducted to probe the method and the preliminary results are reported clustering learning patterns process mining self regulated learning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
23,0,"Mining Software Development Process Variations Process tailoring aims to customize a software process to better suit the specific needs of an organization when executing a software project or due to a social context in which the process is inserted. Tailoring happens, in general, through variations in the process elements, such as activities, artifacts, and control flows. This paper aims to introduce a technique that uses process mining to uncover elements from the software process that are candidates for tailoring. The proposed approach analyzes the execution logs from several process instances that share a common standard process. As a result, execution traces that differ from the standard process flow are identified and assessed to uncover their variable elements. The proposed technique was evaluated with data extracted from a real software development scenario when a large system was under development for a set of Brazilian Federal Institutes of Education, Science and Technology. process mining, process tailoring, software process, variation",mining software development process variations process tailoring aims to customize a software process to better suit the specific needs of an organization when executing a software project or due to a social context in which the process is inserted tailoring happens in general through variations in the process elements such as activities artifacts and control flows this paper aims to introduce a technique that uses process mining to uncover elements from the software process that are candidates for tailoring the proposed approach analyzes the execution logs from several process instances that share a common standard process as a result execution traces that differ from the standard process flow are identified and assessed to uncover their variable elements the proposed technique was evaluated with data extracted from a real software development scenario when a large system was under development for a set of brazilian federal institutes of education science and technology process mining process tailoring software process variation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
24,0,"Next Step Recommendation and Prediction Based on Process Mining in Adaptive Case Management Adaptive Case Management (ACM) is a new paradigm that facilitates the coordination of knowledge work through case handling. Current ACM systems, however, lack support of providing sophisticated user guidance for next step recommendations and predictions about the case future. In recent years, process mining research developed approaches to make recommendations and predictions based on event logs readily available in process-aware information systems. This paper builds upon those approaches and integrates them into an existing ACM solution. The research goal is to design and develop a prototype that gives next step recommendations and predictions based on process mining techniques in ACM systems. The models proposed, recommend actions that shorten the case running time, mitigate deadline transgressions, support case goals and have been used in former cases with similar properties. They further give case predictions about the remaining time, possible deadline violations, and whether the current case path supports given case goals. A final evaluation proves that the prototype is indeed capable of making proper recommendations and predictions. In addition, starting points for further improvement are discussed. adaptive case management, business process management, decision support, process mining, recommender systems",next step recommendation and prediction based on process mining in adaptive case management adaptive case management acm is a new paradigm that facilitates the coordination of knowledge work through case handling current acm systems however lack support of providing sophisticated user guidance for next step recommendations and predictions about the case future in recent years process mining research developed approaches to make recommendations and predictions based on event logs readily available in process aware information systems this paper builds upon those approaches and integrates them into an existing acm solution the research goal is to design and develop a prototype that gives next step recommendations and predictions based on process mining techniques in acm systems the models proposed recommend actions that shorten the case running time mitigate deadline transgressions support case goals and have been used in former cases with similar properties they further give case predictions about the remaining time possible deadline violations and whether the current case path supports given case goals a final evaluation proves that the prototype is indeed capable of making proper recommendations and predictions in addition starting points for further improvement are discussed adaptive case management business process management decision support process mining recommender systems,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
25,0,"Towards Aspects Identification in Business Process Through Process Mining In business process models, elements can be scattered (repeated) within different processes, making it difficult to handle changes, analyze process for improvements, or check crosscutting impacts. These scattered elements are named as Aspects. Similar to the aspect-oriented paradigm in programming languages, in BPM, aspect handling has the goal to modularize the crosscutting concerns spread across the models. This process modularization facilitates the management of the process (reuse, maintenance and understanding). The current approaches for aspect identification are made manually; thus, resulting in the problem of subjectivity and lack of systematization. This paper proposes a method to automatically identify aspects in business process from its event logs. The method is based on mining techniques and it aims to solve the problem of the subjectivity identification made by specialists. The initial results from a preliminary evaluation showed evidences that the method identified correctly the aspects present in the process model. Aspects, Business Process Management, Process Mining",towards aspects identification in business process through process mining in business process models elements can be scattered repeated within different processes making it difficult to handle changes analyze process for improvements or check crosscutting impacts these scattered elements are named as aspects similar to the aspect oriented paradigm in programming languages in bpm aspect handling has the goal to modularize the crosscutting concerns spread across the models this process modularization facilitates the management of the process reuse maintenance and understanding the current approaches for aspect identification are made manually thus resulting in the problem of subjectivity and lack of systematization this paper proposes a method to automatically identify aspects in business process from its event logs the method is based on mining techniques and it aims to solve the problem of the subjectivity identification made by specialists the initial results from a preliminary evaluation showed evidences that the method identified correctly the aspects present in the process model aspects business process management process mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
26,0,"Conformance Check in Healthcare with the Supporting of Processes Mining The healthcare processes are complex and require a certain level of interdisciplinary cooperation among the various specialists and sectors involved in the processes. Besides this complexity, the Brazilian healthcare area has a notorious problem in its public and private health assistance. These problems are structural, organizational and financial, reflecting in the low valuation of quality and service. The goal of this work is propose an adaptation of Process Mining to healthcare processes in order to contribute in improve the healthcare area in Brazil. In order to achieve this goal a study case was carried out in the Erasto Gaertner hospital, situated in Curitiba - PR, Brazil, that is a national reference in treatment of cancer. Process mining, business rules, conformance check, healthcare, process mapping",conformance check in healthcare with the supporting of processes mining the healthcare processes are complex and require a certain level of interdisciplinary cooperation among the various specialists and sectors involved in the processes besides this complexity the brazilian healthcare area has a notorious problem in its public and private health assistance these problems are structural organizational and financial reflecting in the low valuation of quality and service the goal of this work is propose an adaptation of process mining to healthcare processes in order to contribute in improve the healthcare area in brazil in order to achieve this goal a study case was carried out in the erasto gaertner hospital situated in curitiba pr brazil that is a national reference in treatment of cancer process mining business rules conformance check healthcare process mapping,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
27,0,"Goal Achievement Analysis Based on LTL Checking and Decision Tree for Improvements of PAIS Process aware information system (PAIS) is important in the recent business environment. Developments of PAIS need to consider contexts about technical and business elements. They are needed to develop PAIS effectively (e.g. monitoring environment and constructing adequate business process). Process mining is an important method for analyzing a business environment and utilizing PAIS development and improvement. LTL checking is an important method for checking a specific property to be satisfied with business processes, but correctly writing formal language like LTL is difficult. In this paper, we use LTL checking and prediction based on decision-tree learning for checking goal achievement, false detection and oversight detection. It helps writing properly LTL formula for representing the correct goal property. We conducted a case study using a real life log of traffic fine management process in Italy. conformance checking, process aware information system, process mining, process modeling",goal achievement analysis based on ltl checking and decision tree for improvements of pais process aware information system pais is important in the recent business environment developments of pais need to consider contexts about technical and business elements they are needed to develop pais effectively e g monitoring environment and constructing adequate business process process mining is an important method for analyzing a business environment and utilizing pais development and improvement ltl checking is an important method for checking a specific property to be satisfied with business processes but correctly writing formal language like ltl is difficult in this paper we use ltl checking and prediction based on decision tree learning for checking goal achievement false detection and oversight detection it helps writing properly ltl formula for representing the correct goal property we conducted a case study using a real life log of traffic fine management process in italy conformance checking process aware information system process mining process modeling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
28,0,"KDM As the Underlying Metamodel in Architecture-Conformance Checking There are two important artifacts in any Architecture-Conformance Checking (ACC) approach: i) the representation of the PA and ii) the representation of the CA. Many times, inside the same ACC approach, distinct meta-models are adopted for representing the PA and the CA. Besides, it is common the adoption of meta-models unsuitable for representing architectural details. This heterogeneity makes the checking algorithms complex since they must cope with instances that comply with two different meta-models or do not have proper architectural abstractions. KDM is an ISO meta-model proposed by OMG whose goal is to become the standard representation of systems in modernization tools. It is able to represent many aspects of a software system, including source code details, architectural abstractions and the dependencies between them. However, up to this moment, there is no research showing how KDM can be used in ACC approaches. Therefore we present an investigation of adopting KDM as the unique meta-model for representing PA and CA in ACC approaches. We have developed a three-steps ACC approach called ArchKDM. In the first step a DSL assists in the PA specification; in the second step an Eclipse plug-in provides the necessary support and in the last step the checking is conducted. We have also evaluate our approach using two real world systems and the results were very promising, revealing no false positives or negatives. Architectural Conformance Checking, Architectural Reconciliation, Architecture-Description Language, Architecture-Driven Modernization, Knowledge-Discovery Metamodel",kdm as the underlying metamodel in architecture conformance checking there are two important artifacts in any architecture conformance checking acc approach i the representation of the pa and ii the representation of the ca many times inside the same acc approach distinct meta models are adopted for representing the pa and the ca besides it is common the adoption of meta models unsuitable for representing architectural details this heterogeneity makes the checking algorithms complex since they must cope with instances that comply with two different meta models or do not have proper architectural abstractions kdm is an iso meta model proposed by omg whose goal is to become the standard representation of systems in modernization tools it is able to represent many aspects of a software system including source code details architectural abstractions and the dependencies between them however up to this moment there is no research showing how kdm can be used in acc approaches therefore we present an investigation of adopting kdm as the unique meta model for representing pa and ca in acc approaches we have developed a three steps acc approach called archkdm in the first step a dsl assists in the pa specification in the second step an eclipse plug in provides the necessary support and in the last step the checking is conducted we have also evaluate our approach using two real world systems and the results were very promising revealing no false positives or negatives architectural conformance checking architectural reconciliation architecture description language architecture driven modernization knowledge discovery metamodel,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
29,0,"ReStream: Accelerating Backtesting and Stream Replay with Serial-Equivalent Parallel Processing Real-time predictive applications can demand continuous and agile development, with new models constantly being trained, tested, and then deployed. Training and testing are done by replaying stored event logs, running new models in the context of historical data in a form of backtesting or ""what if?"" analysis. To replay weeks or months of logs while developers wait, we need systems that can stream event logs through prediction logic many times faster than the real-time rate. A challenge with high-speed replay is preserving sequential semantics while harnessing parallel processing power. The crux of the problem lies with causal dependencies inherent in the sequential semantics of log replay. We introduce an execution engine that produces serial-equivalent output while accelerating throughput with pipelining and distributed parallelism. This is made possible by optimizing for high throughput rather than the traditional stream processing goal of low latency, and by aggressive sharing of versioned state, a technique we term Multi-Versioned Parallel Streaming (MVPS). In experiments we see that this engine, which we call ReStream, performs as well as batch processing and more than an order of magnitude better than a single-threaded implementation. Stream replay, backtesting, distributed stream processing",restream accelerating backtesting and stream replay with serial equivalent parallel processing real time predictive applications can demand continuous and agile development with new models constantly being trained tested and then deployed training and testing are done by replaying stored event logs running new models in the context of historical data in a form of backtesting or what if analysis to replay weeks or months of logs while developers wait we need systems that can stream event logs through prediction logic many times faster than the real time rate a challenge with high speed replay is preserving sequential semantics while harnessing parallel processing power the crux of the problem lies with causal dependencies inherent in the sequential semantics of log replay we introduce an execution engine that produces serial equivalent output while accelerating throughput with pipelining and distributed parallelism this is made possible by optimizing for high throughput rather than the traditional stream processing goal of low latency and by aggressive sharing of versioned state a technique we term multi versioned parallel streaming mvps in experiments we see that this engine which we call restream performs as well as batch processing and more than an order of magnitude better than a single threaded implementation stream replay backtesting distributed stream processing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
30,0,"A Data-driven Process Recommender Framework We present an approach for improving the performance of complex knowledge-based processes by providing data-driven step-by-step recommendations. Our framework uses the associations between similar historic process performances and contextual information to determine the prototypical way of enacting the process. We introduce a novel similarity metric for grouping traces into clusters that incorporates temporal information about activity performance and handles concurrent activities. Our data-driven recommender system selects the appropriate prototype performance of the process based on user-provided context attributes. Our approach for determining the prototypes discovers the commonly performed activities and their temporal relationships. We tested our system on data from three real-world medical processes and achieved recommendation accuracy up to an F1 score of 0.77 (compared to an F1 score of 0.37 using ZeroR) with 63.2% of recommended enactments being within the first five neighbors of the actual historic enactments in a set of 87 cases. Our framework works as an interactive visual analytic tool for process mining. This work shows the feasibility of data-driven decision support system for complex knowledge-based processes. emergency medical process analysis., process prototype extraction, process recommender system, process trace clustering",a data driven process recommender framework we present an approach for improving the performance of complex knowledge based processes by providing data driven step by step recommendations our framework uses the associations between similar historic process performances and contextual information to determine the prototypical way of enacting the process we introduce a novel similarity metric for grouping traces into clusters that incorporates temporal information about activity performance and handles concurrent activities our data driven recommender system selects the appropriate prototype performance of the process based on user provided context attributes our approach for determining the prototypes discovers the commonly performed activities and their temporal relationships we tested our system on data from three real world medical processes and achieved recommendation accuracy up to an f1 score of 0 77 compared to an f1 score of 0 37 using zeror with 63 2 of recommended enactments being within the first five neighbors of the actual historic enactments in a set of 87 cases our framework works as an interactive visual analytic tool for process mining this work shows the feasibility of data driven decision support system for complex knowledge based processes emergency medical process analysis process prototype extraction process recommender system process trace clustering,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
31,0,"An Architecture for Skill Assessment in Serious Games Based on Event Sequence Analysis Serious games are video games with educational purposes. Players interact in many points during a gameplay. These interactions can be registered, producing data sets with sequence of events which provide relevant information about player's skills. Unfortunately, traditional skill assessment methods present limitations to carry out a detailed analysis of large data sets. Sequence Analysis is a group of techniques which allow to analyze data sets consisting of sequence of events. These techniques have been successfully implemented in different fields, and we consider that they can help overcome these limitations. In this paper, we propose an architecture of skill assessment in learning experiences based on serious games using a set of Sequence Analysis techniques known as Process Mining. First, several in-game events are stored in a log. These events are produced by player's interactions with Key Performance Indicators included in the game. Second, event log is used as input for a Process Mining tool. Discovery process is executed and a behavioural model is provided. Third, an assessment metric must be carried out over the model. Finally, a synthetic experiment is conducted and promising results are obtained. Event-Based Data Analysis, Game-Based Learning, Process Mining, Sequence Analysis, Serious games, Skill assessment, e-Learning",an architecture for skill assessment in serious games based on event sequence analysis serious games are video games with educational purposes players interact in many points during a gameplay these interactions can be registered producing data sets with sequence of events which provide relevant information about player s skills unfortunately traditional skill assessment methods present limitations to carry out a detailed analysis of large data sets sequence analysis is a group of techniques which allow to analyze data sets consisting of sequence of events these techniques have been successfully implemented in different fields and we consider that they can help overcome these limitations in this paper we propose an architecture of skill assessment in learning experiences based on serious games using a set of sequence analysis techniques known as process mining first several in game events are stored in a log these events are produced by player s interactions with key performance indicators included in the game second event log is used as input for a process mining tool discovery process is executed and a behavioural model is provided third an assessment metric must be carried out over the model finally a synthetic experiment is conducted and promising results are obtained event based data analysis game based learning process mining sequence analysis serious games skill assessment e learning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
32,0,"Framework for Supervised and Secure Distributed Simulations of Critical Infrastructures Research on critical infrastructures (CI)s deals with sensitive data that demands underlying platform to be secure, in addition, testing of CI resilience strategies requires reproducibility of results. Disruption or natural disaster scenarios can not be tested on the physical systems, thus simulations are used for experimentation on CIs and as they are inherently distributed and interdependent which is why here, to model CIs, we use a distributed simulations standard namely High Level Architecture (HLA). The main goals of HLA are to provide interoperability and flexibility. Security and events logging are neither included nor emphasis of HLA standard. This paper presents a framework to have a supervised and secure messages exchange across distributed simulations using HLA standard. []",framework for supervised and secure distributed simulations of critical infrastructures research on critical infrastructures ci s deals with sensitive data that demands underlying platform to be secure in addition testing of ci resilience strategies requires reproducibility of results disruption or natural disaster scenarios can not be tested on the physical systems thus simulations are used for experimentation on cis and as they are inherently distributed and interdependent which is why here to model cis we use a distributed simulations standard namely high level architecture hla the main goals of hla are to provide interoperability and flexibility security and events logging are neither included nor emphasis of hla standard this paper presents a framework to have a supervised and secure messages exchange across distributed simulations using hla standard,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
33,0,"Mining Logs to Model the Use of a System Background. Process mining is a technique to build process models from ""execution logs"" (i.e., events triggered by the execution of a process). State-of-the-art tools can provide process managers with different graphical representations of such models. Managers use these models to compare them with an ideal process model or to support process improvement. They typically select the representation based on their experience and knowledge of the system. Aim. This work studies how to automatically build process models representing the actual intents (or uses) of users while interacting with a software system. Such intents are expressed as a set of actions performed by a user to a system to achieve specific use goals. Method. This work applies the theory of Hidden Markov Models to mine use logs and automatically model the use of a system. Results. Unlike the models generated with process mining tools, the Hidden Markov Models automatically generated in this study provide the intents of a user and can be used to recommend managers with a faithful representation of the use of their systems. Conclusions. The automatic generation of the Hidden Markov Models can achieve a good level of accuracy in representing the actual user's intents provided the log dataset is carefully chosen. In our study, the information contained in one-month set of logs helped automatically build Hidden Markov Models with superior accuracy and similar expressiveness of the models built together with the company's stakeholder. hidden markov chain, log analysis, process modelling",mining logs to model the use of a system background process mining is a technique to build process models from execution logs i e events triggered by the execution of a process state of the art tools can provide process managers with different graphical representations of such models managers use these models to compare them with an ideal process model or to support process improvement they typically select the representation based on their experience and knowledge of the system aim this work studies how to automatically build process models representing the actual intents or uses of users while interacting with a software system such intents are expressed as a set of actions performed by a user to a system to achieve specific use goals method this work applies the theory of hidden markov models to mine use logs and automatically model the use of a system results unlike the models generated with process mining tools the hidden markov models automatically generated in this study provide the intents of a user and can be used to recommend managers with a faithful representation of the use of their systems conclusions the automatic generation of the hidden markov models can achieve a good level of accuracy in representing the actual user s intents provided the log dataset is carefully chosen in our study the information contained in one month set of logs helped automatically build hidden markov models with superior accuracy and similar expressiveness of the models built together with the company s stakeholder hidden markov chain log analysis process modelling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
34,0,"Predicting DRAM Reliability in the Field with Machine Learning Uncorrectable errors in dynamic random access memory (DRAM) are a common form of hardware failure in server clusters. Failures are costly both in terms of hardware replacement costs and service disruption. While a large body of work exists on analyzing DRAM reliability in large production clusters, little has been reported on the automatic prediction of such errors ahead of time. In this paper, we present a highly accurate predictive model, based on daily event logs and sensor measurements, in a large fleet of commodity servers going back to 2014. By correlating correctable errors with sensor metrics, we can use ensemble machine learning techniques to predict uncorrectable errors weeks in advance. In addition, we show how such models can be applied in the wild and consumed by customer support teams. Our goal is to minimize false positives, as healthy DRAMs should not be replaced, while accounting for common limitations, such as missing data points and rare occurences of uncorrectable errors. ensemble machine learning, failure prediction, memory systems, reliability",predicting dram reliability in the field with machine learning uncorrectable errors in dynamic random access memory dram are a common form of hardware failure in server clusters failures are costly both in terms of hardware replacement costs and service disruption while a large body of work exists on analyzing dram reliability in large production clusters little has been reported on the automatic prediction of such errors ahead of time in this paper we present a highly accurate predictive model based on daily event logs and sensor measurements in a large fleet of commodity servers going back to 2014 by correlating correctable errors with sensor metrics we can use ensemble machine learning techniques to predict uncorrectable errors weeks in advance in addition we show how such models can be applied in the wild and consumed by customer support teams our goal is to minimize false positives as healthy drams should not be replaced while accounting for common limitations such as missing data points and rare occurences of uncorrectable errors ensemble machine learning failure prediction memory systems reliability,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
35,0,"White-box Prediction of Process Performance Indicators via Flow Analysis Predictive business process monitoring methods exploit historical process execution logs to provide predictions about running instances of a process, which enable process workers and managers to preempt performance issues or compliance violations. A number of approaches have been proposed to predict quantitative process performance indicators, such as remaining cycle time, cost, or probability of deadline violation. However, these approaches adopt a black-box approach, insofar as they predict a single scalar value without decomposing this prediction into more elementary components. In this paper, we propose a white-box approach to predict performance indicators of running process instances. The key idea is to first predict the performance indicator at the level of activities, and then to aggregate these predictions at the level of a process instance by means of flow analysis techniques. The paper specifically develops this idea in the context of predicting the remaining cycle time of ongoing process instances. The proposed approach has been evaluated on four real-life event logs and compared against several baselines. Flow analysis, Predictive Process Monitoring, Process Mining",white box prediction of process performance indicators via flow analysis predictive business process monitoring methods exploit historical process execution logs to provide predictions about running instances of a process which enable process workers and managers to preempt performance issues or compliance violations a number of approaches have been proposed to predict quantitative process performance indicators such as remaining cycle time cost or probability of deadline violation however these approaches adopt a black box approach insofar as they predict a single scalar value without decomposing this prediction into more elementary components in this paper we propose a white box approach to predict performance indicators of running process instances the key idea is to first predict the performance indicator at the level of activities and then to aggregate these predictions at the level of a process instance by means of flow analysis techniques the paper specifically develops this idea in the context of predicting the remaining cycle time of ongoing process instances the proposed approach has been evaluated on four real life event logs and compared against several baselines flow analysis predictive process monitoring process mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
36,0,"Discovery of temporal patterns from process instances Existing work in process mining focuses on the discovery of the underlying process model from their instances. In this paper, we do not assume the existence of a single process model to which all process instances comply, and the goal is to discover a set of frequently occurring temporal patterns. Discovery of temporal patterns can be applied to various application domains to support crucial business decision-making. In this study, we formally defined the temporal pattern discovery problem, and developed and evaluated three different temporal pattern discovery algorithms, namely TP-Graph, TP-Itemset and TP-Sequence. Their relative performances are reported. Process mining, Knowledge discovery, Data mining, Temporal patterns, Association rules, Sequential patterns",discovery of temporal patterns from process instances existing work in process mining focuses on the discovery of the underlying process model from their instances in this paper we do not assume the existence of a single process model to which all process instances comply and the goal is to discover a set of frequently occurring temporal patterns discovery of temporal patterns can be applied to various application domains to support crucial business decision making in this study we formally defined the temporal pattern discovery problem and developed and evaluated three different temporal pattern discovery algorithms namely tp graph tp itemset and tp sequence their relative performances are reported process mining knowledge discovery data mining temporal patterns association rules sequential patterns,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
37,0,"Business process mining: An industrial application Contemporary information systems (e.g., WfM, ERP, CRM, SCM, and B2B systems) record business events in so-called event logs. Business process mining takes these logs to discover process, control, data, organizational, and social structures. Although many researchers are developing new and more powerful process mining techniques and software vendors are incorporating these in their software, few of the more advanced process mining techniques have been tested on real-life processes. This paper describes the application of process mining in one of the provincial offices of the Dutch National Public Works Department, responsible for the construction and maintenance of the road and water infrastructure. Using a variety of process mining techniques, we analyzed the processing of invoices sent by the various subcontractors and suppliers from three different perspectives: (1) the process perspective, (2) the organizational perspective, and (3) the case perspective. For this purpose, we used some of the tools developed in the context of the ProM framework. The goal of this paper is to demonstrate the applicability of process mining in general and our algorithms and tools in particular. Process mining, Social network analysis, Workflow management, Business process management, Business process analysis, Data mining, Petri nets",business process mining an industrial application contemporary information systems e g wfm erp crm scm and b2b systems record business events in so called event logs business process mining takes these logs to discover process control data organizational and social structures although many researchers are developing new and more powerful process mining techniques and software vendors are incorporating these in their software few of the more advanced process mining techniques have been tested on real life processes this paper describes the application of process mining in one of the provincial offices of the dutch national public works department responsible for the construction and maintenance of the road and water infrastructure using a variety of process mining techniques we analyzed the processing of invoices sent by the various subcontractors and suppliers from three different perspectives 1 the process perspective 2 the organizational perspective and 3 the case perspective for this purpose we used some of the tools developed in the context of the prom framework the goal of this paper is to demonstrate the applicability of process mining in general and our algorithms and tools in particular process mining social network analysis workflow management business process management business process analysis data mining petri nets,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
38,0,"Analysing agricultural users' patterns of behaviour: The case of OPTIRas\texttrademark, a decision support system for starch crop selection Redesigning IT systems for specific user groups encompasses a lot of effort with respect to analysing and understanding user behaviour. The goal of this paper is to provide insights into patterns of behaviour of agricultural users, during the usage of a decision support system called OPTIRas\texttrademark. This system aids agricultural users in their cultivar selection activities. We analyse logs resulting from OPTIRas\texttrademark, and we get insights into user's navigational patterns. We claim that the results of our analysis can be used to support the redesign of decision support systems in order to address specific agricultural users' characteristics. Modelling user behaviour, Agricultural users, Decision support systems, Process mining, Cultivar selection",analysing agricultural users patterns of behaviour the case of optiras texttrademark a decision support system for starch crop selection redesigning it systems for specific user groups encompasses a lot of effort with respect to analysing and understanding user behaviour the goal of this paper is to provide insights into patterns of behaviour of agricultural users during the usage of a decision support system called optiras texttrademark this system aids agricultural users in their cultivar selection activities we analyse logs resulting from optiras texttrademark and we get insights into user s navigational patterns we claim that the results of our analysis can be used to support the redesign of decision support systems in order to address specific agricultural users characteristics modelling user behaviour agricultural users decision support systems process mining cultivar selection,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
39,0,"Towards comprehensive support for organizational mining Process mining has emerged as a way to analyze processes based on the event logs of the systems that support them. Today's information systems (e.g., ERP systems) log all kinds of events. Moreover, also embedded systems (e.g., medical equipment, copiers, and other high-tech systems) start producing detailed event logs. The omnipresence of event logs is an important enabler for process mining. The primary goal of process mining is to extract knowledge from these logs and use it for a detailed analysis of reality. Lion's share of the efforts in this domain has been devoted to control-flow discovery. Many algorithms have been proposed to construct a process model based on an analysis of the event sequences observed in the log. As a result, other aspects have been neglected, e.g., the organizational setting and interactions among coworkers. Therefore, we focus on organizational mining. We will present techniques to discover organizational models and social networks and show how these models can assist in improving the underlying processes. To do this, we present new process mining techniques but also use existing techniques in an innovative manner. The approach has been implemented in the context of the ProM framework and has been applied in various case studies. In this paper, we demonstrate the applicability of our techniques by analyzing the logs of a municipality in the Netherlands. Process mining, Social network analysis, Business process management, Workflow management, Data mining, Petri nets",towards comprehensive support for organizational mining process mining has emerged as a way to analyze processes based on the event logs of the systems that support them today s information systems e g erp systems log all kinds of events moreover also embedded systems e g medical equipment copiers and other high tech systems start producing detailed event logs the omnipresence of event logs is an important enabler for process mining the primary goal of process mining is to extract knowledge from these logs and use it for a detailed analysis of reality lion s share of the efforts in this domain has been devoted to control flow discovery many algorithms have been proposed to construct a process model based on an analysis of the event sequences observed in the log as a result other aspects have been neglected e g the organizational setting and interactions among coworkers therefore we focus on organizational mining we will present techniques to discover organizational models and social networks and show how these models can assist in improving the underlying processes to do this we present new process mining techniques but also use existing techniques in an innovative manner the approach has been implemented in the context of the prom framework and has been applied in various case studies in this paper we demonstrate the applicability of our techniques by analyzing the logs of a municipality in the netherlands process mining social network analysis business process management workflow management data mining petri nets,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
40,0,"Mining business process variants: Challenges, scenarios, algorithms During the last years a new generation of process-aware information systems has emerged, which enables process model configurations at buildtime as well as process instance changes during runtime. Respective model adaptations result in a large number of model variants that are derived from the same process model, but slightly differ in structure. Generally, such model variants are expensive to configure and maintain. In this paper we address two scenarios for learning from process model adaptations and for discovering a reference model out of which the variants can be configured with minimum efforts. The first one is characterized by a reference process model and a collection of related process variants. The goal is to improve the original reference process model such that it fits better to the variant models. The second scenario comprises a collection of process variants, while the original reference model is unknown; i.e., the goal is to ``merge'' these variants into a new reference process model. We suggest two algorithms that are applicable in both scenarios, but have their pros and cons. We provide a systematic comparison of the two algorithms and further contrast them with conventional process mining techniques. Comparison results indicate good performance of our algorithms and also show that specific techniques are needed for learning from process configurations and adaptations. Finally, we provide results from a case study in automotive industry in which we successfully applied our algorithms. Process mining, Process configuration, Process change, Process variant",mining business process variants challenges scenarios algorithms during the last years a new generation of process aware information systems has emerged which enables process model configurations at buildtime as well as process instance changes during runtime respective model adaptations result in a large number of model variants that are derived from the same process model but slightly differ in structure generally such model variants are expensive to configure and maintain in this paper we address two scenarios for learning from process model adaptations and for discovering a reference model out of which the variants can be configured with minimum efforts the first one is characterized by a reference process model and a collection of related process variants the goal is to improve the original reference process model such that it fits better to the variant models the second scenario comprises a collection of process variants while the original reference model is unknown i e the goal is to merge these variants into a new reference process model we suggest two algorithms that are applicable in both scenarios but have their pros and cons we provide a systematic comparison of the two algorithms and further contrast them with conventional process mining techniques comparison results indicate good performance of our algorithms and also show that specific techniques are needed for learning from process configurations and adaptations finally we provide results from a case study in automotive industry in which we successfully applied our algorithms process mining process configuration process change process variant,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
41,0,"Mining usage scenarios in business processes: Outlier-aware discovery and run-time prediction A prominent goal of process mining is to build automatically a model explaining all the episodes recorded in the log of some transactional system. Whenever the process to be mined is complex and highly-flexible, however, equipping all the traces with just one model might lead to mixing different usage scenarios, thereby resulting in a spaghetti-like process description. This is, in fact, often circumvented by preliminarily applying clustering methods on the process log in order to identify all its hidden variants. In this paper, two relevant problems that arise in the context of applying such methods are addressed, which have received little attention so far: (i) making the clustering aware of outlier traces, and (ii) finding predictive models for clustering results. The first issue impacts on the effectiveness of clustering algorithms, which can indeed be led to confuse real process variants with exceptional behavior or malfunctions. The second issue instead concerns the opportunity of predicting the behavioral class of future process instances, by taking advantage of context-dependent ``non-structural'' data (e.g., activity executors, parameter values). The paper formalizes and analyzes these two issues and illustrates various mining algorithms to face them. All the algorithms have been implemented and integrated into a system prototype, which has been thoroughly validated over two real-life application scenarios. Business processes, Process mining, Clustering, Decision trees",mining usage scenarios in business processes outlier aware discovery and run time prediction a prominent goal of process mining is to build automatically a model explaining all the episodes recorded in the log of some transactional system whenever the process to be mined is complex and highly flexible however equipping all the traces with just one model might lead to mixing different usage scenarios thereby resulting in a spaghetti like process description this is in fact often circumvented by preliminarily applying clustering methods on the process log in order to identify all its hidden variants in this paper two relevant problems that arise in the context of applying such methods are addressed which have received little attention so far i making the clustering aware of outlier traces and ii finding predictive models for clustering results the first issue impacts on the effectiveness of clustering algorithms which can indeed be led to confuse real process variants with exceptional behavior or malfunctions the second issue instead concerns the opportunity of predicting the behavioral class of future process instances by taking advantage of context dependent non structural data e g activity executors parameter values the paper formalizes and analyzes these two issues and illustrates various mining algorithms to face them all the algorithms have been implemented and integrated into a system prototype which has been thoroughly validated over two real life application scenarios business processes process mining clustering decision trees,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
42,0,"Process diagnostics using trace alignment: Opportunities, issues, and challenges Business processes leave trails in a variety of data sources (e.g., audit trails, databases, and transaction logs). Hence, every process instance can be described by a trace, i.e., a sequence of events. Process mining techniques are able to extract knowledge from such traces and provide a welcome extension to the repertoire of business process analysis techniques. Recently, process mining techniques have been adopted in various commercial BPM systems (e.g., BPM|one, Futura Reflect, ARIS PPM, Fujitsu Interstage, Businesscape, Iontas PDF, and QPR PA). Unfortunately, traditional process discovery algorithms have problems dealing with less structured processes. The resulting models are difficult to comprehend or even misleading. Therefore, we propose a new approach based on trace alignment. The goal is to align traces in such a way that event logs can be explored easily. Trace alignment can be used to explore the process in the early stages of analysis and to answer specific questions in later stages of analysis. Hence, it complements existing process mining techniques focusing on discovery and conformance checking. The proposed techniques have been implemented as plugins in the ProM framework. We report the results of trace alignment on one synthetic and two real-life event logs, and show that trace alignment has significant promise in process diagnostic efforts. Diagnostics, Conformance, Alignment, Execution patterns, Process mining",process diagnostics using trace alignment opportunities issues and challenges business processes leave trails in a variety of data sources e g audit trails databases and transaction logs hence every process instance can be described by a trace i e a sequence of events process mining techniques are able to extract knowledge from such traces and provide a welcome extension to the repertoire of business process analysis techniques recently process mining techniques have been adopted in various commercial bpm systems e g bpm one futura reflect aris ppm fujitsu interstage businesscape iontas pdf and qpr pa unfortunately traditional process discovery algorithms have problems dealing with less structured processes the resulting models are difficult to comprehend or even misleading therefore we propose a new approach based on trace alignment the goal is to align traces in such a way that event logs can be explored easily trace alignment can be used to explore the process in the early stages of analysis and to answer specific questions in later stages of analysis hence it complements existing process mining techniques focusing on discovery and conformance checking the proposed techniques have been implemented as plugins in the prom framework we report the results of trace alignment on one synthetic and two real life event logs and show that trace alignment has significant promise in process diagnostic efforts diagnostics conformance alignment execution patterns process mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
43,0,"Discovering Reference Process Models in the Context of BPM Projects This paper is related to process mining, specifically the processes' discovery. Our goal, through this research work, is to build an approach that extracts a reference model, modeled in BPMN language, from the event logs related to different processes, based on the algorithm α. We also aim to make the configuration of the extracted process models in BPMN language. So, we developed a plug-in in ProM environment. We tested this plug-in by using test cases for which preliminary results are encouraging. Process Mining, Process Discovery, BPMN, event logs, ProM.",discovering reference process models in the context of bpm projects this paper is related to process mining specifically the processes discovery our goal through this research work is to build an approach that extracts a reference model modeled in bpmn language from the event logs related to different processes based on the algorithm we also aim to make the configuration of the extracted process models in bpmn language so we developed a plug in in prom environment we tested this plug in by using test cases for which preliminary results are encouraging process mining process discovery bpmn event logs prom,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
44,0,"A Semantic Rule-based Approach Supported by Process Mining for Personalised Adaptive Learning Currently, automated learning systems are widely used for educational and training purposes within various organisations including, schools, universities and further education centres. There has been a big gap between the extraction of useful patterns from data sources to knowledge, as it is crucial that data is made valid, novel, potentially useful and understandable. To meet the needs of intended users, there is requirement for learning systems to embody technologies that support learners in achieving their learning goals and this process don't happen automatically. This paper propose a novel approach for automated learning that is capable of detecting changing trends in learning behaviours and abilities through the use of process mining techniques. The goal is to discover user interaction patterns within learning processes, and respond by making decisions based on adaptive rules centred on captured user profiles. The approach applies semantic annotation of activity logs within the learning process in order to discover patterns automatically by means of semantic reasoning. Therefore, our proposed approach is grounded on Semantic Modelling and Process Mining techniques. To this end, it is possible to apply effective reasoning methods to make inferences over a Learning Process Knowledge-Base that leads to automated discovery of learning patterns or behaviour. process model, semantic rules, process mining, user profile, learning behaviour, event logs",a semantic rule based approach supported by process mining for personalised adaptive learning currently automated learning systems are widely used for educational and training purposes within various organisations including schools universities and further education centres there has been a big gap between the extraction of useful patterns from data sources to knowledge as it is crucial that data is made valid novel potentially useful and understandable to meet the needs of intended users there is requirement for learning systems to embody technologies that support learners in achieving their learning goals and this process don t happen automatically this paper propose a novel approach for automated learning that is capable of detecting changing trends in learning behaviours and abilities through the use of process mining techniques the goal is to discover user interaction patterns within learning processes and respond by making decisions based on adaptive rules centred on captured user profiles the approach applies semantic annotation of activity logs within the learning process in order to discover patterns automatically by means of semantic reasoning therefore our proposed approach is grounded on semantic modelling and process mining techniques to this end it is possible to apply effective reasoning methods to make inferences over a learning process knowledge base that leads to automated discovery of learning patterns or behaviour process model semantic rules process mining user profile learning behaviour event logs,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
45,0,"Conformance checking and diagnosis for declarative business process models in data-aware scenarios A business process (BP) consists of a set of activities which are performed in coordination in an organizational and technical environment and which jointly realize a business goal. In such context, BP management (BPM) can be seen as supporting BPs using methods, techniques, and software in order to design, enact, control, and analyze operational processes involving humans, organizations, applications, and other sources of information. Since the accurate management of BPs is receiving increasing attention, conformance checking, i.e., verifying whether the observed behavior matches a modelled behavior, is becoming more and more critical. Moreover, declarative languages are more frequently used to provide an increased flexibility. However, whereas there exist solid conformance checking techniques for imperative models, little work has been conducted for declarative models. Furthermore, only control-flow perspective is usually considered although other perspectives (e.g., data) are crucial. In addition, most approaches exclusively check the conformance without providing any related diagnostics. To enhance the accurate management of flexible BPs, this work presents a constraint-based approach for conformance checking over declarative BP models (including both control-flow and data perspectives). In addition, two constraint-based proposals for providing related diagnosis are detailed. To demonstrate both the effectiveness and the efficiency of the proposed approaches, the analysis of different performance measures related to a wide diversified set of test models of varying complexity has been performed. Business process management, Process mining, Conformance checking, Diagnosis, Declarative business process models, Constraint programming",conformance checking and diagnosis for declarative business process models in data aware scenarios a business process bp consists of a set of activities which are performed in coordination in an organizational and technical environment and which jointly realize a business goal in such context bp management bpm can be seen as supporting bps using methods techniques and software in order to design enact control and analyze operational processes involving humans organizations applications and other sources of information since the accurate management of bps is receiving increasing attention conformance checking i e verifying whether the observed behavior matches a modelled behavior is becoming more and more critical moreover declarative languages are more frequently used to provide an increased flexibility however whereas there exist solid conformance checking techniques for imperative models little work has been conducted for declarative models furthermore only control flow perspective is usually considered although other perspectives e g data are crucial in addition most approaches exclusively check the conformance without providing any related diagnostics to enhance the accurate management of flexible bps this work presents a constraint based approach for conformance checking over declarative bp models including both control flow and data perspectives in addition two constraint based proposals for providing related diagnosis are detailed to demonstrate both the effectiveness and the efficiency of the proposed approaches the analysis of different performance measures related to a wide diversified set of test models of varying complexity has been performed business process management process mining conformance checking diagnosis declarative business process models constraint programming,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
46,0,"Improving structural medical process comparison by exploiting domain knowledge and mined information Objectives Process model comparison and similar process retrieval is a key issue to be addressed in many real-world situations, and a particularly relevant one in medical applications, where similarity quantification can be exploited to accomplish goals such as conformance checking, local process adaptation analysis, and hospital ranking. In this paper, we present a framework that allows the user to: (i) mine the actual process model from a database of process execution traces available at a given hospital; and (ii) compare (mined) process models. The tool is currently being applied in stroke management. Methods Our framework relies on process mining to extract process-related information (i.e., process models) from data. As for process comparison, we have modified a state-of-the-art structural similarity metric by exploiting: (i) domain knowledge; (ii) process mining outputs and statistical temporal information. These changes were meant to make the metric more suited to the medical domain. Results Experimental results showed that our metric outperforms the original one, and generated output closer than that provided by a stroke management expert. In particular, our metric correctly rated 11 out of 15 mined hospital models with respect to a given query. On the other hand, the original metric correctly rated only 7 out of 15 models. The experiments also showed that the framework can support stroke management experts in answering key research questions: in particular, average patient improvement decreased as the distance (according to our metric) from the top level hospital process model increased. Conclusions The paper shows that process mining and process comparison, through a similarity metric tailored to medical applications, can be applied successfully to clinical data to gain a better understanding of different medical processes adopted by different hospitals, and of their impact on clinical outcomes. In the future, we plan to make our metric even more general and efficient, by explicitly considering various methodological and technological extensions. We will also test the framework in different domains. Process mining and comparison, Graph edit distance, Stroke management",improving structural medical process comparison by exploiting domain knowledge and mined information objectives process model comparison and similar process retrieval is a key issue to be addressed in many real world situations and a particularly relevant one in medical applications where similarity quantification can be exploited to accomplish goals such as conformance checking local process adaptation analysis and hospital ranking in this paper we present a framework that allows the user to i mine the actual process model from a database of process execution traces available at a given hospital and ii compare mined process models the tool is currently being applied in stroke management methods our framework relies on process mining to extract process related information i e process models from data as for process comparison we have modified a state of the art structural similarity metric by exploiting i domain knowledge ii process mining outputs and statistical temporal information these changes were meant to make the metric more suited to the medical domain results experimental results showed that our metric outperforms the original one and generated output closer than that provided by a stroke management expert in particular our metric correctly rated 11 out of 15 mined hospital models with respect to a given query on the other hand the original metric correctly rated only 7 out of 15 models the experiments also showed that the framework can support stroke management experts in answering key research questions in particular average patient improvement decreased as the distance according to our metric from the top level hospital process model increased conclusions the paper shows that process mining and process comparison through a similarity metric tailored to medical applications can be applied successfully to clinical data to gain a better understanding of different medical processes adopted by different hospitals and of their impact on clinical outcomes in the future we plan to make our metric even more general and efficient by explicitly considering various methodological and technological extensions we will also test the framework in different domains process mining and comparison graph edit distance stroke management,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
47,0,"PETRA: Process Evolution using a TRAce-based system on a maintenance platform To meet increasing needs in the field of maintenance, we studied the dynamic aspect of process and services on a maintenance platform, a major challenge in process mining and knowledge engineering. Hence, we propose a dynamic experience feedback approach to exploit maintenance process behaviors in real execution of the maintenance platform. An active learning process exploiting event log is introduced by taking into account the dynamic aspect of knowledge using trace engineering. Our proposal makes explicit the underlying knowledge of platform users by means of a trace-based system called ``PETRA''. The goal of this system is to extract new knowledge rules about transitions and activities in maintenance processes from previous platform executions as well as its user (i.e. maintenance operators) interactions. While following a Knowledge Traces Discovery process and handling the maintenance ontology IMAMO, ``PETRA'' is composed of three main subsystems: tracking, learning and knowledge capitalization. The capitalized rules are shared in the platform knowledge base in order to be reused in future process executions. The feasibility of this method is proven through concrete use cases involving four maintenance processes and their simulation. Trace-based systems, Process extension, Process mining, Experience reuse, s-Maintenance platform",petra process evolution using a trace based system on a maintenance platform to meet increasing needs in the field of maintenance we studied the dynamic aspect of process and services on a maintenance platform a major challenge in process mining and knowledge engineering hence we propose a dynamic experience feedback approach to exploit maintenance process behaviors in real execution of the maintenance platform an active learning process exploiting event log is introduced by taking into account the dynamic aspect of knowledge using trace engineering our proposal makes explicit the underlying knowledge of platform users by means of a trace based system called petra the goal of this system is to extract new knowledge rules about transitions and activities in maintenance processes from previous platform executions as well as its user i e maintenance operators interactions while following a knowledge traces discovery process and handling the maintenance ontology imamo petra is composed of three main subsystems tracking learning and knowledge capitalization the capitalized rules are shared in the platform knowledge base in order to be reused in future process executions the feasibility of this method is proven through concrete use cases involving four maintenance processes and their simulation trace based systems process extension process mining experience reuse s maintenance platform,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
48,0,"Analysis of Customer Fulfilment with Process Mining: A Case Study in a Telecommunication Company This paper presents results of process mining implementation in a characteristically unstructured customer fulfilment process in a real Telecommunication Company. The aim of process mining implementation is firstly to discover the typical customer fulfilment business process. It is also aimed at assessing the current rate of completed customer fulfilment, the typical component required for the process and the lead time for different types of customer requests. The steps to achieve the goals are to prepare, extract the data and construct the event log from the company's in house built Customer Relationship Management systems. The event log is then processed using Disco and PROM tools. The complete event log when model with Disco results in a Spaghetti-like process model with 673 different variants. In order to identify typical process, the log is filtered to include only business variants with 1% case occurrence of the total case. This enables the identification of 18 typical business variants, which differ based on the order requested, sequence of activities and occurrence of Return Work Order. Based on the typical variants, the components required to fulfil a certain order are identified. Another important findings are the fact that the completion rate is very low (only 8%). This may due to the fact that the issues faced by the field officer in processing the order and the resolution are either recorded manually or in a different systems. Finally, findings from this study can be used by the company to improve their current business process. It also stressed out the importance of resolving data integration issues in implementation of process mining in real cases. Process Mining, Unstructured Process, Customer Order Fulfilment",analysis of customer fulfilment with process mining a case study in a telecommunication company this paper presents results of process mining implementation in a characteristically unstructured customer fulfilment process in a real telecommunication company the aim of process mining implementation is firstly to discover the typical customer fulfilment business process it is also aimed at assessing the current rate of completed customer fulfilment the typical component required for the process and the lead time for different types of customer requests the steps to achieve the goals are to prepare extract the data and construct the event log from the company s in house built customer relationship management systems the event log is then processed using disco and prom tools the complete event log when model with disco results in a spaghetti like process model with 673 different variants in order to identify typical process the log is filtered to include only business variants with 1 case occurrence of the total case this enables the identification of 18 typical business variants which differ based on the order requested sequence of activities and occurrence of return work order based on the typical variants the components required to fulfil a certain order are identified another important findings are the fact that the completion rate is very low only 8 this may due to the fact that the issues faced by the field officer in processing the order and the resolution are either recorded manually or in a different systems finally findings from this study can be used by the company to improve their current business process it also stressed out the importance of resolving data integration issues in implementation of process mining in real cases process mining unstructured process customer order fulfilment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
49,0,"Log-based Evaluation of Label Splits for Process Models Process mining techniques aim to extract insights in processes from event logs. One of the challenges in process mining is identifying interesting and meaningful event labels that contribute to a better understanding of the process. Our application area is mining data from smart homes for elderly, where the ultimate goal is to signal deviations from usual behavior and provide timely recommendations in order to extend the period of independent living. Extracting individual process models showing user behavior is an important instrument in achieving this goal. However, the interpretation of sensor data at an appropriate abstraction level is not straightforward. For example, a motion sensor in a bedroom can be triggered by tossing and turning in bed or by getting up. We try to derive the actual activity depending on the context (time, previous events, etc.). In this paper we introduce the notion of label refinements, which links more abstract event descriptions with their more refined counterparts. We present a statistical evaluation method to determine the usefulness of a label refinement for a given event log from a process perspective. Based on data from smart homes, we show how our statistical evaluation method for label refinements can be used in practice. Our method was able to select two label refinements out of a set of candidate label refinements that both had a positive effect on model precision. Label refinement, Process Mining, Sensor Networks",log based evaluation of label splits for process models process mining techniques aim to extract insights in processes from event logs one of the challenges in process mining is identifying interesting and meaningful event labels that contribute to a better understanding of the process our application area is mining data from smart homes for elderly where the ultimate goal is to signal deviations from usual behavior and provide timely recommendations in order to extend the period of independent living extracting individual process models showing user behavior is an important instrument in achieving this goal however the interpretation of sensor data at an appropriate abstraction level is not straightforward for example a motion sensor in a bedroom can be triggered by tossing and turning in bed or by getting up we try to derive the actual activity depending on the context time previous events etc in this paper we introduce the notion of label refinements which links more abstract event descriptions with their more refined counterparts we present a statistical evaluation method to determine the usefulness of a label refinement for a given event log from a process perspective based on data from smart homes we show how our statistical evaluation method for label refinements can be used in practice our method was able to select two label refinements out of a set of candidate label refinements that both had a positive effect on model precision label refinement process mining sensor networks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
50,0,"Software cybernetics in BPM: Modeling software behavior as feedback for evolution by a novel discovery method based on augmented event logs Business Process Management (BPM) is a quickly developing management theory in recent years. The goal of BPM is to improve corporate performance by managing and optimizing the businesses process in and among enterprises. The goal is easier to achieve with the closed-loop feedback mechanism from business process execution to redesign in BPM life cycle, where the business process itself and the set of activities in BPM are viewed as a controlled object and a controller respectively. In this feedback control system, process mining plays an important role in generating feedback of process execution for redesign. However, the existing discovery methods cannot mine certain special structures from execution logs (e.g., implicit dependency, implicit place and short loops) correctly and their mining efficiencies cannot meet the requirements of online process mining. In this paper, we propose a novel discovery method to overcome these challenges based on a kind of augmented event log that will also bring new research directions for process discovery. A case study is presented for introducing how the mined model can be used in business process evolution. Results of experiments are described to show the improvements of the proposed algorithm compared with others. Software cybernetics, Process discovery, Petri nets",software cybernetics in bpm modeling software behavior as feedback for evolution by a novel discovery method based on augmented event logs business process management bpm is a quickly developing management theory in recent years the goal of bpm is to improve corporate performance by managing and optimizing the businesses process in and among enterprises the goal is easier to achieve with the closed loop feedback mechanism from business process execution to redesign in bpm life cycle where the business process itself and the set of activities in bpm are viewed as a controlled object and a controller respectively in this feedback control system process mining plays an important role in generating feedback of process execution for redesign however the existing discovery methods cannot mine certain special structures from execution logs e g implicit dependency implicit place and short loops correctly and their mining efficiencies cannot meet the requirements of online process mining in this paper we propose a novel discovery method to overcome these challenges based on a kind of augmented event log that will also bring new research directions for process discovery a case study is presented for introducing how the mined model can be used in business process evolution results of experiments are described to show the improvements of the proposed algorithm compared with others software cybernetics process discovery petri nets,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
51,0,"Analyzing Business Processes by Automatically Detecting KPI Thresholds This paper describes a method for automatically detecting key performance indicator (KPI) thresholds by dividing and aggregating process instances on the basis of differences in process models. The thresholds can be used as an analysis axis of data exploration to investigate process models that are discovered from huge logs. The proposed method enables users to minimize the time needed to detect KPI thresholds through trial and error. We applied the method to real-life logs and experiment results showed that thresholds were detected for two types of KPIs. Although one type did not correlate with process patterns, the other highly correlated with them. Such findings are usually obtained from the domain knowledge of business users and analysis results acquired by data analysts with technical expertise. However, with our approach the thresholds can be detected automatically and this helps to expand process analysis for end users. business data processing, data handling, business process analysis, KPI threshold detection, key performance indicator, data exploration, Monitoring, Analytical models, Context, Data models, Engines, Insurance, process analysis, business monitoring, process discovery, KPI, graph edit distance",analyzing business processes by automatically detecting kpi thresholds this paper describes a method for automatically detecting key performance indicator kpi thresholds by dividing and aggregating process instances on the basis of differences in process models the thresholds can be used as an analysis axis of data exploration to investigate process models that are discovered from huge logs the proposed method enables users to minimize the time needed to detect kpi thresholds through trial and error we applied the method to real life logs and experiment results showed that thresholds were detected for two types of kpis although one type did not correlate with process patterns the other highly correlated with them such findings are usually obtained from the domain knowledge of business users and analysis results acquired by data analysts with technical expertise however with our approach the thresholds can be detected automatically and this helps to expand process analysis for end users business data processing data handling business process analysis kpi threshold detection key performance indicator data exploration monitoring analytical models context data models engines insurance process analysis business monitoring process discovery kpi graph edit distance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
52,0,"Process mining for recommender strategies support in news media The strategic transition of media organizations to personalized information delivery has urged the need for richer methods to analyze the customers. Though useful in supporting the creation of recommender strategies, the current data mining techniques create complex models requiring often an understanding of techniques in order to interpret the results. This situation together with the recommender technologies deluge and the particularities of the news industry pose challenges to the news organization in making decisions about the most suitable strategy. Therefore, we propose process mining as a high-level, end-to-end solution to provide insights into the consumers' behavior and content dynamics. Specifically, we explore if it allows news organizations to analyze independently and effectively their data in order to support them in defining recommender strategies. The solution was implemented in a case study with the third largest news provider in Norway and yielded preliminary positive results. To our knowledge, this is the first attempt to apply a process mining methodology and adapt the techniques to support media industry with the recommender strategies. data analysis, data mining, recommender systems, strategic transition, media organizations, personalized information delivery, recommender strategies, data mining, recommender technologies, news organization, consumer behavior, content dynamics, data analysis, Norway, process mining, media industry, Data mining, Media, Recommender systems, Organizations, Biological system modeling, Hidden Markov models, Knowledge based systems, process mining, inferred intentional process models, behavioral analysis, recommender strategies, process mining methodology, news media case study",process mining for recommender strategies support in news media the strategic transition of media organizations to personalized information delivery has urged the need for richer methods to analyze the customers though useful in supporting the creation of recommender strategies the current data mining techniques create complex models requiring often an understanding of techniques in order to interpret the results this situation together with the recommender technologies deluge and the particularities of the news industry pose challenges to the news organization in making decisions about the most suitable strategy therefore we propose process mining as a high level end to end solution to provide insights into the consumers behavior and content dynamics specifically we explore if it allows news organizations to analyze independently and effectively their data in order to support them in defining recommender strategies the solution was implemented in a case study with the third largest news provider in norway and yielded preliminary positive results to our knowledge this is the first attempt to apply a process mining methodology and adapt the techniques to support media industry with the recommender strategies data analysis data mining recommender systems strategic transition media organizations personalized information delivery recommender strategies data mining recommender technologies news organization consumer behavior content dynamics data analysis norway process mining media industry data mining media recommender systems organizations biological system modeling hidden markov models knowledge based systems process mining inferred intentional process models behavioral analysis recommender strategies process mining methodology news media case study,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
53,0,"Traffic prediction in wireless mesh networks using process mining algorithms Prediction of the traffic flow in particular systems will expedite discovering of an optimal path for packet transmitting in dynamic wireless networks. The main goal is to predict traffic overload while changing a network topology. Machine learning techniques and process mining enables prediction of the traffic produced by several moving nodes. Several related approaches are observed. The idea of process mining approach is proposed. data mining, learning (artificial intelligence), prediction theory, telecommunication computing, telecommunication network topology, telecommunication traffic, wireless mesh networks, wireless mesh networks, process mining algorithms, dynamic wireless networks, network topology, packet transmission, traffic prediction flow, traffic overload prediction, machine learning techniques, Algorithm design and analysis, Prediction algorithms, Heuristic algorithms, Network topology, Data mining, Object oriented modeling, Time series analysis, Wireless mesh networks, routing, process mining, traffic overload",traffic prediction in wireless mesh networks using process mining algorithms prediction of the traffic flow in particular systems will expedite discovering of an optimal path for packet transmitting in dynamic wireless networks the main goal is to predict traffic overload while changing a network topology machine learning techniques and process mining enables prediction of the traffic produced by several moving nodes several related approaches are observed the idea of process mining approach is proposed data mining learning artificial intelligence prediction theory telecommunication computing telecommunication network topology telecommunication traffic wireless mesh networks wireless mesh networks process mining algorithms dynamic wireless networks network topology packet transmission traffic prediction flow traffic overload prediction machine learning techniques algorithm design and analysis prediction algorithms heuristic algorithms network topology data mining object oriented modeling time series analysis wireless mesh networks routing process mining traffic overload,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
54,0,"Process Mining Approach Based on Partial Structures of Event Logs and Decision Tree Learning Process mining techniques are able to improve processes by extracting knowledge from event logs commonly available in today's information systems. In the area, it is important to verify whether business goals can be satisfied. LTL (Linear Temporal Logic) verification is an important means for checking the goals automatically and exhaustively. However, writing formal language like LTL is difficult, and the properties by which the user's intentions are not reflected sufficiently have bad influence on the verification results. Therefore, it is needed to help writing correct LTL formula for users who do not have sufficient domain knowledge and knowledge of mathematical logic. We propose an approach for goal achievement prediction based on decision tree learning. It is conducted focusing on partial structures represented as event order relations of each trace. The proposed technique is evaluated on a phone repair process log. business process re-engineering, data mining, decision trees, formal languages, learning (artificial intelligence), temporal logic, process mining approach, event logs partial structures, decision tree learning, knowledge extraction, business goals, LTL verification, linear temporal logic verification, formal language writing, LTL formula, goal achievement prediction, partial structures, trace event order relations, phone repair process log, Decision trees, Business, Feature extraction, Data mining, Training data, Prediction algorithms, business process management, process mining, requirements engineering, process aware information system, business constraints, linear temporal logic",process mining approach based on partial structures of event logs and decision tree learning process mining techniques are able to improve processes by extracting knowledge from event logs commonly available in today s information systems in the area it is important to verify whether business goals can be satisfied ltl linear temporal logic verification is an important means for checking the goals automatically and exhaustively however writing formal language like ltl is difficult and the properties by which the user s intentions are not reflected sufficiently have bad influence on the verification results therefore it is needed to help writing correct ltl formula for users who do not have sufficient domain knowledge and knowledge of mathematical logic we propose an approach for goal achievement prediction based on decision tree learning it is conducted focusing on partial structures represented as event order relations of each trace the proposed technique is evaluated on a phone repair process log business process re engineering data mining decision trees formal languages learning artificial intelligence temporal logic process mining approach event logs partial structures decision tree learning knowledge extraction business goals ltl verification linear temporal logic verification formal language writing ltl formula goal achievement prediction partial structures trace event order relations phone repair process log decision trees business feature extraction data mining training data prediction algorithms business process management process mining requirements engineering process aware information system business constraints linear temporal logic,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
55,0,"Towards a better assessment of event logs quality It is widely observed that the poor event logs quality poses a significant challenge to the process mining project both in terms of choice of process mining algorithms and in terms of the quality of the discovered process model. Therefore, it is important to control the quality of event logs prior to conducting a process mining analysis. In this paper, we propose a qualitative model which aims to assess the quality of event logs before applying process mining algorithms. Our ultimate goal is to give process mining practitioners an overview of the quality of event logs which can help to indicate whether the event log quality is good enough to proceed to process mining and in this case, to suggest both the needed preprocessing steps and the process mining algorithm that is most tailored under such a circumstance. The qualitative model has been evaluated using both artificial and real-life case studies. data mining, event logs quality, process mining project, process mining algorithms, process mining analysis, Data mining, Complexity theory, Measurement, Algorithm design and analysis, Software algorithms, Heuristic algorithms, Finite element analysis, event logs, process mining, process mining algorithms, qualitative model",towards a better assessment of event logs quality it is widely observed that the poor event logs quality poses a significant challenge to the process mining project both in terms of choice of process mining algorithms and in terms of the quality of the discovered process model therefore it is important to control the quality of event logs prior to conducting a process mining analysis in this paper we propose a qualitative model which aims to assess the quality of event logs before applying process mining algorithms our ultimate goal is to give process mining practitioners an overview of the quality of event logs which can help to indicate whether the event log quality is good enough to proceed to process mining and in this case to suggest both the needed preprocessing steps and the process mining algorithm that is most tailored under such a circumstance the qualitative model has been evaluated using both artificial and real life case studies data mining event logs quality process mining project process mining algorithms process mining analysis data mining complexity theory measurement algorithm design and analysis software algorithms heuristic algorithms finite element analysis event logs process mining process mining algorithms qualitative model,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
56,0,"Time performance analysis of medical treatment processes by using disco In this research we applied process mining techniques in order to analyze work processes of a healthcare system. The main objective of the study was to investigate the performance of a private hospital treatment processes in Bangkok based on the event logs. Being aware of the fact that currently healthcare systems of majority of hospitals worldwide are equipped with information systems, provided us a great opportunity to access large amounts of the medical data with the intention of the research and knowledge discovery purposes. In this paper, we emphasized on the ``Time Performance'' of the process instances of the collected event logs from different wards/sections of a hospital in order to better visualize and study the behavior of patients referring to the following sections/wards (as well as the hospital's administrators/personnel attending to each case) during the entire treatment processes. . The results showed that the treatment process with respect to the waiting time was too long between the wards ``Irradiation cystitis'' and ``Osteoradionecrosi'' sections allocating 7.8 waiting time to themselves. Subsequently, the findings of the research can be used in order to help the hospital administrators and managers to better understand the amount of waiting time spent between different treatment processes in such a way that they can improve the performance of handling patients' demands and needs in a more efficient, effective and timely manner, eventually leading to increased customer satisfaction and better performance. customer satisfaction, data mining, health care, medical information systems, patient treatment, time performance analysis, medical treatment process, process mining techniques, healthcare system, private hospital treatment process, Bangkok, event logs, hospital information systems, knowledge discovery, patient behavior, Osteoradionecrosi sections, irradiation cystitis sections, waiting time allocation, hospital administrators, hospital managers, patient handling, customer satisfaction, Disco, Hospitals, Radiation effects, Data models, Diseases, Open systems, Process Mining, time performance analysis, Disco Fluxicon, medical event log, hospital information systems",time performance analysis of medical treatment processes by using disco in this research we applied process mining techniques in order to analyze work processes of a healthcare system the main objective of the study was to investigate the performance of a private hospital treatment processes in bangkok based on the event logs being aware of the fact that currently healthcare systems of majority of hospitals worldwide are equipped with information systems provided us a great opportunity to access large amounts of the medical data with the intention of the research and knowledge discovery purposes in this paper we emphasized on the time performance of the process instances of the collected event logs from different wards sections of a hospital in order to better visualize and study the behavior of patients referring to the following sections wards as well as the hospital s administrators personnel attending to each case during the entire treatment processes the results showed that the treatment process with respect to the waiting time was too long between the wards irradiation cystitis and osteoradionecrosi sections allocating 7 8 waiting time to themselves subsequently the findings of the research can be used in order to help the hospital administrators and managers to better understand the amount of waiting time spent between different treatment processes in such a way that they can improve the performance of handling patients demands and needs in a more efficient effective and timely manner eventually leading to increased customer satisfaction and better performance customer satisfaction data mining health care medical information systems patient treatment time performance analysis medical treatment process process mining techniques healthcare system private hospital treatment process bangkok event logs hospital information systems knowledge discovery patient behavior osteoradionecrosi sections irradiation cystitis sections waiting time allocation hospital administrators hospital managers patient handling customer satisfaction disco hospitals radiation effects data models diseases open systems process mining time performance analysis disco fluxicon medical event log hospital information systems,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
57,0,"Goal-driven social business process management In recent years research on the integration of BPM and social software has tried to overcome the limitations of the traditional BPM approaches. The potential of Social BPM (SBPM) and social software towards the enhancement and advancement of the traditional BPM lifecycle have been argued. This paper aims to address gaps in social BPM research by working towards a goal-driven SBPM meta-model that seamlessly integrate the process design and enactment stages. We argue that this approach will lead to truly social driven process enactment environments. business data processing, social sciences computing, goal-driven social business process management, social software, social BPM, BPM lifecycle, goal-driven SBPM meta-model, process design, social driven process enactment environments, Software, Collaboration, Process design, Adaptation models, Runtime, Organizations, BPM, Social Software, Goal-Based Modeling, Social BPM, Process Discovery, Process Enactment",goal driven social business process management in recent years research on the integration of bpm and social software has tried to overcome the limitations of the traditional bpm approaches the potential of social bpm sbpm and social software towards the enhancement and advancement of the traditional bpm lifecycle have been argued this paper aims to address gaps in social bpm research by working towards a goal driven sbpm meta model that seamlessly integrate the process design and enactment stages we argue that this approach will lead to truly social driven process enactment environments business data processing social sciences computing goal driven social business process management social software social bpm bpm lifecycle goal driven sbpm meta model process design social driven process enactment environments software collaboration process design adaptation models runtime organizations bpm social software goal based modeling social bpm process discovery process enactment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
58,0,"On the Representational Bias in Process Mining Process mining serves a bridge between data mining and business process modeling. The goal is to extract process related knowledge from event data stored in information systems. One of the most challenging process mining tasks is process discovery, i.e., the automatic construction of process models from raw event logs. Today there are dozens of process discovery techniques generating process models using different notations (Petri nets, EPCs, BPMN, heuristic nets, etc.). This paper focuses on the representational bias used by these techniques. We will show that the choice of target model is very important for the discovery process itself. The representational bias should not be driven by the desired graphical representation but by the characteristics of the underlying processes and process discovery techniques. Therefore, we analyze the role of the representational bias in process mining. business process re-engineering, data mining, information systems, process mining, data mining, business process modeling, information systems, process discovery technique, representational bias, Data mining, Registers, Noise, Data models, Noise measurement, Organizations",on the representational bias in process mining process mining serves a bridge between data mining and business process modeling the goal is to extract process related knowledge from event data stored in information systems one of the most challenging process mining tasks is process discovery i e the automatic construction of process models from raw event logs today there are dozens of process discovery techniques generating process models using different notations petri nets epcs bpmn heuristic nets etc this paper focuses on the representational bias used by these techniques we will show that the choice of target model is very important for the discovery process itself the representational bias should not be driven by the desired graphical representation but by the characteristics of the underlying processes and process discovery techniques therefore we analyze the role of the representational bias in process mining business process re engineering data mining information systems process mining data mining business process modeling information systems process discovery technique representational bias data mining registers noise data models noise measurement organizations,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
59,0,"Activity failure prediction based on process mining Based on the state of the art of process mining, we can conclude that quality characteristics (failure rate metrics or loops) are poorly represented or absent in most predictive models that can be found in the literature. The main goal of this present research work is to analyze how to learn prediction model defining failure as response variable. A model of this type can be used for active real-time-controlling (e. g. through the reassignment of workflow activities based on prediction results) or for the automated support of redesign (i.e., prediction results are transformed in software requirements used to implement process improvements). The proposed methodology is based on the application of a data mining process because the objective of this work can be considered as a data mining goal. business data processing, data mining, system recovery, process mining, activity failure prediction, quality characteristics, failure rate metrics, predictive models, response variable, active real-time-controlling, workflow activities, automated support, software requirements, process improvements, data mining goal, business process management, BPM, Data mining, Business, Predictive models, Data models, Analytical models, Process control, Measurement, Process mining, Workflow management software, Business Process Management, Data mining, Supervised learning",activity failure prediction based on process mining based on the state of the art of process mining we can conclude that quality characteristics failure rate metrics or loops are poorly represented or absent in most predictive models that can be found in the literature the main goal of this present research work is to analyze how to learn prediction model defining failure as response variable a model of this type can be used for active real time controlling e g through the reassignment of workflow activities based on prediction results or for the automated support of redesign i e prediction results are transformed in software requirements used to implement process improvements the proposed methodology is based on the application of a data mining process because the objective of this work can be considered as a data mining goal business data processing data mining system recovery process mining activity failure prediction quality characteristics failure rate metrics predictive models response variable active real time controlling workflow activities automated support software requirements process improvements data mining goal business process management bpm data mining business predictive models data models analytical models process control measurement process mining workflow management software business process management data mining supervised learning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
60,0,"Towards a distributed computation platform tailored for educational process discovery and analysis Given the ever changing needs of the job markets, education and training centers are increasingly held accountable for student success. Therefore, education and training centers have to focus on ways to streamline their offers and educational processes in order to achieve the highest level of quality in curriculum contents and managerial decisions. Educational process mining is an emerging field in the educational data mining (EPM) discipline, concerned with developing methods to discover, analyze and provide a visual representation of complete educational processes. In this paper, we present our distributed computation platform, under construction, which allows different education centers and institutions to load their data and access to advanced data mining and process mining services. To achieve this, we present also a comparative study of the different clustering techniques developed in the context of process mining to partition efficiently educational traces. Our goal is to find the best strategy for distributing heavy analysis computations on many processing nodes of our platform. data mining, distributed processing, educational computing, educational institutions, pattern clustering, distributed computation platform, educational process discovery, educational process analysis, student success, education center, training center, curriculum contents, managerial decisions, educational process mining, educational data mining, EPM, visual representation, education institutions, clustering techniques, Analytical models, Context, PROM, Training, Portals",towards a distributed computation platform tailored for educational process discovery and analysis given the ever changing needs of the job markets education and training centers are increasingly held accountable for student success therefore education and training centers have to focus on ways to streamline their offers and educational processes in order to achieve the highest level of quality in curriculum contents and managerial decisions educational process mining is an emerging field in the educational data mining epm discipline concerned with developing methods to discover analyze and provide a visual representation of complete educational processes in this paper we present our distributed computation platform under construction which allows different education centers and institutions to load their data and access to advanced data mining and process mining services to achieve this we present also a comparative study of the different clustering techniques developed in the context of process mining to partition efficiently educational traces our goal is to find the best strategy for distributing heavy analysis computations on many processing nodes of our platform data mining distributed processing educational computing educational institutions pattern clustering distributed computation platform educational process discovery educational process analysis student success education center training center curriculum contents managerial decisions educational process mining educational data mining epm visual representation education institutions clustering techniques analytical models context prom training portals,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
61,0,"Semantic Process Mining Towards Discovery and Enhancement of Learning Model Analysis Process mining algorithms use event logs to learn and reason about processes by technically coupling event history data and process models. During the execution of a learning process, several events occur which are of interest and/or necessary for completing and achieving a learning goal. The work in this paper describes a Semantic Process Mining approach directed towards automated learning. The proposed approach involves the extraction of process history data from learning execution environments, which is then followed by submitting the resulting eXtensible Event Streams (XES) and Mining eXtensible Markup Language (MXML) format to the process analytics environment for mining and further analysis. The XES and MXML data logs are enriched by using Semantic Annotations that references concepts in an Ontology specifically designed for representing learning processes. This involves the identification and modelling of data about different users. The approach focuses on augmenting information values of the resulting model based on individual learner profiles. A series of validation experiments were conducted in order to prove how Semantic Process Mining can be utilized to address the problem of analyzing concepts and relationships amongst learning objects, which also aid in discovering new and enhancement of existing learning processes. To this end, we demonstrate how data from learning processes can be extracted, semantically prepared, and transformed into mining executable formats for improved analysis. data mining, learning (artificial intelligence), ontologies (artificial intelligence), XML, semantic process mining algorithm, learning model analysis, automated learning, resulting extensible event streams, XES, mining extensible markup language, MXML, semantic annotations, ontology, Data mining, Semantics, Cognition, Ontologies, Data models, Context, History, process model, learning process, semantic annotation, ontology, process mining, event logs",semantic process mining towards discovery and enhancement of learning model analysis process mining algorithms use event logs to learn and reason about processes by technically coupling event history data and process models during the execution of a learning process several events occur which are of interest and or necessary for completing and achieving a learning goal the work in this paper describes a semantic process mining approach directed towards automated learning the proposed approach involves the extraction of process history data from learning execution environments which is then followed by submitting the resulting extensible event streams xes and mining extensible markup language mxml format to the process analytics environment for mining and further analysis the xes and mxml data logs are enriched by using semantic annotations that references concepts in an ontology specifically designed for representing learning processes this involves the identification and modelling of data about different users the approach focuses on augmenting information values of the resulting model based on individual learner profiles a series of validation experiments were conducted in order to prove how semantic process mining can be utilized to address the problem of analyzing concepts and relationships amongst learning objects which also aid in discovering new and enhancement of existing learning processes to this end we demonstrate how data from learning processes can be extracted semantically prepared and transformed into mining executable formats for improved analysis data mining learning artificial intelligence ontologies artificial intelligence xml semantic process mining algorithm learning model analysis automated learning resulting extensible event streams xes mining extensible markup language mxml semantic annotations ontology data mining semantics cognition ontologies data models context history process model learning process semantic annotation ontology process mining event logs,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
62,0,"Application of process mining techniques for innovation analysis and support Innovation processes can be defined as sets of goal-driven activities, deeply influenced by human experience and behavior. With respect to traditional, structured operational processes, they present an high degree of uncertainty and heterogeneity with little or no structure. As a consequence, traditional business intelligence tools are mostly not suitable for innovation processes. Although the innovation promotion has become one of the hottest topic in business economy in last decades and its importance in organization growth is widely recognized, currently there are not proposal in Literature for the automatic analysis of innovation activities performed by an organization. Our research is hence aimed to investigate such activities and their corresponding innovation processes, by taking into account the way in which they are really performed in organization daily job. In the present work we briefly sketch the main issues related to innovation activities and their automatic support, firstly considering the current state of the art in Literature and then describing the main ideas of our proposal. competitive intelligence, data mining, economics, innovation management, process mining techniques, innovation analysis, innovation process, goal-driven activities, business intelligence, business economy, Technological innovation, Organizations, Patents, Collaboration, Data mining, Data analysis, open innovation, process mining, pattern discovery",application of process mining techniques for innovation analysis and support innovation processes can be defined as sets of goal driven activities deeply influenced by human experience and behavior with respect to traditional structured operational processes they present an high degree of uncertainty and heterogeneity with little or no structure as a consequence traditional business intelligence tools are mostly not suitable for innovation processes although the innovation promotion has become one of the hottest topic in business economy in last decades and its importance in organization growth is widely recognized currently there are not proposal in literature for the automatic analysis of innovation activities performed by an organization our research is hence aimed to investigate such activities and their corresponding innovation processes by taking into account the way in which they are really performed in organization daily job in the present work we briefly sketch the main issues related to innovation activities and their automatic support firstly considering the current state of the art in literature and then describing the main ideas of our proposal competitive intelligence data mining economics innovation management process mining techniques innovation analysis innovation process goal driven activities business intelligence business economy technological innovation organizations patents collaboration data mining data analysis open innovation process mining pattern discovery,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
63,0,"Information system risk auditing model based on process mining The basic function of audit is finding risk and preventing fraud from occurring as well as maintaining healthy, safe operation of enterprise and even the whole economy. By combining process mining techniques and risk management theory and using the technique of obtaining evidence on fraud risk as a trial, this paper takes business process risk auditing of process-aware information systems as its research goal. The paper aims to find out faults in the business process and audit evidence. It also aims to propose process mining-based risk auditing models of information systems from the perspective of workflow and in allusion to complicated business process. This paper identifies risk and implements continuous audit and monitor as well as searches risk auditing mechanisms and risk control method by using consistency analysis between actual business process and pre-designed business. auditing, data mining, fraud, information systems, risk management, information system risk auditing model, fraud prevention, safe operation, healthy operation, risk management theory, process mining techniques, fraud risk, process-aware information systems, audit evidence, business process, risk control method, risk auditing mechanisms, consistency analysis, Information systems, Monitoring, Process control, Data mining, Immune system, Organizations, process mining, risk auditing mechanism, continuous auditing, continuous monitoring",information system risk auditing model based on process mining the basic function of audit is finding risk and preventing fraud from occurring as well as maintaining healthy safe operation of enterprise and even the whole economy by combining process mining techniques and risk management theory and using the technique of obtaining evidence on fraud risk as a trial this paper takes business process risk auditing of process aware information systems as its research goal the paper aims to find out faults in the business process and audit evidence it also aims to propose process mining based risk auditing models of information systems from the perspective of workflow and in allusion to complicated business process this paper identifies risk and implements continuous audit and monitor as well as searches risk auditing mechanisms and risk control method by using consistency analysis between actual business process and pre designed business auditing data mining fraud information systems risk management information system risk auditing model fraud prevention safe operation healthy operation risk management theory process mining techniques fraud risk process aware information systems audit evidence business process risk control method risk auditing mechanisms consistency analysis information systems monitoring process control data mining immune system organizations process mining risk auditing mechanism continuous auditing continuous monitoring,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
64,0,"Coupling Case Based Reasoning and Process Mining for a Web Based Crisis Management Decision Support System This paper presents a research in progress that aims to design and develop a web-based shared environment for stakeholders involved in disaster management. The goal of this environment is two-fold. Firstly it will provide a reliable disaster information source to facilitate the exchange and the analysis of previous crisis information. Secondly, it will assimilate best practices and provide recommendations based on experiences from previous disasters. One of the first steps towards such an environment is to elaborate a common and generic disaster model. This model is also a reference to define a template for the case base of previous disasters. In order for our system to provide recommendations based on previous practices, we combine case based reasoning with process mining. This article presents the first step towards a disaster management decision support system, specifically providing guidance on how to integrate process mining in the case based reasoning cycle. case-based reasoning, data mining, decision support systems, emergency management, disaster management decision support system, generic disaster model, crisis information, reliable disaster information source, Web based shared environment, Web based crisis management decision support system, process mining, case based reasoning, Cognition, Unified modeling language, Disaster management, Data mining, Crisis management, Databases, Electronic mail, Generic Disaster Model,  Process Mining,  Case Based Reasoning,  Previous practices,  Crisis Management Decision Support.",coupling case based reasoning and process mining for a web based crisis management decision support system this paper presents a research in progress that aims to design and develop a web based shared environment for stakeholders involved in disaster management the goal of this environment is two fold firstly it will provide a reliable disaster information source to facilitate the exchange and the analysis of previous crisis information secondly it will assimilate best practices and provide recommendations based on experiences from previous disasters one of the first steps towards such an environment is to elaborate a common and generic disaster model this model is also a reference to define a template for the case base of previous disasters in order for our system to provide recommendations based on previous practices we combine case based reasoning with process mining this article presents the first step towards a disaster management decision support system specifically providing guidance on how to integrate process mining in the case based reasoning cycle case based reasoning data mining decision support systems emergency management disaster management decision support system generic disaster model crisis information reliable disaster information source web based shared environment web based crisis management decision support system process mining case based reasoning cognition unified modeling language disaster management data mining crisis management databases electronic mail generic disaster model process mining case based reasoning previous practices crisis management decision support,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
65,0,"Business Process Mining by Means of Statistical Languages Model The goal of this research is to provide an alternative for business processes evaluation and tracking, based on the analysis of non-structured information generated by such processes within the organization areas. In this article we introduce a method to determine the occurrence probability of a business process within the enterprisepsilas text documents. The proposed method introduces the use of Statistical language model (SLM), as a new technique in business processes mining area. In order to obtain this objective the following is considered: the probability that a sub process or a process part is in the text paragraph; the probability that this text belongs to a business process; the language model of the processes set; and the set of realized activities which is reconstructed according to the processes that gave origin to the analyzed documents. business data processing, data mining, text analysis, business process mining, statistical languages model, business processes evaluation, nonstructured information, business process occurrence probability, enterprise text documents, text paragraph, Information analysis, Probability, Artificial intelligence, Decision making, Information retrieval, Intelligent networks, Computer networks, Genetic algorithms, Data mining, Biological neural networks, business process management, text mining",business process mining by means of statistical languages model the goal of this research is to provide an alternative for business processes evaluation and tracking based on the analysis of non structured information generated by such processes within the organization areas in this article we introduce a method to determine the occurrence probability of a business process within the enterprisepsilas text documents the proposed method introduces the use of statistical language model slm as a new technique in business processes mining area in order to obtain this objective the following is considered the probability that a sub process or a process part is in the text paragraph the probability that this text belongs to a business process the language model of the processes set and the set of realized activities which is reconstructed according to the processes that gave origin to the analyzed documents business data processing data mining text analysis business process mining statistical languages model business processes evaluation nonstructured information business process occurrence probability enterprise text documents text paragraph information analysis probability artificial intelligence decision making information retrieval intelligent networks computer networks genetic algorithms data mining biological neural networks business process management text mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
66,0,"Finding Structure in Unstructured Processes: The Case for Process Mining Today there are many process mining techniques that allow for the automatic construction of process models based on event logs. Unlike synthesis techniques (e.g., based on regions), process mining aims at the discovery of models (e.g., Petri nets) from incomplete information (i.e., only example behavior is given). The more mature process mining techniques perform well on structured processes. However, most of the existing techniques fail miserably when confronted with unstructured processes. This paper attempts to ""bring structure to the unstructured"" by using an integrated combination of abstraction and clustering techniques. The ultimate goal is to present process models that are understandable by analysts and that lead to improved system/process redesigns. data mining, enterprise resource planning, unstructured processes, process mining, event logs, abstraction, clustering techniques, Bismuth, Petri nets, Concurrent computing, Hospitals, Embedded system, Visualization, Roads, Humans, PROM, Design methodology",finding structure in unstructured processes the case for process mining today there are many process mining techniques that allow for the automatic construction of process models based on event logs unlike synthesis techniques e g based on regions process mining aims at the discovery of models e g petri nets from incomplete information i e only example behavior is given the more mature process mining techniques perform well on structured processes however most of the existing techniques fail miserably when confronted with unstructured processes this paper attempts to bring structure to the unstructured by using an integrated combination of abstraction and clustering techniques the ultimate goal is to present process models that are understandable by analysts and that lead to improved system process redesigns data mining enterprise resource planning unstructured processes process mining event logs abstraction clustering techniques bismuth petri nets concurrent computing hospitals embedded system visualization roads humans prom design methodology,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
67,0,"Using transition systems and regions to analyze and monitor admission procedures of a hospital This research provides topological discovery process that took place and recorded in the log file history. The event log contains information of patient, date/time of the treatment, medical procedures, responsible staff, and case IDs. The goal of this research to verify the conformity of the desinated process workflow using Mine Transition Systems by comparing it with the actual activities recorded in the log file. By using a generated model (Petri Net), a guideline can be developed and used to improve the overall performance of the medical process. The inconsistency between the anticipated workflow and the extracted one from the log file history were found in this study. The main issue was an extensive delay in waiting time for treatment and for payment (prior to medication dispense). These results and findings can be presented to the people that are in charge and hence be used to solve this issue towards better service performance of the hospital. hospitals, medical information systems, Petri nets, admission procedures, hospital, topological discovery process, service performance, Petri net, mine transition systems, process workflow, case ID, responsible staff, medical procedures, event log, log file history, Decision support systems, Process Mining, ProM, Mine Transition System",using transition systems and regions to analyze and monitor admission procedures of a hospital this research provides topological discovery process that took place and recorded in the log file history the event log contains information of patient date time of the treatment medical procedures responsible staff and case ids the goal of this research to verify the conformity of the desinated process workflow using mine transition systems by comparing it with the actual activities recorded in the log file by using a generated model petri net a guideline can be developed and used to improve the overall performance of the medical process the inconsistency between the anticipated workflow and the extracted one from the log file history were found in this study the main issue was an extensive delay in waiting time for treatment and for payment prior to medication dispense these results and findings can be presented to the people that are in charge and hence be used to solve this issue towards better service performance of the hospital hospitals medical information systems petri nets admission procedures hospital topological discovery process service performance petri net mine transition systems process workflow case id responsible staff medical procedures event log log file history decision support systems process mining prom mine transition system,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
68,0,"Incorporating domain knowledge into clinical goal discovering for clinical pathway mining Clinical pathway is a crucial tool for care quality improvement and expense control. Compared to expertdesigned clinical pathway, the topic-based clinical pathway mined from history data is more dynamic and adaptive for real-world application. In this approach, the latent topics discovered by topic modeling are treated as the clinical goals, so that each patient trace is converted to a topic-based sequence. Process mining is used to generate a concise process model on these sequences. However, there is a common problem about the topic modeling that the redundancy between different topics is considerable. It means that some clinical activities strongly correlate to many topics, which has a significant impact on the ability of representing clinical goals of the topics. This paper proposed a novel topic modeling method for clinical goal discovering. Domain knowledge is incorporated to limit the available topic size for each clinical activity. Experiments demonstrate the effectiveness of our approach in discovering quality topics as the clinical goals for clinical pathway mining. data mining, medical computing, domain knowledge, clinical goal discovering, clinical pathway mining, care quality improvement, expense control, topic modeling, process mining, Data mining, Hospitals, Blood, Electrocardiography, Integrated circuits",incorporating domain knowledge into clinical goal discovering for clinical pathway mining clinical pathway is a crucial tool for care quality improvement and expense control compared to expertdesigned clinical pathway the topic based clinical pathway mined from history data is more dynamic and adaptive for real world application in this approach the latent topics discovered by topic modeling are treated as the clinical goals so that each patient trace is converted to a topic based sequence process mining is used to generate a concise process model on these sequences however there is a common problem about the topic modeling that the redundancy between different topics is considerable it means that some clinical activities strongly correlate to many topics which has a significant impact on the ability of representing clinical goals of the topics this paper proposed a novel topic modeling method for clinical goal discovering domain knowledge is incorporated to limit the available topic size for each clinical activity experiments demonstrate the effectiveness of our approach in discovering quality topics as the clinical goals for clinical pathway mining data mining medical computing domain knowledge clinical goal discovering clinical pathway mining care quality improvement expense control topic modeling process mining data mining hospitals blood electrocardiography integrated circuits,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
69,0,"Investigating event log analysis with minimum apriori information This thesis proposes a hybrid log alert detection scheme, which incorporates anomaly detection and signature generation to accomplish its goal. Unlike previous work, minimum apriori knowledge of the system being analyzed is assumed. This assumption enhances the platform portability of the framework. The anomaly detection component works in a bottom-up manner on the contents of historical system log data to detect regions of the log, which contain anomalous (alert) behaviour. The identified anomalous regions (after inspection by a human administrator through a visualization system) are then passed to the signature generation component, which mines them for patterns. Consequently, future occurrences of the underlying alert in the anomalous log region, can be detected on a production system using the discovered patterns. The combination of anomaly detection and signature generation, which is novel when compared to previous work, ensures that a framework which is accurate while still being able to detect new and unknown alerts is attained. digital signatures, investigating event log analysis, minimum Apriori information, hybrid log alert detection scheme, platform portability, anomaly detection component, anomalous regions, visualization system, human administrator, signature generation component, Data visualization, Itemsets, Computers, Monitoring, Production systems, Data mining, Semantics, Algorithms, Networked Systems, System Management, Modeling and Assessment",investigating event log analysis with minimum apriori information this thesis proposes a hybrid log alert detection scheme which incorporates anomaly detection and signature generation to accomplish its goal unlike previous work minimum apriori knowledge of the system being analyzed is assumed this assumption enhances the platform portability of the framework the anomaly detection component works in a bottom up manner on the contents of historical system log data to detect regions of the log which contain anomalous alert behaviour the identified anomalous regions after inspection by a human administrator through a visualization system are then passed to the signature generation component which mines them for patterns consequently future occurrences of the underlying alert in the anomalous log region can be detected on a production system using the discovered patterns the combination of anomaly detection and signature generation which is novel when compared to previous work ensures that a framework which is accurate while still being able to detect new and unknown alerts is attained digital signatures investigating event log analysis minimum apriori information hybrid log alert detection scheme platform portability anomaly detection component anomalous regions visualization system human administrator signature generation component data visualization itemsets computers monitoring production systems data mining semantics algorithms networked systems system management modeling and assessment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
70,0,"Logical structure extraction from software requirements documents Software requirements documents (SRDs) are often authored in general-purpose rich-text editors, such as MS Word. SRDs contain instances of logical structures, such as use case, business rule, and functional requirement. Automated recognition and extraction of these instances enables advanced requirements management features, such as automated traceability, template conformance checking, guided editing, and interoperability with requirements management tools such as RequisitePro. The variability in content and physical representation of these instances poses challenges to their accurate recognition and extraction. To address these challenges, we present a framework allowing 1) the specification of logical structures in terms of their content, textual rendering, and variability and 2) the extraction of instances of such structures from rich-text documents. Our evaluation involves 36 different logical structures identified in 43 SRDs and shows that the intended content, style, and variability of these structures can be specified in the framework such that their instances can be extracted from the documents with high precision and recall, both close to 100%. document handling, formal specification, open systems, logical structure extraction, software requirements documents, SRD, automated recognition, automated traceability, template conformance checking, requirements management, textual rendering, rich-text documents, Unified modeling language, Feature extraction, Software, Organizations, Portable document format, Text analysis, Web pages, Logical Structures, SRS, Requirements Extraction, Software Requirements Documents",logical structure extraction from software requirements documents software requirements documents srds are often authored in general purpose rich text editors such as ms word srds contain instances of logical structures such as use case business rule and functional requirement automated recognition and extraction of these instances enables advanced requirements management features such as automated traceability template conformance checking guided editing and interoperability with requirements management tools such as requisitepro the variability in content and physical representation of these instances poses challenges to their accurate recognition and extraction to address these challenges we present a framework allowing 1 the specification of logical structures in terms of their content textual rendering and variability and 2 the extraction of instances of such structures from rich text documents our evaluation involves 36 different logical structures identified in 43 srds and shows that the intended content style and variability of these structures can be specified in the framework such that their instances can be extracted from the documents with high precision and recall both close to 100 document handling formal specification open systems logical structure extraction software requirements documents srd automated recognition automated traceability template conformance checking requirements management textual rendering rich text documents unified modeling language feature extraction software organizations portable document format text analysis web pages logical structures srs requirements extraction software requirements documents,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
71,0,"Automatic Checking of Conformance to Requirement Boilerplates via Text Chunking: An Industrial Case Study Context. Boilerplates have long been used in Requirements Engineering (RE) to increase the precision of natural language requirements and to avoid ambiguity problems caused by unrestricted natural language. When boilerplates are used, an important quality assurance task is to verify that the requirements indeed conform to the boilerplates. Objective. If done manually, checking conformance to boilerplates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to requirements changes. Our objective is to provide automation for checking conformance to boilerplates using a Natural Language Processing (NLP) technique, called Text Chunking, and to empirically validate the effectiveness of the automation. Method. We use an exploratory case study, conducted in an industrial setting, as the basis for our empirical investigation. Results. We present a generalizable and tool-supported approach for boilerplate conformance checking. We report on the application of our approach to the requirements document for a major software component in the satellite domain. We compare alternative text chunking solutions and argue about their effectiveness for boilerplate conformance checking. Conclusion. Our results indicate that: (1) text chunking provides a robust and accurate basis for checking conformance to boilerplates, and (2) the effectiveness of boilerplate conformance checking based on text chunking is not compromised even when the requirements glossary terms are unknown. This makes our work particularly relevant to practice, as many industrial requirements documents have incomplete glossaries. conformance testing, formal verification, natural language processing, quality assurance, text analysis, requirements engineering, natural language requirements, quality assurance task, natural language processing technique, NLP technique, tool-supported approach, boilerplate conformance checking, requirements document, software component, satellite domain, text chunking solutions, Terminology, Natural language processing, Syntactics, Pipelines, Software, Surveillance, Requirement Boilerplates, Natural Language Processing (NLP), Text Chunking, Case Study Research",automatic checking of conformance to requirement boilerplates via text chunking an industrial case study context boilerplates have long been used in requirements engineering re to increase the precision of natural language requirements and to avoid ambiguity problems caused by unrestricted natural language when boilerplates are used an important quality assurance task is to verify that the requirements indeed conform to the boilerplates objective if done manually checking conformance to boilerplates is laborious presenting a particular challenge when the task has to be repeated multiple times in response to requirements changes our objective is to provide automation for checking conformance to boilerplates using a natural language processing nlp technique called text chunking and to empirically validate the effectiveness of the automation method we use an exploratory case study conducted in an industrial setting as the basis for our empirical investigation results we present a generalizable and tool supported approach for boilerplate conformance checking we report on the application of our approach to the requirements document for a major software component in the satellite domain we compare alternative text chunking solutions and argue about their effectiveness for boilerplate conformance checking conclusion our results indicate that 1 text chunking provides a robust and accurate basis for checking conformance to boilerplates and 2 the effectiveness of boilerplate conformance checking based on text chunking is not compromised even when the requirements glossary terms are unknown this makes our work particularly relevant to practice as many industrial requirements documents have incomplete glossaries conformance testing formal verification natural language processing quality assurance text analysis requirements engineering natural language requirements quality assurance task natural language processing technique nlp technique tool supported approach boilerplate conformance checking requirements document software component satellite domain text chunking solutions terminology natural language processing syntactics pipelines software surveillance requirement boilerplates natural language processing nlp text chunking case study research,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
72,0,"SLA-Driven Business Process Distribution Service oriented architecture (SOA) includes several building blocks among which orchestration engine demands special attention. Although, there are a number of centralized orchestration engines to execute business processes described by BPEL language in SOA, you may find several decentralized orchestration engines and their purpose is to decompose a BPEL process to several software agents to improve some quality factors. On one hand, choosing a suitable method of process distribution may result in better adaptability of process with run-time environment. On the other hand, in the new generation of service oriented architecture (SOA), service level agreements (SLAs) are of paramount important. We need to have a run-time environment to improve the adaptability of run-time environment with SLAs. Thus, this paper we are going to to combine process distribution methods and SLAs. To reach this goal, firstly, we introduce an intelligent method of using process mining for business process distribution (IPD). Secondly, we compare different methods of processes distribution including fully, semi and intelligent process distribution methods. We also show the comparison of these methods considering quality factors such as total execution time, band width usage, agent granularity, resource adaptability, memory usage of agents, number of produced agents and total system adaptability. Thirdly, we consider all of the distribution methods from an SLA view through which users can determine their requirements of executing business processes to be mapped to run-time environment. software agents, software architecture, software quality, SLA-driven business process distribution, service oriented architecture, orchestration engine, centralized orchestration engines, BPEL language, software agents, quality factors, service level agreements, process mining, total execution time, band width usage, agent granularity, resource adaptability, Engines, Runtime environment, Service oriented architecture, Intelligent agent, Q factor, Computer science, Semiconductor optical amplifiers, Knowledge management, Computer architecture, Software agents, Service Level Agreement (SLA), Adaptive Systems, Business Process Mining, BPEL, Service Oriented Architecture, Mobile Agents, Workflow, Distributed Orchestrate Engine",sla driven business process distribution service oriented architecture soa includes several building blocks among which orchestration engine demands special attention although there are a number of centralized orchestration engines to execute business processes described by bpel language in soa you may find several decentralized orchestration engines and their purpose is to decompose a bpel process to several software agents to improve some quality factors on one hand choosing a suitable method of process distribution may result in better adaptability of process with run time environment on the other hand in the new generation of service oriented architecture soa service level agreements slas are of paramount important we need to have a run time environment to improve the adaptability of run time environment with slas thus this paper we are going to to combine process distribution methods and slas to reach this goal firstly we introduce an intelligent method of using process mining for business process distribution ipd secondly we compare different methods of processes distribution including fully semi and intelligent process distribution methods we also show the comparison of these methods considering quality factors such as total execution time band width usage agent granularity resource adaptability memory usage of agents number of produced agents and total system adaptability thirdly we consider all of the distribution methods from an sla view through which users can determine their requirements of executing business processes to be mapped to run time environment software agents software architecture software quality sla driven business process distribution service oriented architecture orchestration engine centralized orchestration engines bpel language software agents quality factors service level agreements process mining total execution time band width usage agent granularity resource adaptability engines runtime environment service oriented architecture intelligent agent q factor computer science semiconductor optical amplifiers knowledge management computer architecture software agents service level agreement sla adaptive systems business process mining bpel service oriented architecture mobile agents workflow distributed orchestrate engine,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
73,0,"Event Logs Generated by an Operating System Running on a COTS Computer During IEMI Exposure Many studies were devoted to the analysis and the detection of electromagnetic attacks against critical electronic systems at the system or the component levels. Some attempts have been made to correlate effects scenarios with events logged by the kernel of the operating system (OS) of commercial-off-the-shelf computer running Windows. Due to the closed principle of the last OS, we decided to perform such an analysis on a computer running a Linux distribution in which a complete access to logs is available. It will be demonstrated that a computer running such an open OS allows detecting the perturbations induced by intentional electromagnetic interferences at different levels of the targeted computer. computational electromagnetics, electromagnetic interference, Linux, operating system kernels, security, intentional electromagnetic interferences, Linux distribution, Windows, commercial-off-the-shelf computer, OS kernel, operating system kernel, critical electronic systems, electromagnetic attack detection, electromagnetic attack analysis, IEMI exposure, COTS computer, event log generation, Universal Serial Bus, Hardware, Sensors, Electromagnetic interference, Monitoring, Electromagnetic compatibility (EMC), electromagnetic interference, software engineering, system analysis and design, Electromagnetic compatibility (EMC), electromagnetic interference, software engineering, system analysis and design",event logs generated by an operating system running on a cots computer during iemi exposure many studies were devoted to the analysis and the detection of electromagnetic attacks against critical electronic systems at the system or the component levels some attempts have been made to correlate effects scenarios with events logged by the kernel of the operating system os of commercial off the shelf computer running windows due to the closed principle of the last os we decided to perform such an analysis on a computer running a linux distribution in which a complete access to logs is available it will be demonstrated that a computer running such an open os allows detecting the perturbations induced by intentional electromagnetic interferences at different levels of the targeted computer computational electromagnetics electromagnetic interference linux operating system kernels security intentional electromagnetic interferences linux distribution windows commercial off the shelf computer os kernel operating system kernel critical electronic systems electromagnetic attack detection electromagnetic attack analysis iemi exposure cots computer event log generation universal serial bus hardware sensors electromagnetic interference monitoring electromagnetic compatibility emc electromagnetic interference software engineering system analysis and design electromagnetic compatibility emc electromagnetic interference software engineering system analysis and design,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
74,0,"Optimization of planning and scheduling for phosphor-chemical enterprises The decision of planning and scheduling for phosphor-chemical enterprise is very complex and difficult, which is a research area that needs to be studied thoroughly. In this paper, a model optimizing planning and scheduling for a phosphor-chemical enterprise is presented in consideration of the operation processes, including mining, mineral processing and chemical fertilizer producing. Also, some numerical examples for the model are computed and analyzed for the purpose of illustration. The results show that it is very useful for an enterprise to optimize production planning and scheduling chemical industry, decision making, fertilisers, mineral processing industry, mining, optimisation, production planning, scheduling, supply chains, optimization, planning, scheduling, phosphor-chemical enterprises, planning decision, scheduling decision, operation process, mining, mineral processing, chemical fertilizer production, production planning, supply chains, Ores, Job shop scheduling, Processor scheduling, Production planning, Supply chains, Minerals, Analytical models, Fertilizers, Chemical processes, Chemical industry, Goal Programming, Optimization, Planning and Scheduling, Supply Chain",optimization of planning and scheduling for phosphor chemical enterprises the decision of planning and scheduling for phosphor chemical enterprise is very complex and difficult which is a research area that needs to be studied thoroughly in this paper a model optimizing planning and scheduling for a phosphor chemical enterprise is presented in consideration of the operation processes including mining mineral processing and chemical fertilizer producing also some numerical examples for the model are computed and analyzed for the purpose of illustration the results show that it is very useful for an enterprise to optimize production planning and scheduling chemical industry decision making fertilisers mineral processing industry mining optimisation production planning scheduling supply chains optimization planning scheduling phosphor chemical enterprises planning decision scheduling decision operation process mining mineral processing chemical fertilizer production production planning supply chains ores job shop scheduling processor scheduling production planning supply chains minerals analytical models fertilizers chemical processes chemical industry goal programming optimization planning and scheduling supply chain,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
75,0,"Building data warehouses based on mine-production geoontology We propose a new approach to decision support system based on conceptualized and systemized domain knowledge for reducing semantic heterogeneity and cognitive biases among multiple data resource and achieving spatial data integration, exchange & share of mine enterprises. Firstly, an architecture and design ideas for data warehouse are established with formalized geo-semantic knowledge. Then the hybrid geoontology model and a goal-driven modeling methods covering static knowledge, dynamic events and humans actors of mine production process is presented based on mining terms, concepts, entities and characteristics. Finally, algorithms and examples to automatically build multidimensional model have been given according to mapping rules between OWL and XML. It is tested that the proposed method has superior usability, could implement standardized and sharing multidimensional models of data warehouses and provide a new way for data integration and decision-making application from different business domains. data integration, data warehouses, decision making, decision support systems, knowledge representation languages, mining, ontologies (artificial intelligence), XML, data warehouse building, mine-production geoontology, decision support system, conceptualized domain knowledge, systemized domain knowledge, semantic heterogeneity reduction, cognitive bias, multiple data resource, spatial data integration, spatial data exchange, spatial data sharing, mine enterprise, formalized geo-semantic knowledge, goal-driven modeling method, static knowledge, dynamic events, mine production process, mining terms, mining concepts, mining entities, mining characteristics, multidimensional model, OWL, XML, decision making, Ontologies, Production, Geology, Semantics, Data mining, Data warehouses, Fuel processing industries, mine production, data warehouse, geo-ontology, data integration",building data warehouses based on mine production geoontology we propose a new approach to decision support system based on conceptualized and systemized domain knowledge for reducing semantic heterogeneity and cognitive biases among multiple data resource and achieving spatial data integration exchange share of mine enterprises firstly an architecture and design ideas for data warehouse are established with formalized geo semantic knowledge then the hybrid geoontology model and a goal driven modeling methods covering static knowledge dynamic events and humans actors of mine production process is presented based on mining terms concepts entities and characteristics finally algorithms and examples to automatically build multidimensional model have been given according to mapping rules between owl and xml it is tested that the proposed method has superior usability could implement standardized and sharing multidimensional models of data warehouses and provide a new way for data integration and decision making application from different business domains data integration data warehouses decision making decision support systems knowledge representation languages mining ontologies artificial intelligence xml data warehouse building mine production geoontology decision support system conceptualized domain knowledge systemized domain knowledge semantic heterogeneity reduction cognitive bias multiple data resource spatial data integration spatial data exchange spatial data sharing mine enterprise formalized geo semantic knowledge goal driven modeling method static knowledge dynamic events mine production process mining terms mining concepts mining entities mining characteristics multidimensional model owl xml decision making ontologies production geology semantics data mining data warehouses fuel processing industries mine production data warehouse geo ontology data integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
76,0,"Knowledge sharing honeynets Due to the prevalence of distributed and coordinated Internet attacks, many researchers and network administrators study the nature and strategies of attackers. To analyze event logs, using intrusion detection systems and active network monitoring, honeynets are being deployed to attract potential attackers in order to investigate their modus operandi. The goal is to use honeynet clusters as real-time warning systems in production networks. Towards satisfying this objective, we have built a honeynet cluster and have run experiments to determine its effectiveness. Majority of the honeynets function in isolation, not sharing information in real time. In order to rectify this deficiency, the authors built a federation of cooperating honeynets (referred to as knowledge sharing honeynets) that shares knowledge of malicious traffic. This paper describes the methods in building a hardware assisted honeynet cluster and testing its effectiveness. security of data, computer networks, real-time systems, distributed Internet attacks, coordinated Internet attacks, network administration, attack strategy, event log analysis, intrusion detection systems, active network monitoring, honeynet clusters, real-time warning systems, production networks, cooperating honeynets, knowledge sharing honeynets, malicious traffic, Network servers, Web server, Switches, Intrusion detection, Monitoring, Telecommunication traffic, Real time systems, Production systems, Hardware, Military computing",knowledge sharing honeynets due to the prevalence of distributed and coordinated internet attacks many researchers and network administrators study the nature and strategies of attackers to analyze event logs using intrusion detection systems and active network monitoring honeynets are being deployed to attract potential attackers in order to investigate their modus operandi the goal is to use honeynet clusters as real time warning systems in production networks towards satisfying this objective we have built a honeynet cluster and have run experiments to determine its effectiveness majority of the honeynets function in isolation not sharing information in real time in order to rectify this deficiency the authors built a federation of cooperating honeynets referred to as knowledge sharing honeynets that shares knowledge of malicious traffic this paper describes the methods in building a hardware assisted honeynet cluster and testing its effectiveness security of data computer networks real time systems distributed internet attacks coordinated internet attacks network administration attack strategy event log analysis intrusion detection systems active network monitoring honeynet clusters real time warning systems production networks cooperating honeynets knowledge sharing honeynets malicious traffic network servers web server switches intrusion detection monitoring telecommunication traffic real time systems production systems hardware military computing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
77,0,"Summarizing patient daily activities for clinical pathway mining Clinical Pathway is ubiquitous and plays an essential role in clinical workflow management. The combination of topic modeling and process mining is an efficient approach to get a non-static and topic-based process model. Topic modeling is used to group the activities of each clinical day into the latent topics, and process mining is used to generate a concise workflow model based on these topics. However, because of the specificity of clinical data, it usually suffers from the performance of topic modeling. In this paper, we take an important clinical practice, all the same activities in one clinical day tend to represent the same clinical goal, into account to enhance the effectiveness of topic modeling. The experiments on real data show significant performance gains of our approach. data mining, patient care, patient monitoring, ubiquitous computing, clinical pathway mining, patient daily activities, clinical workflow management, process mining, topic-based process model, nonstatic model, latent topics, Data mining, Blood, Neurology, Drugs, Business, Vocabulary, clinical activity clustering, topic modeling, clinical pathway",summarizing patient daily activities for clinical pathway mining clinical pathway is ubiquitous and plays an essential role in clinical workflow management the combination of topic modeling and process mining is an efficient approach to get a non static and topic based process model topic modeling is used to group the activities of each clinical day into the latent topics and process mining is used to generate a concise workflow model based on these topics however because of the specificity of clinical data it usually suffers from the performance of topic modeling in this paper we take an important clinical practice all the same activities in one clinical day tend to represent the same clinical goal into account to enhance the effectiveness of topic modeling the experiments on real data show significant performance gains of our approach data mining patient care patient monitoring ubiquitous computing clinical pathway mining patient daily activities clinical workflow management process mining topic based process model nonstatic model latent topics data mining blood neurology drugs business vocabulary clinical activity clustering topic modeling clinical pathway,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
78,0,"Stigmergy and Collaboration: Tracing the Contingencies of Mediated Interaction Social network analysis is primarily based in the investigation of ties between nodes and the groups that those ties form. Computer-mediated interaction has introduced many unique forms of tie data to the field. The form of data used in this research are traces of activity left when people create and edit digital artifacts, and when navigating around hyperlinked environments. Using intentional and unintentional traces of activity to generate social graphs provides a unique window into collaboration and interaction. This paper elaborates a technique that uses event log data to trace contingencies in user activity and generate directed two-mode graphs, associograms, which can then be abstracted to sociogram representations. The social network ties generated represent connections between people based on actions contingent on one another. These ties can be used to represent potential social connections for collaboration, social collectives for coordination, and stigmergic self-organization. directed graphs, groupware, human computer interaction, interactive systems, social networking (online), social network analysis, computer mediated interaction, digital artifacts, social graphs, event log data, directed two-mode graphs, associograms, sociogram representations, stigmergic self-organization, contingency tracing, Social network services, Collaboration, Organizations, Media, Navigation, Context, Communities",stigmergy and collaboration tracing the contingencies of mediated interaction social network analysis is primarily based in the investigation of ties between nodes and the groups that those ties form computer mediated interaction has introduced many unique forms of tie data to the field the form of data used in this research are traces of activity left when people create and edit digital artifacts and when navigating around hyperlinked environments using intentional and unintentional traces of activity to generate social graphs provides a unique window into collaboration and interaction this paper elaborates a technique that uses event log data to trace contingencies in user activity and generate directed two mode graphs associograms which can then be abstracted to sociogram representations the social network ties generated represent connections between people based on actions contingent on one another these ties can be used to represent potential social connections for collaboration social collectives for coordination and stigmergic self organization directed graphs groupware human computer interaction interactive systems social networking online social network analysis computer mediated interaction digital artifacts social graphs event log data directed two mode graphs associograms sociogram representations stigmergic self organization contingency tracing social network services collaboration organizations media navigation context communities,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
79,0,"A User Behavior-Based Approach to Detect the Insider Threat in Distributed Diagnostic Imaging Systems A modern diagnostic imaging system integrates several PACS (Picture Archiving and Communication System) through datacenters that allow a large community of users to access and share sensitive patient medical images. In such integration user access to the medical images that are stored in non-local PACS systems is based on a trust model, which makes data integrity and privacy vulnerable due to possible malicious user behaviors. Moreover, the limited scope and precision of the existing policy-based access control solutions prevent them from detecting suspicious behaviors of the authenticated users. In this paper, we propose an approach for analyzing the user behaviors that allows the administrators to identify the users whose behaviors may jeopardize the data privacy and system integrity. In this context, the system administrator can define an arbitrary pattern of a suspicious user behavior using our new behavior pattern language. A constraint-based pattern-matching engine will identify the instances of the suspicious behavior pattern in the system's audit-log repository. Finally, a decision support system will present the excerpt findings to the system administrator with the overall goal of refining the access control policy rules. We present a case study which indicates our proposed approach provides promising results. authorisation, data integrity, data privacy, decision support systems, medical image processing, message authentication, PACS, pattern matching, trusted computing, user behavior-based approach, insider threat detection, distributed diagnostic imaging systems, PACS, picture archiving and communication system, datacenters, trust model, data integrity, data privacy, malicious user behaviors, policy-based access control solutions, suspicious behavior detection, user authentication, user behavior analysis, user identification, system integrity, behavior pattern language, constraint-based pattern-matching engine, audit-log repository, decision support system, access control policy rules, Pattern matching, Picture archiving and communication systems, Security, Monitoring, Power line communications, Biomedical imaging, User Behavior, Security, Pattern Matching, Insider Threat, Diagnostic Imaging System, Event-log Repository",a user behavior based approach to detect the insider threat in distributed diagnostic imaging systems a modern diagnostic imaging system integrates several pacs picture archiving and communication system through datacenters that allow a large community of users to access and share sensitive patient medical images in such integration user access to the medical images that are stored in non local pacs systems is based on a trust model which makes data integrity and privacy vulnerable due to possible malicious user behaviors moreover the limited scope and precision of the existing policy based access control solutions prevent them from detecting suspicious behaviors of the authenticated users in this paper we propose an approach for analyzing the user behaviors that allows the administrators to identify the users whose behaviors may jeopardize the data privacy and system integrity in this context the system administrator can define an arbitrary pattern of a suspicious user behavior using our new behavior pattern language a constraint based pattern matching engine will identify the instances of the suspicious behavior pattern in the system s audit log repository finally a decision support system will present the excerpt findings to the system administrator with the overall goal of refining the access control policy rules we present a case study which indicates our proposed approach provides promising results authorisation data integrity data privacy decision support systems medical image processing message authentication pacs pattern matching trusted computing user behavior based approach insider threat detection distributed diagnostic imaging systems pacs picture archiving and communication system datacenters trust model data integrity data privacy malicious user behaviors policy based access control solutions suspicious behavior detection user authentication user behavior analysis user identification system integrity behavior pattern language constraint based pattern matching engine audit log repository decision support system access control policy rules pattern matching picture archiving and communication systems security monitoring power line communications biomedical imaging user behavior security pattern matching insider threat diagnostic imaging system event log repository,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
80,0,"CHANGEMINER: A solution for discovering IT change templates from past execution traces The main goal of change management is to ensure that standardized methods and procedures are used for the efficient and prompt handling of changes in IT systems, in order to minimize change-related incidents and service-delivery disruption. To meet this goal, it is of paramount importance reusing the experience acquired from previous changes in the design of subsequent ones. Two distinct approaches may be usefully combined to this end. In a top-down approach, IT operators may manually design change templates based on the knowledge owned/acquired in the past. Considering a reverse, bottom-up perspective, these templates could be discovered from past execution traces gathered from IT provisioning tools. While the former has been satisfactorily explored in previous investigations, the latter - despite its undeniable potential to result in accurate templates in a reduced time scale - has not been subject of research, as far as the authors are aware of, by the service operations and management community. To fill in this gap, this paper proposes a solution, inspired on process mining techniques, to discover change templates from past changes. The solution is analyzed through a prototypical implementation of a change template miner subsystem called CHANGEMINER, and a set of experiments based on a real-life scenario. data mining, information management, information systems, management of change, CHANGEMINER, IT change template, change management, standardized method, IT system, change-related incident, service-delivery disruption, IT provisioning tool, process mining, change template miner subsystem, Laboratories, Informatics, Prototypes, Information technology, Information management, Technology management, Companies, Context-aware services, Libraries, Best practices",changeminer a solution for discovering it change templates from past execution traces the main goal of change management is to ensure that standardized methods and procedures are used for the efficient and prompt handling of changes in it systems in order to minimize change related incidents and service delivery disruption to meet this goal it is of paramount importance reusing the experience acquired from previous changes in the design of subsequent ones two distinct approaches may be usefully combined to this end in a top down approach it operators may manually design change templates based on the knowledge owned acquired in the past considering a reverse bottom up perspective these templates could be discovered from past execution traces gathered from it provisioning tools while the former has been satisfactorily explored in previous investigations the latter despite its undeniable potential to result in accurate templates in a reduced time scale has not been subject of research as far as the authors are aware of by the service operations and management community to fill in this gap this paper proposes a solution inspired on process mining techniques to discover change templates from past changes the solution is analyzed through a prototypical implementation of a change template miner subsystem called changeminer and a set of experiments based on a real life scenario data mining information management information systems management of change changeminer it change template change management standardized method it system change related incident service delivery disruption it provisioning tool process mining change template miner subsystem laboratories informatics prototypes information technology information management technology management companies context aware services libraries best practices,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
81,0,"Reachability Analysis in Dynamically Routed Networks In this paper, we introduce a novel approach to reachability analysis of dynamically routed networks. The goal is to determine the network-wide reachability using static analysis of configuration files gathered from forwarding devices. We describe a method that can compute the reachability in networks with a mix of static routing configurations, distance vector routing protocols, filtering routing updates and redistributions. The method computes a network-wide approximation of distributed routing information using the standard graph algorithms. Thus, for any network state, we can determine a set of active paths used for packet delivery. The outcomes of the method can be, for instance, used during the conformance checking of distributed access control lists against network security policies. authorisation, computer network security, formal verification, IP networks, program diagnostics, reachability analysis, routing protocols, telecommunication computing, reachability analysis, dynamically routed networks, network-wide reachability, static analysis, configuration files, forwarding devices, static routing configurations, distance vector routing protocols, filtering routing updates, redistributions, network-wide approximation, distributed routing information, standard graph algorithms, network state, active paths, packet delivery, conformance checking, distributed access control, network security policy, Routing, Routing protocols, Ribs, Computational modeling, Access control, IP-networks, network configuration, network design, network reachability, routing protocols",reachability analysis in dynamically routed networks in this paper we introduce a novel approach to reachability analysis of dynamically routed networks the goal is to determine the network wide reachability using static analysis of configuration files gathered from forwarding devices we describe a method that can compute the reachability in networks with a mix of static routing configurations distance vector routing protocols filtering routing updates and redistributions the method computes a network wide approximation of distributed routing information using the standard graph algorithms thus for any network state we can determine a set of active paths used for packet delivery the outcomes of the method can be for instance used during the conformance checking of distributed access control lists against network security policies authorisation computer network security formal verification ip networks program diagnostics reachability analysis routing protocols telecommunication computing reachability analysis dynamically routed networks network wide reachability static analysis configuration files forwarding devices static routing configurations distance vector routing protocols filtering routing updates redistributions network wide approximation distributed routing information standard graph algorithms network state active paths packet delivery conformance checking distributed access control network security policy routing routing protocols ribs computational modeling access control ip networks network configuration network design network reachability routing protocols,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
82,0,"Fault prediction under the microscope: A closer look into HPC systems A large percentage of computing capacity in today's large high-performance computing systems is wasted because of failures. Consequently current research is focusing on providing fault tolerance strategies that aim to minimize fault's effects on applications. By far the most popular technique is the checkpointrestart strategy. A complement to this classical approach is failure avoidance, by which the occurrence of a fault is predicted and preventive measures are taken. This requires a reliable prediction system to anticipate failures and their locations. Thus far, research in this field has used ideal predictors that were not implemented in real HPC systems. In this paper, we merge signal analysis concepts with data mining techniques to extend the ELSA (Event Log Signal Analyzer) toolkit and offer an adaptive and more efficient prediction module. Our goal is to provide models that characterize the normal behavior of a system and the way faults affect it. Being able to detect deviations from normality quickly is the foundation of accurate fault prediction. However, this is challenging because component failure dynamics are heterogeneous in space and time. To this end, a large part of the paper is focused on a detailed analysis of the prediction method, by applying it to two large-scale systems and by investigating the characteristics and bottlenecks of each step of the prediction process. Furthermore, we analyze the prediction's precision and recall impact on current checkpointing strategies and highlight future improvements and directions for research in this field. data mining, fault tolerant computing, microscopes, parallel processing, signal processing, large-scale systems, component failure dynamics, event log signal analyzer toolkit, ELSA toolkit, data mining techniques, checkpoint-restart strategy, fault tolerance strategies, high-performance computing systems, HPC systems, microscope, fault prediction, Correlation, Itemsets, Signal analysis, Data mining, Checkpointing, Algorithm design and analysis, Prediction algorithms, fault tolerance, large-scale HPC systems, signal analysis, fault detection",fault prediction under the microscope a closer look into hpc systems a large percentage of computing capacity in today s large high performance computing systems is wasted because of failures consequently current research is focusing on providing fault tolerance strategies that aim to minimize fault s effects on applications by far the most popular technique is the checkpointrestart strategy a complement to this classical approach is failure avoidance by which the occurrence of a fault is predicted and preventive measures are taken this requires a reliable prediction system to anticipate failures and their locations thus far research in this field has used ideal predictors that were not implemented in real hpc systems in this paper we merge signal analysis concepts with data mining techniques to extend the elsa event log signal analyzer toolkit and offer an adaptive and more efficient prediction module our goal is to provide models that characterize the normal behavior of a system and the way faults affect it being able to detect deviations from normality quickly is the foundation of accurate fault prediction however this is challenging because component failure dynamics are heterogeneous in space and time to this end a large part of the paper is focused on a detailed analysis of the prediction method by applying it to two large scale systems and by investigating the characteristics and bottlenecks of each step of the prediction process furthermore we analyze the prediction s precision and recall impact on current checkpointing strategies and highlight future improvements and directions for research in this field data mining fault tolerant computing microscopes parallel processing signal processing large scale systems component failure dynamics event log signal analyzer toolkit elsa toolkit data mining techniques checkpoint restart strategy fault tolerance strategies high performance computing systems hpc systems microscope fault prediction correlation itemsets signal analysis data mining checkpointing algorithm design and analysis prediction algorithms fault tolerance large scale hpc systems signal analysis fault detection,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
83,0,"Heuristic approaches for generating Local Process Models through log projections Local Process Model (LPM) discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially, i.e. subsets of possible events are taken into account to create so-called local process models. Often such smaller models provide valuable insights into the behavior of the process, especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end. The practical application of LPM discovery is however hindered by computational issues in the case of logs with many activities (problems may already occur when there are more than 17 unique activities). In this paper, we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up LPM discovery considerably while still finding high-quality LPMs. We found that a Markov clustering approach to create projection sets results in the largest improvement of execution time, with discovered LPMs still being better than with the use of randomly generated activity sets of the same size. Another heuristic, based on log entropy, yields a more moderate speedup, but enables the discovery of higher quality LPMs. The third heuristic, based on the relative information gain, shows unstable performance: for some data sets the speedup and LPM quality are higher than with the log entropy based method, while for other data sets there is no speedup at all. data mining, entropy, Markov processes, pattern clustering, heuristic approach, local process model generation, log projections, LPM discovery, process model mining, event log, Markov clustering approach, execution time improvement, log entropy, relative information gain, Petri nets, Unified modeling language, Computational modeling, Ventilation, Data mining, Mathematical model, Process modeling",heuristic approaches for generating local process models through log projections local process model lpm discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially i e subsets of possible events are taken into account to create so called local process models often such smaller models provide valuable insights into the behavior of the process especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end the practical application of lpm discovery is however hindered by computational issues in the case of logs with many activities problems may already occur when there are more than 17 unique activities in this paper we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up lpm discovery considerably while still finding high quality lpms we found that a markov clustering approach to create projection sets results in the largest improvement of execution time with discovered lpms still being better than with the use of randomly generated activity sets of the same size another heuristic based on log entropy yields a more moderate speedup but enables the discovery of higher quality lpms the third heuristic based on the relative information gain shows unstable performance for some data sets the speedup and lpm quality are higher than with the log entropy based method while for other data sets there is no speedup at all data mining entropy markov processes pattern clustering heuristic approach local process model generation log projections lpm discovery process model mining event log markov clustering approach execution time improvement log entropy relative information gain petri nets unified modeling language computational modeling ventilation data mining mathematical model process modeling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
84,0,"Data-aware remaining time prediction of business process instances Accurate prediction of the completion time of a business process instance would constitute a valuable tool when managing processes under service level agreement constraints. Such prediction, however, is a very challenging task. A wide variety of factors could influence the trend of a process instance, and hence just using time statistics of historical cases cannot be sufficient to get accurate predictions. Here we propose a new approach where, in order to improve the prediction quality, both the control and the data flow perspectives are jointly used. To achieve this goal, our approach builds a process model which is augmented by time and data information in order to enable remaining time prediction. The remaining time prediction of a running case is calculated combining two factors: (a) the likelihood of all the following activities, given the data collected so far; and (b) the remaining time estimation given by a regression model built upon the data. business data processing, contracts, regression analysis, business process instance, service level agreement constraints, time statistics, data flow perspectives, time estimation, regression model, data-aware remaining time prediction, Vectors, Predictive models, Data mining, Business, Gold, Data models, Indexes, Data-aware Prediction, Process mining, Naive Bayes, Support Vector Regression",data aware remaining time prediction of business process instances accurate prediction of the completion time of a business process instance would constitute a valuable tool when managing processes under service level agreement constraints such prediction however is a very challenging task a wide variety of factors could influence the trend of a process instance and hence just using time statistics of historical cases cannot be sufficient to get accurate predictions here we propose a new approach where in order to improve the prediction quality both the control and the data flow perspectives are jointly used to achieve this goal our approach builds a process model which is augmented by time and data information in order to enable remaining time prediction the remaining time prediction of a running case is calculated combining two factors a the likelihood of all the following activities given the data collected so far and b the remaining time estimation given by a regression model built upon the data business data processing contracts regression analysis business process instance service level agreement constraints time statistics data flow perspectives time estimation regression model data aware remaining time prediction vectors predictive models data mining business gold data models indexes data aware prediction process mining naive bayes support vector regression,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
85,0,"Analyzing Machine Performance Using Data Mining This paper focuses on analysis of machine performance in a manufacturing company. Machine behavior can be complex, because it usually consists of many tasks. Performance of these tasks depends on product attributes, worker's speed, and therefore, analysis is not simple. Performance analysis results can be used for different purposes. Prediction and description are typical products of data mining. Prediction should be used for online monitoring of the manufactory process and as an input for a scheduler. Description can serve as information for managers to know which attributes of products cause problems more frequently. However manufacturing processes are complex, every process is quite unique. Our long term goal is to generalize the most common patterns to build general analyzer. This task is not simple because the lack of real word data and information. Therefore this work may contribute to the other researchers in their understanding of real world manufacturing problems. data mining, production engineering computing, production equipment, manufactory process online monitoring, manufacturing company, data mining, machine performance analysis, Data mining, Employment, Manufacturing, Time measurement, Job shop scheduling, Algorithm design and analysis, Process mining, data mining, manufacturing, performance analysis, simulation, prediction, association rules, scheduling",analyzing machine performance using data mining this paper focuses on analysis of machine performance in a manufacturing company machine behavior can be complex because it usually consists of many tasks performance of these tasks depends on product attributes worker s speed and therefore analysis is not simple performance analysis results can be used for different purposes prediction and description are typical products of data mining prediction should be used for online monitoring of the manufactory process and as an input for a scheduler description can serve as information for managers to know which attributes of products cause problems more frequently however manufacturing processes are complex every process is quite unique our long term goal is to generalize the most common patterns to build general analyzer this task is not simple because the lack of real word data and information therefore this work may contribute to the other researchers in their understanding of real world manufacturing problems data mining production engineering computing production equipment manufactory process online monitoring manufacturing company data mining machine performance analysis data mining employment manufacturing time measurement job shop scheduling algorithm design and analysis process mining data mining manufacturing performance analysis simulation prediction association rules scheduling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,1.0
86,0,"A Keyword Recommendation Experiment to Support Information Organization and Folksonomies in Edu-AREA Edu-AREA is a Web 2.0 application whose main goal is to contribute to teaching innovation. It provides descriptions of educational resources and guidelines that can be used by teachers to create their lesson plans and later to document their teaching experiences. At the current stage of the Edu-AREA development, a main issue is related to the management and classification of information provided by users. This paper introduces a folksonomy approach, the architecture of the system, and the results of an experiment about keyword recommendations. pattern classification, recommender systems, social networking (online), teaching, keyword recommendation, information organization, Edu-AREA, Web 2.0 application, teaching innovation, educational resources, educational guidelines, lesson plan creation, teaching experience documentation, information classification, information management, folksonomy approach, system architecture, Education, Web 2.0, Ontologies, Organizations, Data models, Vocabulary, Semantics, Social network services, Educational activities, Information sharing, Tagging, Social network services, educational activities, information sharing, tagging",a keyword recommendation experiment to support information organization and folksonomies in edu area edu area is a web 2 0 application whose main goal is to contribute to teaching innovation it provides descriptions of educational resources and guidelines that can be used by teachers to create their lesson plans and later to document their teaching experiences at the current stage of the edu area development a main issue is related to the management and classification of information provided by users this paper introduces a folksonomy approach the architecture of the system and the results of an experiment about keyword recommendations pattern classification recommender systems social networking online teaching keyword recommendation information organization edu area web 2 0 application teaching innovation educational resources educational guidelines lesson plan creation teaching experience documentation information classification information management folksonomy approach system architecture education web 2 0 ontologies organizations data models vocabulary semantics social network services educational activities information sharing tagging social network services educational activities information sharing tagging,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
87,1,"Evaluating the effect of best practices for business process redesign: An evidence-based approach based on process mining techniques The management of business processes in modern times is rapidly shifting towards being evidence-based. Business process evaluation indicators tend to focus on process performance only, neglecting the definition of indicators to evaluate other concerns of interest in different phases of the business process lifecycle. Moreover, they usually do not discuss specifically which data must be collected to calculate indicators and whether collecting these data is feasible or not. This paper proposes a business process assessment framework focused on the process redesign lifecycle phase and tightly coupled with process mining as an operational framework to calculate indicators. The framework includes process performance indicators and indicators to assess whether process redesign best practices have been applied and to what extent. Both sets of indicators can be calculated using standard process mining functionality. This, implicitly, also defines what data must be collected during process execution to enable their calculation. The framework is evaluated through case studies and a thorough comparison against other approaches in the literature. Process redesign,  Best practice,  Process performance indicator,  Process mining,  Case study,  Business process management",evaluating the effect of best practices for business process redesign an evidence based approach based on process mining techniques the management of business processes in modern times is rapidly shifting towards being evidence based business process evaluation indicators tend to focus on process performance only neglecting the definition of indicators to evaluate other concerns of interest in different phases of the business process lifecycle moreover they usually do not discuss specifically which data must be collected to calculate indicators and whether collecting these data is feasible or not this paper proposes a business process assessment framework focused on the process redesign lifecycle phase and tightly coupled with process mining as an operational framework to calculate indicators the framework includes process performance indicators and indicators to assess whether process redesign best practices have been applied and to what extent both sets of indicators can be calculated using standard process mining functionality this implicitly also defines what data must be collected during process execution to enable their calculation the framework is evaluated through case studies and a thorough comparison against other approaches in the literature process redesign best practice process performance indicator process mining case study business process management,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
88,1,"Enhancing Process Models to Improve Business Performance: A Methodology and Case Studies Process mining is not only about discovery and conformance checking of business processes. It is also focused on enhancing processes to improve the business performance. While from a business perspective this third main stream is definitely as important as the others if not even more, little research work has been conducted. The existing body of work on process enhancement mainly focuses on ensuring that the process model is adapted to incorporate behavior that is observed in reality. It is less focused on improving the performance of the process. This paper reports on a methodology that creates an enhanced model with an improved performance level. The enhancements of the model limit incorporated behavior to only those parts that do not violate any business rules. Finally, the enhanced model is kept as close to the original model as possible. The practical relevance and feasibility of the methodology is assessed through two case studies. The result shows that the process models improved through our methodology, in comparison with state-of the art techniques, have improved KPI levels while still adhering to the desired prescriptive model. []",enhancing process models to improve business performance a methodology and case studies process mining is not only about discovery and conformance checking of business processes it is also focused on enhancing processes to improve the business performance while from a business perspective this third main stream is definitely as important as the others if not even more little research work has been conducted the existing body of work on process enhancement mainly focuses on ensuring that the process model is adapted to incorporate behavior that is observed in reality it is less focused on improving the performance of the process this paper reports on a methodology that creates an enhanced model with an improved performance level the enhancements of the model limit incorporated behavior to only those parts that do not violate any business rules finally the enhanced model is kept as close to the original model as possible the practical relevance and feasibility of the methodology is assessed through two case studies the result shows that the process models improved through our methodology in comparison with state of the art techniques have improved kpi levels while still adhering to the desired prescriptive model,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
89,1,"A Framework for Inter-Organizational Performance Analysis from EDI Messages The evaluation of an organization's performance may also consider the assessment of inter-organizational relationships (IORs). However, the evaluation of IORs is typically based on success factors, such as trust, which are difficult to be measured quantitatively. In this paper, we present a framework supporting inter-organizational performance evaluation which integrates (i) a bottom-up approach supporting the identification of Key Performance Indicators (KPIs) from business information, event logs, as well as process models, and (ii) a top-down approach for measuring business performance on the strategic level based on the Balanced Scorecard (BSC) method. In order to prove the feasibility of the framework, we present an inter-organizational performance analysis case study of a beverage manufacturing company. The case study shows that the framework enables (i) the derivation of quantifiable KPIs from operational data and (ii) the alignment of KPIs with business objectives allowing an evaluation of IORs on the strategic level. []",a framework for inter organizational performance analysis from edi messages the evaluation of an organization s performance may also consider the assessment of inter organizational relationships iors however the evaluation of iors is typically based on success factors such as trust which are difficult to be measured quantitatively in this paper we present a framework supporting inter organizational performance evaluation which integrates i a bottom up approach supporting the identification of key performance indicators kpis from business information event logs as well as process models and ii a top down approach for measuring business performance on the strategic level based on the balanced scorecard bsc method in order to prove the feasibility of the framework we present an inter organizational performance analysis case study of a beverage manufacturing company the case study shows that the framework enables i the derivation of quantifiable kpis from operational data and ii the alignment of kpis with business objectives allowing an evaluation of iors on the strategic level,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0.0,0.0,0.0
90,1,"Identifying Inter-organizational Key Performance Indicators from EDIFACT Messages Inter-organizational relationships (IORs) are important for creating business potential and increasing business performance. The evaluation of IORs is necessary for analyzing the collaboration between businesses as well as for assessing business partners. However, the evaluation of IORs is ambiguous since it is usually measured by success factors, such as trust and information sharing, which are difficult to be measured quantitatively. In this paper, we propose using quantifiable Key Performance Indicators (KPIs) for measuring success factors. We aim to identify KPIs in inter-organizational scenarios where information is exchanged electronically based on EDIFACT message types. In particular, we i) derive inter-organizational KPIs and propose guidelines for their calculation from EDIFACT data elements, and ii) aggregate these KPIs to define quantitative measurements reflecting inter-organizational success factors. Therefore, we first define a method for the systematic selection of suitable data elements from EDIFACT message types based on frequency analysis. Second, we consider the semantics of data elements and message types in defining KPIs. Having these KPIs at hand supports the quantitative evaluation of success factors which in turn enables the evaluation of IORs. Key performance indicators (KPI),  Electronic Data Interchange (EDI),  EDIFACT",identifying inter organizational key performance indicators from edifact messages inter organizational relationships iors are important for creating business potential and increasing business performance the evaluation of iors is necessary for analyzing the collaboration between businesses as well as for assessing business partners however the evaluation of iors is ambiguous since it is usually measured by success factors such as trust and information sharing which are difficult to be measured quantitatively in this paper we propose using quantifiable key performance indicators kpis for measuring success factors we aim to identify kpis in inter organizational scenarios where information is exchanged electronically based on edifact message types in particular we i derive inter organizational kpis and propose guidelines for their calculation from edifact data elements and ii aggregate these kpis to define quantitative measurements reflecting inter organizational success factors therefore we first define a method for the systematic selection of suitable data elements from edifact message types based on frequency analysis second we consider the semantics of data elements and message types in defining kpis having these kpis at hand supports the quantitative evaluation of success factors which in turn enables the evaluation of iors key performance indicators kpi electronic data interchange edi edifact,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0.0,0.0,0.0
91,1,"Mining Users' Intents from Logs Intentions play a key role in information systems engineering. Research on process modeling has highlighted that specifying intentions can expressly mitigate many problems encountered in process modeling as lack of flexibility or adaptation. Process mining approaches mine processes in terms of tasks and branching. To identify and formalize intentions from event logs, this work presents a novel approach of process mining, called Map Miner Method (MMM). This method automates the construction of intentional process models from event logs. First, MMM estimates users' strategies (i.e., the different ways to fulfill the intentions) in terms of their activities. These estimated strategies are then used to infer users' intentions at different levels of abstraction using two tailored algorithms. MMM constructs intentional process models with respect to the Map metamodel formalism. MMM is applied on a real-world dataset, i.e. event logs of developers of Eclipse UDC (Usage Data Collector). The resulting Map process model provides a precious understanding of the processes followed by the developers, and also provide feedback on the effectiveness and demonstrate scalability of MMM. Intentional Process Models,  Machine Learning,  Process Mining,  Hidden Markov Models",mining users intents from logs intentions play a key role in information systems engineering research on process modeling has highlighted that specifying intentions can expressly mitigate many problems encountered in process modeling as lack of flexibility or adaptation process mining approaches mine processes in terms of tasks and branching to identify and formalize intentions from event logs this work presents a novel approach of process mining called map miner method mmm this method automates the construction of intentional process models from event logs first mmm estimates users strategies i e the different ways to fulfill the intentions in terms of their activities these estimated strategies are then used to infer users intentions at different levels of abstraction using two tailored algorithms mmm constructs intentional process models with respect to the map metamodel formalism mmm is applied on a real world dataset i e event logs of developers of eclipse udc usage data collector the resulting map process model provides a precious understanding of the processes followed by the developers and also provide feedback on the effectiveness and demonstrate scalability of mmm intentional process models machine learning process mining hidden markov models,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,1.0,0.0
92,1,"What Shall I Do Next? Intention Mining for Flexible Process Enactment Besides the benefits of flexible processes, practical implementations of process aware information systems have also revealed difficulties encountered by process participants during enactment. Several support and guidance solutions based on process mining have been proposed, but they lack a suitable semantics for human reasoning and decisions making as they mainly rely on low level activities. Applying design science, we created FlexPAISSeer, an intention mining oriented approach, with its component artifacts: 1) IntentMiner which discovers the intentional model of the executable process in an unsupervised manner; 2) IntentRecommender which generates recommendations as intentions and confidence factors, based on the mined intentional process model and probabilistic calculus. The artifacts were evaluated in a case study with a Netherlands software company, using a Childcare system that allows flexible data-driven process enactment. intention mining,  process mining,  flexible processes,  process aware information systems,  process recommendations",what shall i do next intention mining for flexible process enactment besides the benefits of flexible processes practical implementations of process aware information systems have also revealed difficulties encountered by process participants during enactment several support and guidance solutions based on process mining have been proposed but they lack a suitable semantics for human reasoning and decisions making as they mainly rely on low level activities applying design science we created flexpaisseer an intention mining oriented approach with its component artifacts 1 intentminer which discovers the intentional model of the executable process in an unsupervised manner 2 intentrecommender which generates recommendations as intentions and confidence factors based on the mined intentional process model and probabilistic calculus the artifacts were evaluated in a case study with a netherlands software company using a childcare system that allows flexible data driven process enactment intention mining process mining flexible processes process aware information systems process recommendations,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0.0,0.0,0.0
93,1,"Mining Intentional Process Models So far, process mining techniques suggested to model processes in terms of tasks that occur during the enactment of a process. However, research on process modeling has illustrated that many issues, such as lack of flexibility or adaptation, are solved more effectively when intentions are explicitly specified. This thesis presents a novel approach of process mining, called Map Miner Method (MMM). This method is designed to automate the construction of intentional process models from traces. MMM uses Hidden Markov Models to model the relationship between users' activities and the strategies (i.e., the different ways to fulfill the intentions). The method also includes two specific algorithms developed to infer users' intentions and construct intentional process model (Map), respectively. MMM can construct Map process models with different levels of granularity (pseudo-Map and Map process models) with respect to the Map metamodel formalism. The entire proposed method was applied and validated on practical traces in a large-scale experiment, on event logs of developers of Eclipse UDC (Usage Data Collector). The resulting Map process models provide a precious understanding of the processes followed by the developers, and also provide feedback on the effectiveness and demonstrate scalability of MMM in terms of traces. Map Miner tool has been developed to enable practicing the proposed approach. This permits users to obtain the pseudo-Map and Map process model out of traces. Intentional Process Models,  Machine Learning,  process mining,  Hidden Markov Models",mining intentional process models so far process mining techniques suggested to model processes in terms of tasks that occur during the enactment of a process however research on process modeling has illustrated that many issues such as lack of flexibility or adaptation are solved more effectively when intentions are explicitly specified this thesis presents a novel approach of process mining called map miner method mmm this method is designed to automate the construction of intentional process models from traces mmm uses hidden markov models to model the relationship between users activities and the strategies i e the different ways to fulfill the intentions the method also includes two specific algorithms developed to infer users intentions and construct intentional process model map respectively mmm can construct map process models with different levels of granularity pseudo map and map process models with respect to the map metamodel formalism the entire proposed method was applied and validated on practical traces in a large scale experiment on event logs of developers of eclipse udc usage data collector the resulting map process models provide a precious understanding of the processes followed by the developers and also provide feedback on the effectiveness and demonstrate scalability of mmm in terms of traces map miner tool has been developed to enable practicing the proposed approach this permits users to obtain the pseudo map and map process model out of traces intentional process models machine learning process mining hidden markov models,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
94,1,"Supervised vs. Unsupervised Learning for Intentional Process Model Discovery Learning humans' behavior from activity logs requires choosing an adequate machine learning technique regarding the situation at hand. This choice impacts significantly results reliability. In this paper, Hidden Markov Models (HMMs) are used to build intentional process models (Maps) from activity logs. Since HMMs parameters require to be learned, the main contribution of this paper is to compare supervised and unsupervised learning approaches of HMMs. After a theoretical comparison of both approaches, they are applied on two controlled experiments to compare the Maps thereby obtained. The results demonstrate using supervised learning leads to a poor performance because it imposes binding conditions in terms of data labeling, introduces inherent humans' biases, provides unreliable results in the absence of ground truth, etc. Instead, unsupervised learning obtains efficient Maps with a higher performance and lower humans' effort. Supervised Learning,  Unsupervised Learning,  Intentional Process Modeling,  Hidden Markov Models",supervised vs unsupervised learning for intentional process model discovery learning humans behavior from activity logs requires choosing an adequate machine learning technique regarding the situation at hand this choice impacts significantly results reliability in this paper hidden markov models hmms are used to build intentional process models maps from activity logs since hmms parameters require to be learned the main contribution of this paper is to compare supervised and unsupervised learning approaches of hmms after a theoretical comparison of both approaches they are applied on two controlled experiments to compare the maps thereby obtained the results demonstrate using supervised learning leads to a poor performance because it imposes binding conditions in terms of data labeling introduces inherent humans biases provides unreliable results in the absence of ground truth etc instead unsupervised learning obtains efficient maps with a higher performance and lower humans effort supervised learning unsupervised learning intentional process modeling hidden markov models,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,0.0,0.0
95,1,"Unsupervised discovery of intentional process models from event logs Research on guidance and method engineering has highlighted that many method engineering issues, such as lack of flexibility or adaptation, are solved more effectively when intentions are explicitly specified. However, software engineering process models are most often described in terms of sequences of activities. This paper presents a novel approach, so-called Map Miner Method (MMM), designed to automate the construction of intentional process models from process logs. To do so, MMM uses Hidden Markov Models to model users' activities logs in terms of users' strategies. MMM also infers users' intentions and constructs fine-grained and coarse-grained intentional process models with respect to the Map metamodel syntax (i.e., metamodel that specifies intentions and strategies of process actors). These models are obtained by optimizing a new precision-fitness metric. The result is a software engineering method process specification aligned with state of the art of method engineering approaches. As a case study, the MMM is used to mine the intentional process associated to the Eclipse platform usage. Observations show that the obtained intentional process model offers a new understanding of software processes, and could readily be used for recommender systems. Event logs Mining,  Software Development Process,  Intentional Process Modeling",unsupervised discovery of intentional process models from event logs research on guidance and method engineering has highlighted that many method engineering issues such as lack of flexibility or adaptation are solved more effectively when intentions are explicitly specified however software engineering process models are most often described in terms of sequences of activities this paper presents a novel approach so called map miner method mmm designed to automate the construction of intentional process models from process logs to do so mmm uses hidden markov models to model users activities logs in terms of users strategies mmm also infers users intentions and constructs fine grained and coarse grained intentional process models with respect to the map metamodel syntax i e metamodel that specifies intentions and strategies of process actors these models are obtained by optimizing a new precision fitness metric the result is a software engineering method process specification aligned with state of the art of method engineering approaches as a case study the mmm is used to mine the intentional process associated to the eclipse platform usage observations show that the obtained intentional process model offers a new understanding of software processes and could readily be used for recommender systems event logs mining software development process intentional process modeling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,0.0,0.0
96,1,"Intentional Process Mining: Discovering and Modeling the Goals Behind Processes using Supervised Learning Understanding people's goals is a challenging issue that is met in many different areas such as security, sales, information retrieval, etc. Intention Mining aims at uncovering intentions from observations of actual activities. While most Intention Mining techniques proposed so far focus on mining individual intentions to analyze web engine queries, this paper proposes a generic technique to mine intentions from activity traces. The proposed technique relies on supervised learning and generates intentional models specified with the Map formalism. The originality of the contribution lies in the demonstration that it is actually possible to reverse engineer the underlying intentional plans built by people when in action, and specify them in models e.g. with intentions at different levels, dependencies, links with other concepts, etc. After an introduction on intention mining, the paper presents the Supervised Map Miner Method and reports two controlled experiments that were undertaken to evaluate precision, recall and F-Score. The results are promising since the authors were able to find the intentions underlying the activities as well as the corresponding map process model with satisfying accuracy, efficiency and performance. intention mining,  trace,  supervised learning,  Hidden Markov model,  goal modeling,  event log",intentional process mining discovering and modeling the goals behind processes using supervised learning understanding people s goals is a challenging issue that is met in many different areas such as security sales information retrieval etc intention mining aims at uncovering intentions from observations of actual activities while most intention mining techniques proposed so far focus on mining individual intentions to analyze web engine queries this paper proposes a generic technique to mine intentions from activity traces the proposed technique relies on supervised learning and generates intentional models specified with the map formalism the originality of the contribution lies in the demonstration that it is actually possible to reverse engineer the underlying intentional plans built by people when in action and specify them in models e g with intentions at different levels dependencies links with other concepts etc after an introduction on intention mining the paper presents the supervised map miner method and reports two controlled experiments that were undertaken to evaluate precision recall and f score the results are promising since the authors were able to find the intentions underlying the activities as well as the corresponding map process model with satisfying accuracy efficiency and performance intention mining trace supervised learning hidden markov model goal modeling event log,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,0.0,0.0
97,1,"A novel approach to process mining: Intentional process models discovery So far, process mining techniques have suggested to model processes in terms of tasks that occur during the enactment of a process. However, research on method engineering and guidance has illustrated that many issues, such as lack of flexibility or adaptation, are solved more effectively when intentions are explicitly specified. This paper presents a novel approach of process mining, called Map Miner Method (MMM). This method is designed to automate the construction of intentional process models from process logs. MMM uses Hidden Markov Models to model the relationship between users' activities logs and the strategies to fulfill their intentions. The method also includes two specific algorithms developed to infer users' intentions and construct intentional process model (Map) respectively. MMM can construct Map process models with different levels of abstraction (fine-grained and coarse-grained process models) with respect to the Map metamodel formalism (i.e., metamodel that specifies intentions and strategies of process actors). This paper presents all steps toward the construction of Map process models topology. The entire method is applied on a large-scale case study (Eclipse UDC) to mine the associated intentional process. The likelihood of the obtained process model shows a satisfying efficiency for the proposed method. Intention-oriented Process Modeling,  Process Mining,  unsupervised learning",a novel approach to process mining intentional process models discovery so far process mining techniques have suggested to model processes in terms of tasks that occur during the enactment of a process however research on method engineering and guidance has illustrated that many issues such as lack of flexibility or adaptation are solved more effectively when intentions are explicitly specified this paper presents a novel approach of process mining called map miner method mmm this method is designed to automate the construction of intentional process models from process logs mmm uses hidden markov models to model the relationship between users activities logs and the strategies to fulfill their intentions the method also includes two specific algorithms developed to infer users intentions and construct intentional process model map respectively mmm can construct map process models with different levels of abstraction fine grained and coarse grained process models with respect to the map metamodel formalism i e metamodel that specifies intentions and strategies of process actors this paper presents all steps toward the construction of map process models topology the entire method is applied on a large scale case study eclipse udc to mine the associated intentional process the likelihood of the obtained process model shows a satisfying efficiency for the proposed method intention oriented process modeling process mining unsupervised learning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,0.0,0.0
98,1,"Process Mining Versus Intention Mining Process mining aims to discover, enhance or check the conformance of activity-oriented process models from event logs. A new field of research, called intention mining, recently emerged. This field has the same objectives as process mining but specifically addresses intentional process models (processes focused on the reasoning behind the activities). This paper aims to highlight the differences between these two fields of research and illustrates the use of mining techniques on a dataset of event logs, to discover an activity process model as well as an intentional process model. process mining,  intention mining,  intentional process modeling",process mining versus intention mining process mining aims to discover enhance or check the conformance of activity oriented process models from event logs a new field of research called intention mining recently emerged this field has the same objectives as process mining but specifically addresses intentional process models processes focused on the reasoning behind the activities this paper aims to highlight the differences between these two fields of research and illustrates the use of mining techniques on a dataset of event logs to discover an activity process model as well as an intentional process model process mining intention mining intentional process modeling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,0.0,0.0
99,1,"Supervised intentional process models discovery using Hidden Markov models Since several decades, discovering process models is a subject of interest in the Information System (IS) community. Approaches have been proposed to recover process models, based on the recorded sequential tasks (traces) done by IS's actors. However, these approaches only focused on activities and the process models identified are, in consequence, activity-oriented. Intentional process models focus on the intentions underlying activities rather than activities, in order to offer a better guidance through the processes. Unfortunately, the existing process-mining approaches do not take into account the hidden aspect of the intentions behind the recorded user activities. We think that we can discover the intentional process models underlying user activities by using Intention mining techniques. The aim of this paper is to propose the use of probabilistic models to evaluate the most likely intentions behind traces of activities, namely Hidden Markov Models (HMMs). We focus on this paper on a supervised approach that allows discovering the intentions behind the user activities traces and to compare them to the prescribed intentional process model. intention mining,  process modeling,  supervised learning,  process discovery",supervised intentional process models discovery using hidden markov models since several decades discovering process models is a subject of interest in the information system is community approaches have been proposed to recover process models based on the recorded sequential tasks traces done by is s actors however these approaches only focused on activities and the process models identified are in consequence activity oriented intentional process models focus on the intentions underlying activities rather than activities in order to offer a better guidance through the processes unfortunately the existing process mining approaches do not take into account the hidden aspect of the intentions behind the recorded user activities we think that we can discover the intentional process models underlying user activities by using intention mining techniques the aim of this paper is to propose the use of probabilistic models to evaluate the most likely intentions behind traces of activities namely hidden markov models hmms we focus on this paper on a supervised approach that allows discovering the intentions behind the user activities traces and to compare them to the prescribed intentional process model intention mining process modeling supervised learning process discovery,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0.0,0.0,0.0
100,1,"Contextual recommendations using intention mining on process traces: Doctoral consortium paper Nowadays, digital traces are omnipresent in Information System (IS). Companies track IS interactions to retrieve and compile information about actors. Researchers of various streams, within IT and beyond, focused on recording actor interactions with systems and the technical possibilities to identify record and store these interactions. Tracing functionality has appeared in almost all common computer applications. This PhD project will focus on the establishment of a trace-based system and propose recommendations to actors regarding to their context. The objective of this thesis is to study process traces to propose recommendations to the actors by identifying a set of generic processes adaptable to the current actors' context. Thus, any actor, expert or novice, will be able to use this knowledge that gives contextual clues to identify the potential steps he could perform. intention mining,  recommendations,  process traces",contextual recommendations using intention mining on process traces doctoral consortium paper nowadays digital traces are omnipresent in information system is companies track is interactions to retrieve and compile information about actors researchers of various streams within it and beyond focused on recording actor interactions with systems and the technical possibilities to identify record and store these interactions tracing functionality has appeared in almost all common computer applications this phd project will focus on the establishment of a trace based system and propose recommendations to actors regarding to their context the objective of this thesis is to study process traces to propose recommendations to the actors by identifying a set of generic processes adaptable to the current actors context thus any actor expert or novice will be able to use this knowledge that gives contextual clues to identify the potential steps he could perform intention mining recommendations process traces,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
101,1,"Incorporating Topic Assignment Constraint and Topic Correlation Limitation into Clinical Goal Discovering for Clinical Pathway Mining Clinical pathways are widely used around the world for providing quality medical treatment and controlling healthcare cost. However, the expert-designed clinical pathways can hardly deal with the variances among hospitals and patients. It calls for more dynamic and adaptive process, which is derived from various clinical data. Topic-based clinical pathway mining is an effective approach to discover a concise process model. Through this approach, the latent topics found by latent Dirichlet allocation (LDA) represent the clinical goals. And process mining methods are used to extract the temporal relations between these topics. However, the topic quality is usually not desirable due to the low performance of the LDA in clinical data. In this paper, we incorporate topic assignment constraint and topic correlation limitation into the LDA to enhance the ability of discovering high-quality topics. Two real-world datasets are used to evaluate the proposed method. The results show that the topics discovered by our method are with higher coherence, informativeness, and coverage than the original LDA. These quality topics are suitable to represent the clinical goals. Also, we illustrate that our method is effective in generating a comprehensive topic-based clinical pathway model. []",incorporating topic assignment constraint and topic correlation limitation into clinical goal discovering for clinical pathway mining clinical pathways are widely used around the world for providing quality medical treatment and controlling healthcare cost however the expert designed clinical pathways can hardly deal with the variances among hospitals and patients it calls for more dynamic and adaptive process which is derived from various clinical data topic based clinical pathway mining is an effective approach to discover a concise process model through this approach the latent topics found by latent dirichlet allocation lda represent the clinical goals and process mining methods are used to extract the temporal relations between these topics however the topic quality is usually not desirable due to the low performance of the lda in clinical data in this paper we incorporate topic assignment constraint and topic correlation limitation into the lda to enhance the ability of discovering high quality topics two real world datasets are used to evaluate the proposed method the results show that the topics discovered by our method are with higher coherence informativeness and coverage than the original lda these quality topics are suitable to represent the clinical goals also we illustrate that our method is effective in generating a comprehensive topic based clinical pathway model,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
102,1,"Discovering Requirements through Goal-Driven Process Mining Software systems are designed to support their users in performing tasks that are parts of more general processes. Unfortunately, software designers often make invalid assumptions about the users' processes and therefore about the requirements to support such processes. Eliciting and validating such assumptions through manual means (e.g., through observations, interviews, and workshops) is expensive, time-consuming, and may fail to identify the users' real processes. Using process mining may reduce these problems by automating the monitoring and discovery of the actual processes followed by a crowd of users. The Crowd provides an opportunity to involve diverse groups of users to interact with a system and conduct their intended processes. This implicit feedback in the form of discovered processes can then be used to modify the existing system's functionalities and ensure whether or not a software product is used as initially designed. In addition, the analysis of user-system interactions may reveal lacking functionalities and quality issues. These ideas are illustrated on the GreenSoft personal energy management system. process mining,  process validation,  crowdsourcing,  requirements engineering,  goal orientation",discovering requirements through goal driven process mining software systems are designed to support their users in performing tasks that are parts of more general processes unfortunately software designers often make invalid assumptions about the users processes and therefore about the requirements to support such processes eliciting and validating such assumptions through manual means e g through observations interviews and workshops is expensive time consuming and may fail to identify the users real processes using process mining may reduce these problems by automating the monitoring and discovery of the actual processes followed by a crowd of users the crowd provides an opportunity to involve diverse groups of users to interact with a system and conduct their intended processes this implicit feedback in the form of discovered processes can then be used to modify the existing system s functionalities and ensure whether or not a software product is used as initially designed in addition the analysis of user system interactions may reveal lacking functionalities and quality issues these ideas are illustrated on the greensoft personal energy management system process mining process validation crowdsourcing requirements engineering goal orientation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
103,1,"Mining Goal Refinement Patterns: Distilling Know-How from Data Goal models play an important role by providing a hierarchic representation of stakeholder intent, and by providing a representation of lower-level subgoals that must be achieved to enable the achievement of higher-level goals. A goal model can be viewed as a composition of a number of goal refinement patterns that relate parent goals to subgoals. In this paper, we offer a means for mining these patterns from enterprise event logs and a technique to leverage vector representations of words and phrases to compose these patterns to obtain complete goal models. The resulting machinery can be quiote powerful in its ability to mine know-how or constitutive norms. We offer an empirical evaluation using both real-life and synthetic datasets. Goal model mining,  Goal refinement,  Know-how",mining goal refinement patterns distilling know how from data goal models play an important role by providing a hierarchic representation of stakeholder intent and by providing a representation of lower level subgoals that must be achieved to enable the achievement of higher level goals a goal model can be viewed as a composition of a number of goal refinement patterns that relate parent goals to subgoals in this paper we offer a means for mining these patterns from enterprise event logs and a technique to leverage vector representations of words and phrases to compose these patterns to obtain complete goal models the resulting machinery can be quiote powerful in its ability to mine know how or constitutive norms we offer an empirical evaluation using both real life and synthetic datasets goal model mining goal refinement know how,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0.0,0.0,0.0
104,1,"CJM-ex: Goal-oriented Exploration of Customer Journey Maps using Event Logs and Data Analytics Customer Journey Mapping (CJM), is an emerging area of research tackling issues related to customer behavior and user trajectories when consuming a service. The increasing complexity of the service in- dustry makes this type of tools popular amongst practitioners. However, to date, it is not clear how a CJM can be used to depict hundreds or thousands of customer journeys. Inspired by process discovery techniques -- borrowed from Process Mining -- we present CJM-explorer (CJM-ex). CJM-ex is a web interface that uses hierarchical clustering and statistical indexes to allow interactive navigation, with or without a-priori informa- tion, through numerous journeys stored in standard event log formats. The exploration of the underlying journeys can be done in the whole set of data available or driven by user goals in order to examine events and patterns in specific areas of interest. customer journey mapping,  process mining,  customer journey analytics,  hierarchical clustering,  sequence mining",cjm ex goal oriented exploration of customer journey maps using event logs and data analytics customer journey mapping cjm is an emerging area of research tackling issues related to customer behavior and user trajectories when consuming a service the increasing complexity of the service in dustry makes this type of tools popular amongst practitioners however to date it is not clear how a cjm can be used to depict hundreds or thousands of customer journeys inspired by process discovery techniques borrowed from process mining we present cjm explorer cjm ex cjm ex is a web interface that uses hierarchical clustering and statistical indexes to allow interactive navigation with or without a priori informa tion through numerous journeys stored in standard event log formats the exploration of the underlying journeys can be done in the whole set of data available or driven by user goals in order to examine events and patterns in specific areas of interest customer journey mapping process mining customer journey analytics hierarchical clustering sequence mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
105,1,"A process mining-based analysis of business process work-arounds Business process work-arounds are specific forms of incompliant behavior, where employees intentionally decide to deviate from the required procedures although they are aware of them. Detecting and understanding the work-arounds performed can guide organizations in redesigning and improving their processes and support systems. Existing process mining techniques for compliance checking and diagnosis of incompliant behavior rely on the available information in event logs and emphasize technological capabilities for analyzing this information. They do not distinguish intentional incompliance and do not address the sources of this behavior. In contrast, the paper builds on a list of generic types of work-arounds found in practice and explores whether and how they can be detected by process mining techniques. Results obtained for four work-around types in five real-life processes are reported. The remaining two types are not reflected in events logs and cannot be currently detected by process mining. The detected work-around data are further analyzed for identifying correlations between the frequency of specific work-around types and properties of the processes and of specific activities. The analysis results promote the understanding of work-around situations and sources. Business process work-arounds,  Process mining,  Compliance checking",a process mining based analysis of business process work arounds business process work arounds are specific forms of incompliant behavior where employees intentionally decide to deviate from the required procedures although they are aware of them detecting and understanding the work arounds performed can guide organizations in redesigning and improving their processes and support systems existing process mining techniques for compliance checking and diagnosis of incompliant behavior rely on the available information in event logs and emphasize technological capabilities for analyzing this information they do not distinguish intentional incompliance and do not address the sources of this behavior in contrast the paper builds on a list of generic types of work arounds found in practice and explores whether and how they can be detected by process mining techniques results obtained for four work around types in five real life processes are reported the remaining two types are not reflected in events logs and cannot be currently detected by process mining the detected work around data are further analyzed for identifying correlations between the frequency of specific work around types and properties of the processes and of specific activities the analysis results promote the understanding of work around situations and sources business process work arounds process mining compliance checking,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
106,1,"Goal-Aligned Categorization of Instance Variants in Knowledge-Intensive Processes Discovering and reasoning about deviations of business process executions (from intended designs) enables organizations to continuously evaluate their execution/performance relative to their strategic goals. We leverage the observation that a deviating process instance can be viewed as a valid variant of the intended process design provided it achieves the same goals as the intended process design. However, organizations often find it difficult to categorize and classify process execution deviations in a goal-based fashion (necessary to decide if a deviation represents a valid variant). Given that industry-scale knowledge-intensive processes typically manifest a large number of variants, this can pose a problem. In this paper, we propose an approach to help decide whether process instances in execution logs are valid variants using the goal-based notion of validity described above. Our proposed approach also enables analysis of the impact of contextual factors in the execution of specific goal-aligned process variants. We demonstrate our approach with an Eclipse-based plugin and evaluate it using an industry-scale setting in IT Incident Management with a process log of 25000 events. Variability,  Goals,  Business Process Mining",goal aligned categorization of instance variants in knowledge intensive processes discovering and reasoning about deviations of business process executions from intended designs enables organizations to continuously evaluate their execution performance relative to their strategic goals we leverage the observation that a deviating process instance can be viewed as a valid variant of the intended process design provided it achieves the same goals as the intended process design however organizations often find it difficult to categorize and classify process execution deviations in a goal based fashion necessary to decide if a deviation represents a valid variant given that industry scale knowledge intensive processes typically manifest a large number of variants this can pose a problem in this paper we propose an approach to help decide whether process instances in execution logs are valid variants using the goal based notion of validity described above our proposed approach also enables analysis of the impact of contextual factors in the execution of specific goal aligned process variants we demonstrate our approach with an eclipse based plugin and evaluate it using an industry scale setting in it incident management with a process log of 25000 events variability goals business process mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0.0,1.0,0.0
107,1,"Towards Goal-Oriented Conformance Checking Constructing a business process is important area between requirements engineering and business process management. Goal-oriented requirements analysis method is widely researched in requirements engineering and useful for reflecting organizational requirements to business process models, but actual business processes deviate from defined process models. Therefore, it is not sufficient for business process analysis only using model's information. It is important to analyze actual conducted business process logged data. Analyzing business process logged data is called process mining and detecting differences between models and logs is called conformance checking. A lot of conformance checking approaches mainly focus on process aspects of business process, but this is not sufficient for analysis whether actual business processes can satisfy organizational goals. In this paper, we propose a goal-oriented conformance checking approach which can detect deviations between logs and models, and can analyze the effects of the deviation. It is useful for evaluation of the detected deviation. We represent the effectiveness of our approach conducting a case study using the publicly available log. []",towards goal oriented conformance checking constructing a business process is important area between requirements engineering and business process management goal oriented requirements analysis method is widely researched in requirements engineering and useful for reflecting organizational requirements to business process models but actual business processes deviate from defined process models therefore it is not sufficient for business process analysis only using model s information it is important to analyze actual conducted business process logged data analyzing business process logged data is called process mining and detecting differences between models and logs is called conformance checking a lot of conformance checking approaches mainly focus on process aspects of business process but this is not sufficient for analysis whether actual business processes can satisfy organizational goals in this paper we propose a goal oriented conformance checking approach which can detect deviations between logs and models and can analyze the effects of the deviation it is useful for evaluation of the detected deviation we represent the effectiveness of our approach conducting a case study using the publicly available log,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,1.0,0.0
108,1,"Mining Agents' Goals in Agent-Oriented Business Processes When designing a business process, individual agents are assigned to perform tasks based on certain goals (i.e., designed process goals). However, based on their own interests, real-world agents often have different goals (i.e., agents' goals) and thus may behave differently than designed, often resulting in reduced effectiveness or efficiencies of the executed process. Moreover, existing business process research lacks effective methods for discovering agents' goals in the actual execution of the designed business processes. To address this problem, we propose an agent-oriented goal mining approach to modeling, discovering, and analyzing agents' goals in executed business processes using historical event logs and domain data. To the best of our knowledge, our research is the first to adopt the agents' goal perspective to study inconsistencies between the design and execution of business processes. Moreover, it also provides a useful tool for stakeholders to discover real-world agents' actual goals and thus provides insights for improving the task assignment mechanism or business process design in general. Goal mining,  agent-oriented business process,  belief-desire-intention model",mining agents goals in agent oriented business processes when designing a business process individual agents are assigned to perform tasks based on certain goals i e designed process goals however based on their own interests real world agents often have different goals i e agents goals and thus may behave differently than designed often resulting in reduced effectiveness or efficiencies of the executed process moreover existing business process research lacks effective methods for discovering agents goals in the actual execution of the designed business processes to address this problem we propose an agent oriented goal mining approach to modeling discovering and analyzing agents goals in executed business processes using historical event logs and domain data to the best of our knowledge our research is the first to adopt the agents goal perspective to study inconsistencies between the design and execution of business processes moreover it also provides a useful tool for stakeholders to discover real world agents actual goals and thus provides insights for improving the task assignment mechanism or business process design in general goal mining agent oriented business process belief desire intention model,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0.0,0.0,0.0
109,1,"Towards a Goal Recognition Model for the Organizational Memory Automatically building a model of the different goals underlying a workflow is very important for an organization's memory since we will be able to capture the implicit knowledge that is hosted in the employees. The automatic recognition of the goal that motivates an employee to execute a particular sequence of tasks is crucial to determine what tasks are expected to be performed next in order to achieve that goal within the dynamics of the organization. Furthermore, an early recognition of the employee's goal can also prevent deviations in his/her behavior from the expected behavior by providing personalized assistance. In this article we propose a model to capture regularities in the activities carried out by employees of an organization when they are pursuing different goals. An experimental evaluation was conducted in order to determine the validity of our approach and promising results are reported. goal recognition,  process mining,  organizational memory",towards a goal recognition model for the organizational memory automatically building a model of the different goals underlying a workflow is very important for an organization s memory since we will be able to capture the implicit knowledge that is hosted in the employees the automatic recognition of the goal that motivates an employee to execute a particular sequence of tasks is crucial to determine what tasks are expected to be performed next in order to achieve that goal within the dynamics of the organization furthermore an early recognition of the employee s goal can also prevent deviations in his her behavior from the expected behavior by providing personalized assistance in this article we propose a model to capture regularities in the activities carried out by employees of an organization when they are pursuing different goals an experimental evaluation was conducted in order to determine the validity of our approach and promising results are reported goal recognition process mining organizational memory,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0.0,0.0
110,1,"Goal-Heuristic Analysis Method for an Adaptive Process Mining Because of the rapid changes in the market environment and the uncertain demands from the customers, the investment in the information system by the corporate is increasing. This also resulted in the adoption of the process management system, which is intended for the adaptation to the speed of such changes, creation of competitiveness, and systematic management of the business process. To process the service demands from the customers that come in a dynamic manner, an analysis on the possible scope of changes on the recognition of the problems will be required, as well as the concept of data mining to redesign the process based on the adaptive decisions. The existing workflow mining technology was designed to extract business process redesign information from simple database fields or create a process model by collecting, identifying, and analyzing log information from the system that it could not be dynamically reconfigured by exploring the process flow suitable for new requests made on business process. In this study, an analytical method will be suggested using a heuristic algorithm based on the goals to create an adaptive process mining model that could provide a continuous service demand scenario that is created dynamically. Adaptive business process,  Process management system,  Process mining",goal heuristic analysis method for an adaptive process mining because of the rapid changes in the market environment and the uncertain demands from the customers the investment in the information system by the corporate is increasing this also resulted in the adoption of the process management system which is intended for the adaptation to the speed of such changes creation of competitiveness and systematic management of the business process to process the service demands from the customers that come in a dynamic manner an analysis on the possible scope of changes on the recognition of the problems will be required as well as the concept of data mining to redesign the process based on the adaptive decisions the existing workflow mining technology was designed to extract business process redesign information from simple database fields or create a process model by collecting identifying and analyzing log information from the system that it could not be dynamically reconfigured by exploring the process flow suitable for new requests made on business process in this study an analytical method will be suggested using a heuristic algorithm based on the goals to create an adaptive process mining model that could provide a continuous service demand scenario that is created dynamically adaptive business process process management system process mining,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.0,0.0,0.0
