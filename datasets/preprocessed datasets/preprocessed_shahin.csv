,label,n_words_in_Abstract,text,text_clean,Tarkoma S,Henry TR,Thakur S,Saha M,Buchmann A,Eliazar KJ,Mongazon-Cazavet B,Aravinthan G,Ristenpart T,Parekh S,Balaji B,Hawkey K,Ko AJ,An K,Billingsley W,Pasquier J,Zander S,Muriithi GM,Ortmann L,Nittel S,Neville F,Rigault JP,Fortes RP,Matera M,Matharu GS,Mishra A,Lee K,Baba AI,Vrbsky S,Salvini S,Tang B,Chen Z,Hefferman G,Wei T,Skočir P,Balasubramaniam B,Gordon A,James R,Newall L,Hay S,Kim JY,Hauswirth M,Salehi A,Corradi A,Liu Q,Zhu C,Chu TH,Chan HC,Porres I,Sun P,Rexford J,Azua M,Singh I,Kumar S,Phillimore P,Dolstra E,Kundu A,Farcas C,Judd G,Tsou T,Sundaresan N,Di Fabbrizio G,Wohlfart F,Abdlhamed M,Bae J,Pan Y,Chen L,Lv K,Dzvonyar D,Penta MD,Song M,Mattila AL,Berbers Y,Ramos-Hernandez DN,Conlon S,White T,Hieb M,Thomson C,Florio V,Shook D,Raman J,DuPont F,Adams B,Xu X,Zhu L,Eyl M,Kuz I,Reeves S,Castell M,Gmeiner J,Savor T,Sauvola T,Olsson HH,Fallik B,Rogers RO,Gorton I,Zhang Z,Fleurey F,Chen CT,Orban de Xivry G,Rahmer G,Erculiani F,Pereira Moreira GD,Marques da Cunha A,Rito Silva A,Yang Z,Marshall-Keim T,Moir K,publication_date_2011
0,0,162,"Vision: Augmenting WiFi Offloading with An Open-source Collaborative Platform Offloading mobile traffic to WiFi networks (WiFi Offloading) is a cost-effective technique to alleviate the pressure on mobile networks for meeting the surge of data capacity demand. However, most existing proposals from standards developing organizations (SDOs) and research communities are facing a deployment dilemma, either due to overlooking device limitations, lack user incentives, or missing operator supports. In this position paper, we introduce an open-source platform for WiFi offloading to tackle the deployment challenge. Our solution leverages the programmable feature of software-defined networking (SDN) to enhance extensibility and deployability in a collaborative manner. Inspired by our field measurements covering 4G/LTE and 802.11ac/n, we exploit context awareness as a use case to demonstrate the efficacy of our solution. We also discuss the potential usage by cloud service providers given the opportunities behind the growing popularity of mobile virtual network operators (MVNO). We have released our platform under open-source licenses to encourage future collaboration and development with SDOs and research communities. SDN, WiFi offloading, cloud service provider, mobile virtual network operator",vision augmenting wifi offloading with an open source collaborative platform offloading mobile traffic to wifi networks wifi offloading is a cost effective technique to alleviate the pressure on mobile networks for meeting the surge of data capacity demand however most existing proposals from standards developing organizations sdos and research communities are facing a deployment dilemma either due to overlooking device limitations lack user incentives or missing operator supports in this position paper we introduce an open source platform for wifi offloading to tackle the deployment challenge our solution leverages the programmable feature of software defined networking sdn to enhance extensibility and deployability in a collaborative manner inspired by our field measurements covering 4g lte and 802 11ac n we exploit context awareness as a use case to demonstrate the efficacy of our solution we also discuss the potential usage by cloud service providers given the opportunities behind the growing popularity of mobile virtual network operators mvno we have released our platform under open source licenses to encourage future collaboration and development with sdos and research communities sdn wifi offloading cloud service provider mobile virtual network operator,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
1,0,229,"Vision: The Case for Cellular Small Cells for Cloudlets Today's cellular networks are built with``macro cell'' basestations connected to the Internet via a rigid, complicated backhaul. Even with state-of-art technologies like LTE, users get limited throughput and high latency, with high variance. Performance enhancing IP boxes are deployed in the cellular operator's datacenters, far from the user. As a result, the most compelling cloudlet applications are difficult to realize on such networks and cloudlet researchers have thus far focused on Wi-Fi networks only. We argue that the cloudlet community should consider small cell networks in addition to Wi-Fi networks. Small cells, such as femtocells and picocells, are relatively new additions to the cellular standards. By reducing the cell size compared to the traditional macro cells, they increase spatial reuse of precious licensed frequencies. Users get higher bandwidth and lower latency, with relatively less variance. This architecture, where small cells are deployed simply with power and Ethernet connectivity, lends itself well to cloudlet augmentation. In this position paper, we describe why deployed macro cell basestations are unsuitable for cloudlet deployment. In contrast, we describe why a small cell architecture is amenable for cloudlet deployments. Our experience from operating a small cell testbed in licensed frequencies matches that reported by equipment vendors. The applications we care about require high throughput and low latency. In a cellular network this can be achieved today by augmenting small cells with powerful cloudlets. cloudlets, lte, small cells",vision the case for cellular small cells for cloudlets today s cellular networks are built with macro cell basestations connected to the internet via a rigid complicated backhaul even with state of art technologies like lte users get limited throughput and high latency with high variance performance enhancing ip boxes are deployed in the cellular operator s datacenters far from the user as a result the most compelling cloudlet applications are difficult to realize on such networks and cloudlet researchers have thus far focused on wi fi networks only we argue that the cloudlet community should consider small cell networks in addition to wi fi networks small cells such as femtocells and picocells are relatively new additions to the cellular standards by reducing the cell size compared to the traditional macro cells they increase spatial reuse of precious licensed frequencies users get higher bandwidth and lower latency with relatively less variance this architecture where small cells are deployed simply with power and ethernet connectivity lends itself well to cloudlet augmentation in this position paper we describe why deployed macro cell basestations are unsuitable for cloudlet deployment in contrast we describe why a small cell architecture is amenable for cloudlet deployments our experience from operating a small cell testbed in licensed frequencies matches that reported by equipment vendors the applications we care about require high throughput and low latency in a cellular network this can be achieved today by augmenting small cells with powerful cloudlets cloudlets lte small cells,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
2,0,144,"Visualizing the Network of Software Agents for Verification of Multiagent Systems The verification of Multiagent Systems (MAS) and Distributed Software Systems (DSS) has taken a special attention due to the growing demand of having DSS in recent years. The distributed functionality and lack of having a central control in MAS and DSS may cause to emerge new behaviors in the execution time. This unexpected behavior which was not seen in the requirements is known as emergent behavior and may cause irreparable damages. Detection of these emergent behaviors is more valuable and cost effective in the early phases compared to detecting them after the deployment. In this paper we propose a new technique for the detection of a specific type of emergent behavior in the design phase. We take the advantage of social network visualization in this method. The novelty and direct advantage of this technique is presenting the exact point and cause of emergent behavior. distributed software systems, emergent behavior, multiagent systems, scenario-based software engineering, social network visualization",visualizing the network of software agents for verification of multiagent systems the verification of multiagent systems mas and distributed software systems dss has taken a special attention due to the growing demand of having dss in recent years the distributed functionality and lack of having a central control in mas and dss may cause to emerge new behaviors in the execution time this unexpected behavior which was not seen in the requirements is known as emergent behavior and may cause irreparable damages detection of these emergent behaviors is more valuable and cost effective in the early phases compared to detecting them after the deployment in this paper we propose a new technique for the detection of a specific type of emergent behavior in the design phase we take the advantage of social network visualization in this method the novelty and direct advantage of this technique is presenting the exact point and cause of emergent behavior distributed software systems emergent behavior multiagent systems scenario based software engineering social network visualization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
3,0,160,"Volunteerism: A New Idea for Filling University Information Technology Needs The world-wide-web provides a great means for delivering information services to a university community[1,2]. An unexpected side-effect of using the web to deliver information services is that a community member can develop new services independent of the computing services department. As an example, one of the authors (a computer science student at California State University Chico) has independently developed a tool to help students create class schedules. This tool reads course information from the university's public web pages and uses it to automatically create custom class schedules for individual students. Over five thousand students (nearly half of all registering students) used this tool to create their Fall 2005 schedules.This paper presents the development and deployment of the scheduling tool as an example of information services volunteerism. It includes a description of the genesis of the tool, a discussion of how it has been integrated into the university supported information services, and an overview of the scheduling tool. class schedule, university services, volunteerism, world-wide-web",volunteerism a new idea for filling university information technology needs the world wide web provides a great means for delivering information services to a university community 1 2 an unexpected side effect of using the web to deliver information services is that a community member can develop new services independent of the computing services department as an example one of the authors a computer science student at california state university chico has independently developed a tool to help students create class schedules this tool reads course information from the university s public web pages and uses it to automatically create custom class schedules for individual students over five thousand students nearly half of all registering students used this tool to create their fall 2005 schedules this paper presents the development and deployment of the scheduling tool as an example of information services volunteerism it includes a description of the genesis of the tool a discussion of how it has been integrated into the university supported information services and an overview of the scheduling tool class schedule university services volunteerism world wide web,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
4,0,195,"VoxNet: An Interactive, Rapidly-Deployable Acoustic Monitoring Platform Distributed acoustic sensing underlies an increasingly important class of sensor network applications, from habitat monitoring and bioacoustic census to security applications and virtual fences. VoxNet is a complete hardware and software platform for distributed acoustic monitoring applications that focuses on three key goals: (1) rapid deployment in realistic environments; (2) a high level programming language that abstracts the user from platform and network details and compiles into a high performance distributed application; and (3) an interactive usage model based on run-time installable programs, with the ability to run the same high level program seamlessly over live or stored data. The VoxNet hardware is self-contained and weather-resistant, and supports a four-channel microphone array with automated time synchronization, localization, and network coordination. Using VoxNet, an investigator can visualize phenomena in real-time, develop and tune online analysis, and record raw data for off-line analysis and archival. This paper describes both the hardware and software elements of the platform, as well as the architecture required to support distributed programs running over a heterogeneous network. We characterize the performance of the platform, using both microbenchmarks that evaluate specific aspects of the platform and a real application running in the field. acoustic source localization, bioacoustics, distributed signal processing, platforms, wireless sensor networks",voxnet an interactive rapidly deployable acoustic monitoring platform distributed acoustic sensing underlies an increasingly important class of sensor network applications from habitat monitoring and bioacoustic census to security applications and virtual fences voxnet is a complete hardware and software platform for distributed acoustic monitoring applications that focuses on three key goals 1 rapid deployment in realistic environments 2 a high level programming language that abstracts the user from platform and network details and compiles into a high performance distributed application and 3 an interactive usage model based on run time installable programs with the ability to run the same high level program seamlessly over live or stored data the voxnet hardware is self contained and weather resistant and supports a four channel microphone array with automated time synchronization localization and network coordination using voxnet an investigator can visualize phenomena in real time develop and tune online analysis and record raw data for off line analysis and archival this paper describes both the hardware and software elements of the platform as well as the architecture required to support distributed programs running over a heterogeneous network we characterize the performance of the platform using both microbenchmarks that evaluate specific aspects of the platform and a real application running in the field acoustic source localization bioacoustics distributed signal processing platforms wireless sensor networks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
5,0,169,"VULCAN: Architecture-model-based Software Development Workbench Recently, software developers are faced with a fierce market competition with: diverse market needs, ever increasing number of features, and shortening product life cycle. To survive in this fierce competition, software developers are searching for methods and tools to develop various products with reduced time-to-market and improved quality. In response to these needs, we present a new CASE called VULCAN. VULCAN is a software development workbench comprising various tools for supporting the entire phases of feature-oriented product line software development from feature modeling to asset and product development. Especially, it provides several tools for supporting architecture-model-based software development where: (1) product line architectures can be specified using various architecture patterns, (2) application-specific architectures can be derived from the product line architecture specifications, (3) application-specific control components can be generated from the application architecture specifications, and (4) different deployment architectures can be configured with various component communication mechanisms. Of various tools included in VULCAN, we focus on this tool set for supporting architecture-model-based software development in this paper and demonstration. architecture-model-based, component connection mechanism, deployment architecture, feature-oriented, software product line",vulcan architecture model based software development workbench recently software developers are faced with a fierce market competition with diverse market needs ever increasing number of features and shortening product life cycle to survive in this fierce competition software developers are searching for methods and tools to develop various products with reduced time to market and improved quality in response to these needs we present a new case called vulcan vulcan is a software development workbench comprising various tools for supporting the entire phases of feature oriented product line software development from feature modeling to asset and product development especially it provides several tools for supporting architecture model based software development where 1 product line architectures can be specified using various architecture patterns 2 application specific architectures can be derived from the product line architecture specifications 3 application specific control components can be generated from the application architecture specifications and 4 different deployment architectures can be configured with various component communication mechanisms of various tools included in vulcan we focus on this tool set for supporting architecture model based software development in this paper and demonstration architecture model based component connection mechanism deployment architecture feature oriented software product line,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
6,0,141,"Wait for It: Determinants of Pull Request Evaluation Latency on GitHub The pull-based development model, enabled by git and popularised by collaborative coding platforms like BitBucket, Gitorius, and GitHub, is widely used in distributed software teams. While this model lowers the barrier to entry for potential contributors (since anyone can submit pull requests to any repository), it also increases the burden on integrators (i.e., members of a project's core team, responsible for evaluating the proposed changes and integrating them into the main development line), who struggle to keep up with the volume of incoming pull requests. In this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in GitHub. Using regression modeling on data extracted from a sample of GitHub projects using the Travis-CI continuous integration service, we find that latency is a complex issue, requiring many independent variables to explain adequately. []",wait for it determinants of pull request evaluation latency on github the pull based development model enabled by git and popularised by collaborative coding platforms like bitbucket gitorius and github is widely used in distributed software teams while this model lowers the barrier to entry for potential contributors since anyone can submit pull requests to any repository it also increases the burden on integrators i e members of a project s core team responsible for evaluating the proposed changes and integrating them into the main development line who struggle to keep up with the volume of incoming pull requests in this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in github using regression modeling on data extracted from a sample of github projects using the travis ci continuous integration service we find that latency is a complex issue requiring many independent variables to explain adequately,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
7,0,226,"WattShare: Detailed Energy Apportionment in Shared Living Spaces Within Commercial Buildings Increasing energy consumption of commercial buildings has motivated numerous energy tracking and monitoring systems in the recent years. A particular area that is less explored in this domain is that of energy apportionment whereby total energy usage of a shared space such as a building is disaggregated to attribute it to an individual occupant. This particular scenario of individual apportionment is important for increased transparency in the actual energy consumption of shared living spaces in commercial buildings e.g. hotels, student dormitories and hospitals amongst others. Accurate energy accounting is a difficult problem to solve using only a single smart meter. In this paper, we present a novel, scalable and a low cost energy apportionment system called WattShare that builds upon our EnergyLens architecture, where data from a common electricity meter and smartphones (carried by the occupants) is fused, and then used for detailed energy disaggregation. This information is then used to measure the room-level energy consumption. We evaluate WattShare using a week long deployment conducted in a student dormitory in a campus in India. We show that WattShare is able to disaggregate the total energy usage from a single smart meter to individual rooms with an average precision of 96.42% and average recall of 94.96%. WattShare achieves 86.42% energy apportionment accuracy which increases to 94.57% when an outlier room is removed. energy disaggregation, personal energy apportionment, smart meters, smartphones",wattshare detailed energy apportionment in shared living spaces within commercial buildings increasing energy consumption of commercial buildings has motivated numerous energy tracking and monitoring systems in the recent years a particular area that is less explored in this domain is that of energy apportionment whereby total energy usage of a shared space such as a building is disaggregated to attribute it to an individual occupant this particular scenario of individual apportionment is important for increased transparency in the actual energy consumption of shared living spaces in commercial buildings e g hotels student dormitories and hospitals amongst others accurate energy accounting is a difficult problem to solve using only a single smart meter in this paper we present a novel scalable and a low cost energy apportionment system called wattshare that builds upon our energylens architecture where data from a common electricity meter and smartphones carried by the occupants is fused and then used for detailed energy disaggregation this information is then used to measure the room level energy consumption we evaluate wattshare using a week long deployment conducted in a student dormitory in a campus in india we show that wattshare is able to disaggregate the total energy usage from a single smart meter to individual rooms with an average precision of 96 42 and average recall of 94 96 wattshare achieves 86 42 energy apportionment accuracy which increases to 94 57 when an outlier room is removed energy disaggregation personal energy apportionment smart meters smartphones,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
8,0,153,"Weaving in Patterns into It Infrastructure Models: Industry Case and Exemplary Approaches Architectural patterns are a helpful means for designing IT architectures, as they facilitate re-using proven knowledge (good practices) from previous exercises. Furthermore referencing a pattern in an architecture model helps improving the understandability of the model, as it directs to a comprehensive description of the pattern, but does not require to include the full description into the model. In this paper we describe how patterns can be woven into architecture models, focusing on deployment views of the IT infrastructure. Two different modeling approaches, Fundamental Modeling Concepts (FMC) and ArchiMate, are compared based on a real-world case concerning the infrastructure architecture of a large data center. This paper provides practical insights for IT architects from the industry by discussing the practical case and comparing both modeling approaches. Furthermore, it is supposed to intensify the exchange between industry experts and scientific researchers and it should motivate pursuing further research concerning patterns and IT infrastructure models. it infrastructure, software architecture",weaving in patterns into it infrastructure models industry case and exemplary approaches architectural patterns are a helpful means for designing it architectures as they facilitate re using proven knowledge good practices from previous exercises furthermore referencing a pattern in an architecture model helps improving the understandability of the model as it directs to a comprehensive description of the pattern but does not require to include the full description into the model in this paper we describe how patterns can be woven into architecture models focusing on deployment views of the it infrastructure two different modeling approaches fundamental modeling concepts fmc and archimate are compared based on a real world case concerning the infrastructure architecture of a large data center this paper provides practical insights for it architects from the industry by discussing the practical case and comparing both modeling approaches furthermore it is supposed to intensify the exchange between industry experts and scientific researchers and it should motivate pursuing further research concerning patterns and it infrastructure models it infrastructure software architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
9,0,161,"Web Based Poultry Farm Monitoring System Using Wireless Sensor Network In this paper, we have proposed and developed a Poultry Farm Monitoring System based on Wireless Sensor Network (WSN) using Crossbow's TelosB motes integrated with commercial sensors capable of measuring temperature and humidity values. The data collected from the sensors is uploaded to an online database using an agent program and afterward accessed via the internet using web analysis applications. The feasibility of the developed system was tested by deploying the proposed system at N-W.F.P. Agricultural University's research poultry farm in Peshawar in the North-Western Frontier Province of Pakistan. To emulate the proposed idea, the data collected during a daylong experiment was put to test, evaluating the WSN's reliability and its ability to detect and report anomalies in the environment. This paper is the first step towards WSN based poultry farm monitoring systems. We have provided an online monitoring solution for poultry farms and tested its feasibility and reliability by presenting a thorough data analysis of our pilot deployment. WSN, poultry farm monitoring system",web based poultry farm monitoring system using wireless sensor network in this paper we have proposed and developed a poultry farm monitoring system based on wireless sensor network wsn using crossbow s telosb motes integrated with commercial sensors capable of measuring temperature and humidity values the data collected from the sensors is uploaded to an online database using an agent program and afterward accessed via the internet using web analysis applications the feasibility of the developed system was tested by deploying the proposed system at n w f p agricultural university s research poultry farm in peshawar in the north western frontier province of pakistan to emulate the proposed idea the data collected during a daylong experiment was put to test evaluating the wsn s reliability and its ability to detect and report anomalies in the environment this paper is the first step towards wsn based poultry farm monitoring systems we have provided an online monitoring solution for poultry farms and tested its feasibility and reliability by presenting a thorough data analysis of our pilot deployment wsn poultry farm monitoring system,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
10,0,221,"Web Services Enterprise Security Architecture: A Case Study Web Services (WS hereafter) Security is a crucial aspect for technologies based on this paradigm to be completely adopted by the industry. As a consequence, a lot of initiativesof initiatives have arisen during the last years setting as their main purpose the standardization of the security factors related to this paradigm. In fact, over the past years, the most important consortiums ofof Internet Internet, like IETF, W3C or OASIS, are producing a huge number of WS-based security standards. Despite of this growing, there's not exist yet a process that guides developers in the critical task of integrating security within all the stages of the development's life cycle of WS-based software. Such a process should facilitate developers in the activities of web service-specific security requirents specification, web services-based security architecture design and web services security standards selection, integration and deployment. In this article we briefly present the PWSSec (Process for Web Services Security) process that is composed of three stages, WSSecReq (Web Services Security Requirents), WSSecArch (Web Services Security Architecture) and WSSecTech (Web Services Security Technologies) that accomplishes the mentioned activities, respectively. In this article wWe also provide an thorough explanation of the WSSecArch (Web Services Security Stage) stage intended to design the web services-based security architecture. In addition, a real case study where this stage in being applied is also included. security, software architecture, software development process, web services",web services enterprise security architecture a case study web services ws hereafter security is a crucial aspect for technologies based on this paradigm to be completely adopted by the industry as a consequence a lot of initiativesof initiatives have arisen during the last years setting as their main purpose the standardization of the security factors related to this paradigm in fact over the past years the most important consortiums ofof internet internet like ietf w3c or oasis are producing a huge number of ws based security standards despite of this growing there s not exist yet a process that guides developers in the critical task of integrating security within all the stages of the development s life cycle of ws based software such a process should facilitate developers in the activities of web service specific security requirents specification web services based security architecture design and web services security standards selection integration and deployment in this article we briefly present the pwssec process for web services security process that is composed of three stages wssecreq web services security requirents wssecarch web services security architecture and wssectech web services security technologies that accomplishes the mentioned activities respectively in this article wwe also provide an thorough explanation of the wssecarch web services security stage stage intended to design the web services based security architecture in addition a real case study where this stage in being applied is also included security software architecture software development process web services,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
11,0,158,"Web Services Integration on the Fly for Service-oriented Computing and Simulation In a net-centric environment, data, tools and people operate in a distributed network. We explore a generic, flexible, scalable, usable and intelligent web services architecture framework that enables sharing and integration of web services on the fly. As an example application, the envisioned Web Service Architecture Intelligent Framework (WSAIF) is applied to the Modeling, Virtual Environments and Simulation (MOVES) domain. This paper elaborates on the design and implementation of web services for the Scenario Authoring and Visualization for Advanced Graphical Environments (SAVAGE) web-based graphics models and discusses the deployment and test results of those services. The study and comparison of various modeling techniques that enable integration, orchestration and adaptation of composable web services is mentioned. The modeling techniques are essential to and will eventually be used in WSAIF Orchestrator and Adaptor components. The paper further explains how WSAIF software agents and modeling data enable web services integration on the fly. The paper concludes with recommendations for future work. semantic web services, service oriented architecture, software agents, web services, web services architecture",web services integration on the fly for service oriented computing and simulation in a net centric environment data tools and people operate in a distributed network we explore a generic flexible scalable usable and intelligent web services architecture framework that enables sharing and integration of web services on the fly as an example application the envisioned web service architecture intelligent framework wsaif is applied to the modeling virtual environments and simulation moves domain this paper elaborates on the design and implementation of web services for the scenario authoring and visualization for advanced graphical environments savage web based graphics models and discusses the deployment and test results of those services the study and comparison of various modeling techniques that enable integration orchestration and adaptation of composable web services is mentioned the modeling techniques are essential to and will eventually be used in wsaif orchestrator and adaptor components the paper further explains how wsaif software agents and modeling data enable web services integration on the fly the paper concludes with recommendations for future work semantic web services service oriented architecture software agents web services web services architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
12,0,249,"Web Services: The Story So Far - an Academic and Industrial Account Recent advances in networks, information and computation grids, and the Web have resulted in the proliferation of a multitude of physically distributed and autonomously developed component Web services. The W3C Web Services Architecture defines ""Web service as a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface described in a machine processible format (specifically WSDL) and other systems can interact with it in a manner prescribed by its description using SOAP messages, typically conveyed using HTTP with an XML serialization in conjunction with other Web-related standards"". So, web-services constitute a distributed computing infrastructure made up of many different systems trying to communicate over the Internet to virtually form a single logical system. Web-services are an effective means for linking loosely coupled systems together using a technology that does not bind to a particular component model, programming language or platform. Used according to the semantic web principles, semantic web services offer key integration capabilities in businesses as well as in scientific research over the Internet and corporate intranets, and so are applicable to a broad variety of applications including e-Enterprise, e-Business, e-Government, and e-Science. Correspondingly, the construction and deployment of composite services by combining and reusing independently developed component services is an important capability in the emerging Web-based computing infrastructure. In this presentation we will focus on a detailed analysis of existing discovery reasoning mechanisms and the current key solutions for composition of component services. We will conclude with current roadblocks and pilot applications. []",web services the story so far an academic and industrial account recent advances in networks information and computation grids and the web have resulted in the proliferation of a multitude of physically distributed and autonomously developed component web services the w3c web services architecture defines web service as a software system designed to support interoperable machine to machine interaction over a network it has an interface described in a machine processible format specifically wsdl and other systems can interact with it in a manner prescribed by its description using soap messages typically conveyed using http with an xml serialization in conjunction with other web related standards so web services constitute a distributed computing infrastructure made up of many different systems trying to communicate over the internet to virtually form a single logical system web services are an effective means for linking loosely coupled systems together using a technology that does not bind to a particular component model programming language or platform used according to the semantic web principles semantic web services offer key integration capabilities in businesses as well as in scientific research over the internet and corporate intranets and so are applicable to a broad variety of applications including e enterprise e business e government and e science correspondingly the construction and deployment of composite services by combining and reusing independently developed component services is an important capability in the emerging web based computing infrastructure in this presentation we will focus on a detailed analysis of existing discovery reasoning mechanisms and the current key solutions for composition of component services we will conclude with current roadblocks and pilot applications,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
13,0,88,"Whac-A-Bee: A Sensor Network Game This paper illustrates both challenges and benefits found in expanding a traditional game concept to a situated environment with a distributed set of wireless sensing modules. Our pervasive game equivalent of the Whac-A-Mole game, Whac-A-Bee, retains the find-and-seek aspects of the original game while extending the location, the number of players, and the time-span in which it can be played. We discuss the obstacles met during this work, and specifically address challenges in making the game robust and flexible enough for large and long-term deployments in unknown territory. pervasive game, sensor network, wireless",whac a bee a sensor network game this paper illustrates both challenges and benefits found in expanding a traditional game concept to a situated environment with a distributed set of wireless sensing modules our pervasive game equivalent of the whac a mole game whac a bee retains the find and seek aspects of the original game while extending the location the number of players and the time span in which it can be played we discuss the obstacles met during this work and specifically address challenges in making the game robust and flexible enough for large and long term deployments in unknown territory pervasive game sensor network wireless,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
14,0,109,"What Bugs Live in the Cloud? A Study of 3000+ Issues in Cloud Systems We conduct a comprehensive study of development and deployment issues of six popular and important cloud systems (Hadoop MapReduce, HDFS, HBase, Cassandra, ZooKeeper and Flume). From the bug repositories, we review in total 21,399 submitted issues within a three-year period (2011-2014). Among these issues, we perform a deep analysis of 3655 ""vital"" issues (i.e., real issues affecting deployments) with a set of detailed classifications. We name the product of our one-year study Cloud Bug Study database (CbsDB) [9], with which we derive numerous interesting insights unique to cloud systems. To the best of our knowledge, our work is the largest bug study for cloud systems to date. []",what bugs live in the cloud a study of 3000 issues in cloud systems we conduct a comprehensive study of development and deployment issues of six popular and important cloud systems hadoop mapreduce hdfs hbase cassandra zookeeper and flume from the bug repositories we review in total 21 399 submitted issues within a three year period 2011 2014 among these issues we perform a deep analysis of 3655 vital issues i e real issues affecting deployments with a set of detailed classifications we name the product of our one year study cloud bug study database cbsdb 9 with which we derive numerous interesting insights unique to cloud systems to the best of our knowledge our work is the largest bug study for cloud systems to date,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
15,0,68,"When Agile Meets OO Testing: A Case Study This paper describes the testing approach for an object-oriented rich client application based on an agile software development process. The paper starts with an overview of project and the testing strategy being used. It then goes on to describe the test tools used in the project and the results achieved. The paper ends with a discussion of the discovered defects, their distribution and improvements for the testing process. agile testing, continuous integration, defect rates, integration testing, object-oriented testing, technical debts, unit testing",when agile meets oo testing a case study this paper describes the testing approach for an object oriented rich client application based on an agile software development process the paper starts with an overview of project and the testing strategy being used it then goes on to describe the test tools used in the project and the results achieved the paper ends with a discussion of the discovered defects their distribution and improvements for the testing process agile testing continuous integration defect rates integration testing object oriented testing technical debts unit testing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
16,0,109,"When Blazing a Trail Leads over the Mountainside Cliff: Lessons Learned from Our First Rapid Deployment At SIGUCCS 2007, one of the themes promoted by James Hilton of the University of Virginia was that in the world of IT at colleges and universities, we are likely to see more change happening more quickly than ever before, and that we need to learn ways to make the change happen as quickly and as painlessly as possible. A variety of circumstances led UVa to make a switch in its enterprise calendaring system between December 4, 2007 and January 15, 2008. Learn about the reasons behind this change, the obstacles we encountered, the user experience of the change and the lessons we learned in our first rapid change. rapid deployment",when blazing a trail leads over the mountainside cliff lessons learned from our first rapid deployment at siguccs 2007 one of the themes promoted by james hilton of the university of virginia was that in the world of it at colleges and universities we are likely to see more change happening more quickly than ever before and that we need to learn ways to make the change happen as quickly and as painlessly as possible a variety of circumstances led uva to make a switch in its enterprise calendaring system between december 4 2007 and january 15 2008 learn about the reasons behind this change the obstacles we encountered the user experience of the change and the lessons we learned in our first rapid change rapid deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
17,0,84,"When IT Meets Telco: RAN As a Service Network Virtualization Functions (NFV) and Software-Defined Networking (SDN) are changing the landscape of the telecommunications industry, particularly infrastructure and network systems of Telco operators with the introduction of cloud computing, paradigms of virtualization and software approaches. In this paper, we describe a demonstrator which shows how IT technologies reduce the time of deployment of a wireless infrastructure. In less than 60s, a wireless LTE network is available for connecting Smartphone's. In this demo, the eNodeB, virtualized using docker containers, is orchestrated by OpenStack heat. LTE, docker, heat, openairinterface, openstack, orchestration, virtualization",when it meets telco ran as a service network virtualization functions nfv and software defined networking sdn are changing the landscape of the telecommunications industry particularly infrastructure and network systems of telco operators with the introduction of cloud computing paradigms of virtualization and software approaches in this paper we describe a demonstrator which shows how it technologies reduce the time of deployment of a wireless infrastructure in less than 60s a wireless lte network is available for connecting smartphone s in this demo the enodeb virtualized using docker containers is orchestrated by openstack heat lte docker heat openairinterface openstack orchestration virtualization,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
18,0,184,"Where the Curb Meets the Cloud: Urban Innovation in the Digital Age Cities have benefited from the three greatest technological innovations of the past 200 years: the steam engine, electrification, and the automobile. But each advance has created its own challenges, including pollution, overcrowding, sprawl. As the digital revolution transforms cities once again, how can we make sure it improves quality of life while minimizing the downside? With population density comes the possibility of deploying network connectivity and wayfinding at lower cost, but density also increases complexity of deployment. How can digital technology reduce the bad friction of urban environments, such as congestion, cost, and complexity, while increasing good friction, such as all of the serendipitous interactions that cities encourage? Underlying all this change is ubiquitous connectivity and mobile technology, from the phones people carry with them to the supporting devices and network endpoints embedded in the urban infrastructure. Sidewalk Labs is an Alphabet company that works with cities to develop new technology that can improve urban life. We will discuss some of our discoveries and beliefs, and talk about our plans to use mobile technology to help cities take full advantage of the digital revolution. sidewalk labs, ubiquitous and mobile computing, urban technologies",where the curb meets the cloud urban innovation in the digital age cities have benefited from the three greatest technological innovations of the past 200 years the steam engine electrification and the automobile but each advance has created its own challenges including pollution overcrowding sprawl as the digital revolution transforms cities once again how can we make sure it improves quality of life while minimizing the downside with population density comes the possibility of deploying network connectivity and wayfinding at lower cost but density also increases complexity of deployment how can digital technology reduce the bad friction of urban environments such as congestion cost and complexity while increasing good friction such as all of the serendipitous interactions that cities encourage underlying all this change is ubiquitous connectivity and mobile technology from the phones people carry with them to the supporting devices and network endpoints embedded in the urban infrastructure sidewalk labs is an alphabet company that works with cities to develop new technology that can improve urban life we will discuss some of our discoveries and beliefs and talk about our plans to use mobile technology to help cities take full advantage of the digital revolution sidewalk labs ubiquitous and mobile computing urban technologies,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
19,0,163,"Which Visualizations Work, for What Purpose, for Whom?: Evaluating Visualizations of Terrestrial and Aquatic Systems A need for better ecology visualization tools is well documented, and development of these is underway, including our own NSF funded Visualization of Terrestrial and Aquatic Systems (VISTAS) project, now beginning its second of four years. VISTAS' goal is not only to devise visualizations that help ecologists in research and in communicating that research, but also to evaluate the visualizations and software. Thus, we ask ""which visualizations work, for what purpose, and for which audiences,"" and our project involves equal participation of ecologists, computer scientists, and social scientists. We have begun to study visualization use by ecologists, assessed some existing software products, and implemented a prototype. This position paper reports how we apply social science methods in establishing context for VISTAS' evaluation and development. We describe our initial surveys of ecologists and ecology journals to determine current visualization use, outline our visualization evaluation strategies, and in conclusion pose questions critical to the evaluation, deployment, and adoption of VISTAS and VISTAS-like visualizations and software. ecology informatics, scientific visualization, software evaluation, visualization development lifecycle",which visualizations work for what purpose for whom evaluating visualizations of terrestrial and aquatic systems a need for better ecology visualization tools is well documented and development of these is underway including our own nsf funded visualization of terrestrial and aquatic systems vistas project now beginning its second of four years vistas goal is not only to devise visualizations that help ecologists in research and in communicating that research but also to evaluate the visualizations and software thus we ask which visualizations work for what purpose and for which audiences and our project involves equal participation of ecologists computer scientists and social scientists we have begun to study visualization use by ecologists assessed some existing software products and implemented a prototype this position paper reports how we apply social science methods in establishing context for vistas evaluation and development we describe our initial surveys of ecologists and ecology journals to determine current visualization use outline our visualization evaluation strategies and in conclusion pose questions critical to the evaluation deployment and adoption of vistas and vistas like visualizations and software ecology informatics scientific visualization software evaluation visualization development lifecycle,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
20,0,138,"WhoWas: A Platform for Measuring Web Deployments on IaaS Clouds Public infrastructure-as-a-service (IaaS) clouds such as Amazon EC2 and Microsoft Azure host an increasing number of web services. The dynamic, pay-as-you-go nature of modern IaaS systems enable web services to scale up or down with demand, and only pay for the resources they need. We are unaware, however, of any studies reporting on measurements of the patterns of usage over time in IaaS clouds as seen in practice. We fill this gap, offering a measurement platform that we call WhoWas. Using active, but lightweight, probing, it enables associating web content to public IP addresses on a day-by-day basis. We exercise WhoWas to provide the first measurement study of churn rates in EC2 and Azure, the efficacy of IP blacklists for malicious activity in clouds, the rate of adoption of new web software by public cloud customers, and more. active measurement, azure, cloud computing, ec2, web service",whowas a platform for measuring web deployments on iaas clouds public infrastructure as a service iaas clouds such as amazon ec2 and microsoft azure host an increasing number of web services the dynamic pay as you go nature of modern iaas systems enable web services to scale up or down with demand and only pay for the resources they need we are unaware however of any studies reporting on measurements of the patterns of usage over time in iaas clouds as seen in practice we fill this gap offering a measurement platform that we call whowas using active but lightweight probing it enables associating web content to public ip addresses on a day by day basis we exercise whowas to provide the first measurement study of churn rates in ec2 and azure the efficacy of ip blacklists for malicious activity in clouds the rate of adoption of new web software by public cloud customers and more active measurement azure cloud computing ec2 web service,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
21,0,455,"(Wiki + ResTechs) = (Fresh Documentation + Organic Knowledge Management + Training Materials + Good, Cheap Technical Writers) Most Information Technology Departments in academia have their historical roots in a culture best described by words such as ""geeks,"" ""beta-testers,"" ""troubleshooters,"" ""debuggers,"" ""early adopters,"" and so on. This culture was partly created by the fact that the mission of academia is indeed to keep looking forward to new developments and cutting-edge technologies. It is the role of industry to adopt the outcome of academia's frenetic efforts to move forward and to then produce commodity-like products. At the same time, in most academic institutions, Information Technology Departments have also evolved into providers of services that use mature but still evolving technology. The consumers (customers) have come to treat these technologies as commodity products or utilities, and they expect to be able to easily learn how to use them, and to be able to easily figure out what are the services offered and how they can be configured. In such an environment, ""deployment cycles"" and ""personnel training"" are often seen as a complete waste of time by the academics, as they themselves are pushing for the next generation of technology to be adopted by IT as fast as possible. For these reasons, the creation of detailed documentation by technical writers is usually not deemed worth funding, and with the advent of internet searches and FAQ blogging is seen often as completely unnecessary. This is a true statement for many areas of technology, where there exists ambient literature and FAQs (such as any MS Office product, for example). It is disastrous for customer satisfaction though, if it is applied on issues of configuration that are specific to the institution. Nevertheless, this type of documentation, the kind that describes ""how do we do things here"" is also passed up as a secondary, non-glamorous task. The fact that usually ""the way we do things"" also changes very rapidly, adds another layer of discouragement to webmasters and FAQ owners to try to keep up and maintain relevant content. The final issue that IT departments also have to grapple with is the thin staffing and the fact that even if technical writers were at hand, the staff that ""owns"" the information to be documented does not have time to explain it or relay it to others. This was the kind of impossible situation that Rice University Information Technology had reached: stale documentation, orphaned ownership of the content, a customer base that was very conducive to using documentation and very resentful of the fact that we were not providing any. We experimented with ""wiki"" technology and we experimented with involving different groups within IT as potential technical writers. We were able to overcome the impasse and discovered some added benefits that have affected the training that we offer to our customer base. Wiki technology, knowledge management",wiki restechs fresh documentation organic knowledge management training materials good cheap technical writers most information technology departments in academia have their historical roots in a culture best described by words such as geeks beta testers troubleshooters debuggers early adopters and so on this culture was partly created by the fact that the mission of academia is indeed to keep looking forward to new developments and cutting edge technologies it is the role of industry to adopt the outcome of academia s frenetic efforts to move forward and to then produce commodity like products at the same time in most academic institutions information technology departments have also evolved into providers of services that use mature but still evolving technology the consumers customers have come to treat these technologies as commodity products or utilities and they expect to be able to easily learn how to use them and to be able to easily figure out what are the services offered and how they can be configured in such an environment deployment cycles and personnel training are often seen as a complete waste of time by the academics as they themselves are pushing for the next generation of technology to be adopted by it as fast as possible for these reasons the creation of detailed documentation by technical writers is usually not deemed worth funding and with the advent of internet searches and faq blogging is seen often as completely unnecessary this is a true statement for many areas of technology where there exists ambient literature and faqs such as any ms office product for example it is disastrous for customer satisfaction though if it is applied on issues of configuration that are specific to the institution nevertheless this type of documentation the kind that describes how do we do things here is also passed up as a secondary non glamorous task the fact that usually the way we do things also changes very rapidly adds another layer of discouragement to webmasters and faq owners to try to keep up and maintain relevant content the final issue that it departments also have to grapple with is the thin staffing and the fact that even if technical writers were at hand the staff that owns the information to be documented does not have time to explain it or relay it to others this was the kind of impossible situation that rice university information technology had reached stale documentation orphaned ownership of the content a customer base that was very conducive to using documentation and very resentful of the fact that we were not providing any we experimented with wiki technology and we experimented with involving different groups within it as potential technical writers we were able to overcome the impasse and discovered some added benefits that have affected the training that we offer to our customer base wiki technology knowledge management,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
22,0,95,WiMAX Opportunities for Emerging Markets WiMAX is a novel wireless broadband technology with lots of promises for Urban and rural areas. It has the potential of bringing true wireless broadband to the masses at a fraction of the cost of existing solutions. The technology comes with many benefits and a few challenges. This tutorial discusses the technology benefits and applications. It also addresses some of the hurdles towards deployment and what the industry is doing to overcome those hurdles. A special focus will be given to the Middle East and an overview of the opportunities that exist will be presented. [],wimax opportunities for emerging markets wimax is a novel wireless broadband technology with lots of promises for urban and rural areas it has the potential of bringing true wireless broadband to the masses at a fraction of the cost of existing solutions the technology comes with many benefits and a few challenges this tutorial discusses the technology benefits and applications it also addresses some of the hurdles towards deployment and what the industry is doing to overcome those hurdles a special focus will be given to the middle east and an overview of the opportunities that exist will be presented,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
23,0,254,"Windows 7: Trials and Tribulations Giving up on something you know in order to risk the unknown usually proves to be a challenge. It is customary to have created a comfortable atmosphere of using what's now considered the old, but there is a demand for the new. So now is the time to learn, explore and ultimately understand the new. This new version of Windows is radically different from the old version. It requires much time, patience and planning, which in the Information Technology world is something we do not have. This requires us to collectively pool our experiences together and collaborate to achieve our goals. Windows XP has been around for a long time and most have become comfortable in developing and deploying it. Windows 7 offers a substantial upgrade from Windows XP in regards to features for users, but not necessarily for image developers. Image developers are discovering that creating a Windows 7 image is not an easy process compared to using Windows XP. It requires undergoing extensive test trials, researching new approaches to conduct old tasks, and giving up on features that can no longer be achieved using this new operating system. At Lewis & Clark College, we have migrated all of our PC computers and dual-boot labs to Windows 7 64-bit from Windows XP. We overcame many obstacles while striving to meet our goals. This paper will highlight the in-depth process it took to develop and deploy a Windows 7 image for our environment, including the researched documentation and resources that assisted us through our endeavor. deployment, sysprep, windows 7",windows 7 trials and tribulations giving up on something you know in order to risk the unknown usually proves to be a challenge it is customary to have created a comfortable atmosphere of using what s now considered the old but there is a demand for the new so now is the time to learn explore and ultimately understand the new this new version of windows is radically different from the old version it requires much time patience and planning which in the information technology world is something we do not have this requires us to collectively pool our experiences together and collaborate to achieve our goals windows xp has been around for a long time and most have become comfortable in developing and deploying it windows 7 offers a substantial upgrade from windows xp in regards to features for users but not necessarily for image developers image developers are discovering that creating a windows 7 image is not an easy process compared to using windows xp it requires undergoing extensive test trials researching new approaches to conduct old tasks and giving up on features that can no longer be achieved using this new operating system at lewis clark college we have migrated all of our pc computers and dual boot labs to windows 7 64 bit from windows xp we overcame many obstacles while striving to meet our goals this paper will highlight the in depth process it took to develop and deploy a windows 7 image for our environment including the researched documentation and resources that assisted us through our endeavor deployment sysprep windows 7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
24,0,220,"Wireless Adhoc Sensor and Actuator Networks on the Farm Agriculture accounts for a significant portion of the GDP in most developed countries. However, managing farms, particularly large-scale extensive farming systems, is hindered by lack of data and increasing shortage of labour. We have deployed a large heterogeneous sensor network on a working farm to explore sensor network applications that can address some of the issues identified above. Our network is solar powered and has been running for over 6 months. The current deployment consists of over 40 moisture sensors that provide soil moisture profiles at varying depths, weight sensors to compute the amount of food and water consumed by animals, electronic tag readers, up to 40 sensors that can be used to track animal movement (consisting of GPS, compass and accelerometers), and 20 sensor/actuators that can be used to apply different stimuli (audio, vibration and mild electric shock) to the animal. The static part of the network is designed for 24/7 operation and is linked to the Internet via a dedicated high-gain radio link, also solar powered. The initial goals of the deployment are to provide a testbed for sensor network research in programmability and data handling while also being a vital tool for scientists to study animal behavior. Our longer term aim is to create a management system that completely transforms the way farms are managed. farm management, sensor network applications",wireless adhoc sensor and actuator networks on the farm agriculture accounts for a significant portion of the gdp in most developed countries however managing farms particularly large scale extensive farming systems is hindered by lack of data and increasing shortage of labour we have deployed a large heterogeneous sensor network on a working farm to explore sensor network applications that can address some of the issues identified above our network is solar powered and has been running for over 6 months the current deployment consists of over 40 moisture sensors that provide soil moisture profiles at varying depths weight sensors to compute the amount of food and water consumed by animals electronic tag readers up to 40 sensors that can be used to track animal movement consisting of gps compass and accelerometers and 20 sensor actuators that can be used to apply different stimuli audio vibration and mild electric shock to the animal the static part of the network is designed for 24 7 operation and is linked to the internet via a dedicated high gain radio link also solar powered the initial goals of the deployment are to provide a testbed for sensor network research in programmability and data handling while also being a vital tool for scientists to study animal behavior our longer term aim is to create a management system that completely transforms the way farms are managed farm management sensor network applications,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
25,0,282,"Wireless Carts: An Inexpensive Education and Research Platform As part of a complete wireless networking education, students must have an in-depth understanding of basic concepts such as signal propagation, environmental effects on RF signals, FCC regulations and limits, power levels, antenna construction and antenna operation. Lecture based curricula can only go so far in preparing a wireless professional to succeed in industry. To be complete, the student must have practical experience.Our wireless coursework is comprised of three courses, the first of which is a wireless concepts course. This course has a significant hands-on component that requires students to understand the tools while applying what they are learning about the physical layer and basic network operation. The students engage in two very large projects; a wireless building survey and signal propagation testing using specialized equipment.As part of the projects, students create a series of experiments with a variety of equipment and provide useable test data. Examples of the tests include interference, Fresnel zone effects, throughput, range and the effects of multi-path on signals. However, in the presence of an increasing number of wireless networks, obtaining real world, reliable data illustrating the various physical layer phenomena is difficult. Our solution was to build several wireless carts outfitted with various antennas, transmission equipment from different portions of the spectrum and that used different encoding or modulation techniques. In addition, a major requirement was that the carts be able to operate away from infrastructure support, including AC power.The carts have enabled students to isolate themselves from other wireless signals and have provided an extremely adaptive platform for experiments and projects. This paper will describe the coursework, projects, functions, costs, lessons learned and the data gathered as a result of their deployment. carts, platform, teaching, wireless",wireless carts an inexpensive education and research platform as part of a complete wireless networking education students must have an in depth understanding of basic concepts such as signal propagation environmental effects on rf signals fcc regulations and limits power levels antenna construction and antenna operation lecture based curricula can only go so far in preparing a wireless professional to succeed in industry to be complete the student must have practical experience our wireless coursework is comprised of three courses the first of which is a wireless concepts course this course has a significant hands on component that requires students to understand the tools while applying what they are learning about the physical layer and basic network operation the students engage in two very large projects a wireless building survey and signal propagation testing using specialized equipment as part of the projects students create a series of experiments with a variety of equipment and provide useable test data examples of the tests include interference fresnel zone effects throughput range and the effects of multi path on signals however in the presence of an increasing number of wireless networks obtaining real world reliable data illustrating the various physical layer phenomena is difficult our solution was to build several wireless carts outfitted with various antennas transmission equipment from different portions of the spectrum and that used different encoding or modulation techniques in addition a major requirement was that the carts be able to operate away from infrastructure support including ac power the carts have enabled students to isolate themselves from other wireless signals and have provided an extremely adaptive platform for experiments and projects this paper will describe the coursework projects functions costs lessons learned and the data gathered as a result of their deployment carts platform teaching wireless,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
26,0,238,"Wireless Sensor Network Aided Search and Rescue in Trails In recent years, wireless sensor networks have been used in applications of data gathering and target localization across large geographical areas. In this paper, we study the issues involved in applying wireless sensor networks to search and rescue of lost hikers in trails and focus on the optimal placement of sensors and access points such that the cost of search and rescue is minimized. Particularly, we address two problems: a) how to identify the lost hiker position as accurately as possible, i.e., obtain a small trail segments containing the lost hiker; and (b) how to search efficiently in trail segments for different trail topologies and search agent capabilities. For the optimal access point deployment problem, we propose theoretical models that consider both efficiency and accuracy criteria and present analytical results for simpler trail topologies. For complicated graph topologies, we develop efficient heuristic algorithms with various heuristics. After access point deployment is decided, the actual cost of search in individual trail segment can be computed. We analyze four different types of search and rescue agents, present algorithms to find the optimal search paths for each one of them, and compute their search costs. The algorithms are developed based on solving Chinese Postman problems. Finally, we present extensive experimental results to examine the accuracy of the mathematical models and compare the performances of different methods. A heuristic method, divide-merge, is shown to outperform all others and finds near-optimal solutions. Chinese postman problem, graph partitioning, mobile agent, search and rescue, wireless sensor network",wireless sensor network aided search and rescue in trails in recent years wireless sensor networks have been used in applications of data gathering and target localization across large geographical areas in this paper we study the issues involved in applying wireless sensor networks to search and rescue of lost hikers in trails and focus on the optimal placement of sensors and access points such that the cost of search and rescue is minimized particularly we address two problems a how to identify the lost hiker position as accurately as possible i e obtain a small trail segments containing the lost hiker and b how to search efficiently in trail segments for different trail topologies and search agent capabilities for the optimal access point deployment problem we propose theoretical models that consider both efficiency and accuracy criteria and present analytical results for simpler trail topologies for complicated graph topologies we develop efficient heuristic algorithms with various heuristics after access point deployment is decided the actual cost of search in individual trail segment can be computed we analyze four different types of search and rescue agents present algorithms to find the optimal search paths for each one of them and compute their search costs the algorithms are developed based on solving chinese postman problems finally we present extensive experimental results to examine the accuracy of the mathematical models and compare the performances of different methods a heuristic method divide merge is shown to outperform all others and finds near optimal solutions chinese postman problem graph partitioning mobile agent search and rescue wireless sensor network,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
27,0,195,"Wireless Sensor Networks in Emergency Scenarios: The FeuerWhere Deployment In the project FeuerWhere we researched the use of Wireless Sensor Networks (WSNs) in rescue scenarios by combining the monitoring of the environment and vital signs, as well as estimating the location of the nodes in a WSN. The goal of this project was to monitor vital signs, envi- ronmental conditions and positions of firefighters in a large indoor emergency scenario using a meshed WSN. The WSN consists of one node for each firefighter which transports the data to the mesh network and which is also connected to a Body Area Network (BAN) [8]. The BAN itself consists of several nodes placed into protective suits. The main requirement to all parts of the system is robustness against all kinds of extreme environmental conditions, like extreme air temperatures up to 800$ ^\circ$C, extreme humidity up to 100% including condensation and wet walls in unknown buildings. We report on an experimental evaluation of the deployment of a prototype that addresses the concern of monitoring firefighters in extreme environmental conditions. This will establish the general feasibility of WSN in extreme conditions and show that precise indoor localization using radio runtime measurements is not affected by these conditions. extreme environmental conditions, real world deployment, wireless sensor networks",wireless sensor networks in emergency scenarios the feuerwhere deployment in the project feuerwhere we researched the use of wireless sensor networks wsns in rescue scenarios by combining the monitoring of the environment and vital signs as well as estimating the location of the nodes in a wsn the goal of this project was to monitor vital signs envi ronmental conditions and positions of firefighters in a large indoor emergency scenario using a meshed wsn the wsn consists of one node for each firefighter which transports the data to the mesh network and which is also connected to a body area network ban 8 the ban itself consists of several nodes placed into protective suits the main requirement to all parts of the system is robustness against all kinds of extreme environmental conditions like extreme air temperatures up to 800 circ c extreme humidity up to 100 including condensation and wet walls in unknown buildings we report on an experimental evaluation of the deployment of a prototype that addresses the concern of monitoring firefighters in extreme environmental conditions this will establish the general feasibility of wsn in extreme conditions and show that precise indoor localization using radio runtime measurements is not affected by these conditions extreme environmental conditions real world deployment wireless sensor networks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
28,0,226,"Wireless Sensor Networks: Learning and Teaching There have been many response deficiencies cited regarding a fire department's ability to fight a structure fire. The attacks on the World Trade Center on 9/11 highlighted many of these problems and demanded the attention of the world. Wireless Sensor Networks (WSN) can benefit fire fighters as they bravely enter a structure fire. WSNs are networks consisting of many small sensors or nodes. The sensors can monitor a variety of data, such as the environment, movement and patient health readings. There has been much research completed in the area of WSNs but most of this research is proven via simulations with little actual experimentation or deployment of devices. Wireless sensor networks is an exciting and a new area of research; it has captured the interest of many researchers. The intrigue easily attracts the attention of students as well. With the help of various students, a wireless sensor network will be deployed in a fire training center to test a deployment in a structure. The primary measurement will be the performance of the sensors and the sensor network. Several students have been utilized to assist in writing the necessary programs, and more will continue to contribute to the project. This project will maintain research in utilizing WSNs in a fire fighting scenario and will continue to employ students to get them excited about learning and research. implementation, nesC, network course, performance, teaching, wireless sensor networks",wireless sensor networks learning and teaching there have been many response deficiencies cited regarding a fire department s ability to fight a structure fire the attacks on the world trade center on 9 11 highlighted many of these problems and demanded the attention of the world wireless sensor networks wsn can benefit fire fighters as they bravely enter a structure fire wsns are networks consisting of many small sensors or nodes the sensors can monitor a variety of data such as the environment movement and patient health readings there has been much research completed in the area of wsns but most of this research is proven via simulations with little actual experimentation or deployment of devices wireless sensor networks is an exciting and a new area of research it has captured the interest of many researchers the intrigue easily attracts the attention of students as well with the help of various students a wireless sensor network will be deployed in a fire training center to test a deployment in a structure the primary measurement will be the performance of the sensors and the sensor network several students have been utilized to assist in writing the necessary programs and more will continue to contribute to the project this project will maintain research in utilizing wsns in a fire fighting scenario and will continue to employ students to get them excited about learning and research implementation nesc network course performance teaching wireless sensor networks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
29,0,118,"WISE-SPL: Bringing Multi-tenancy to the Weather InSights Environment System Weather conditions affect many cities and companies. The WISE (Weather InSights Environment) system serves as a central place to gather and present weather related information for decision makers. It was initially developed to fit a single tenant. Due to a multi-tenant opportunity, WISE is evolving to be deployed on a Cloud environment to support on-demand computing resources and multiple clients. Software product line techniques were applied to model common and variable features of tenants. WISE-SPL enables the derivation of products for each client and also the deployment on Cloud infrastructure. The contribution of this work is a demonstration and discussion of benefits and limitations in applying SPL techniques, following a extractive approach, to build a multi-tenant Cloud application. []",wise spl bringing multi tenancy to the weather insights environment system weather conditions affect many cities and companies the wise weather insights environment system serves as a central place to gather and present weather related information for decision makers it was initially developed to fit a single tenant due to a multi tenant opportunity wise is evolving to be deployed on a cloud environment to support on demand computing resources and multiple clients software product line techniques were applied to model common and variable features of tenants wise spl enables the derivation of products for each client and also the deployment on cloud infrastructure the contribution of this work is a demonstration and discussion of benefits and limitations in applying spl techniques following a extractive approach to build a multi tenant cloud application,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
30,0,162,"Workflow Partitioning and Deployment on the Cloud Using Orchestra Orchestrating service-oriented workflows is typically based on a design model that routes both data and control through a single point -- the centralised workflow engine. This causes scalability problems that include the unnecessary consumption of the network bandwidth, high latency in transmitting data between the services, and performance bottlenecks. These problems are highly prominent when orchestrating workflows that are composed from services dispersed across distant geographical locations. This paper presents a novel workflow partitioning approach, which attempts to improve the scalability of orchestrating large-scale workflows. It permits the workflow computation to be moved towards the services providing the data in order to garner optimal performance results. This is achieved by decomposing the workflow into smaller sub workflows for parallel execution, and determining the most appropriate network locations to which these sub workflows are transmitted and subsequently executed. This paper demonstrates the efficiency of our approach using a set of experimental workflows that are orchestrated over Amazon EC2 and across several geographic network regions. Service-oriented workflows, computation placement analysis, deployment, orchestration, partitioning",workflow partitioning and deployment on the cloud using orchestra orchestrating service oriented workflows is typically based on a design model that routes both data and control through a single point the centralised workflow engine this causes scalability problems that include the unnecessary consumption of the network bandwidth high latency in transmitting data between the services and performance bottlenecks these problems are highly prominent when orchestrating workflows that are composed from services dispersed across distant geographical locations this paper presents a novel workflow partitioning approach which attempts to improve the scalability of orchestrating large scale workflows it permits the workflow computation to be moved towards the services providing the data in order to garner optimal performance results this is achieved by decomposing the workflow into smaller sub workflows for parallel execution and determining the most appropriate network locations to which these sub workflows are transmitted and subsequently executed this paper demonstrates the efficiency of our approach using a set of experimental workflows that are orchestrated over amazon ec2 and across several geographic network regions service oriented workflows computation placement analysis deployment orchestration partitioning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
31,0,113,"Working the Contract This paper presents data and analysis from a long term ethnographic study of the design and development of an electronic patient records system in a UK hospital Trust. The project is a public private partnership (PPP) between the Trust and a US based software house (OurComp) contracted to supply, configure and support their customizable-off-the-shelf (COTS) healthcare information system in cooperation with an in-hospital project team. Given this contractual relationship for system delivery and support (increasingly common, and 'standard' in UK healthcare) we focus on the ways in which issues to do with the 'contract' enter into and impinge on everyday design and deployment work as part of the process of delivering dependable systems. contracts, customizable-off-the-shelf (COTS) healthcare information systems, ethnography, project management",working the contract this paper presents data and analysis from a long term ethnographic study of the design and development of an electronic patient records system in a uk hospital trust the project is a public private partnership ppp between the trust and a us based software house ourcomp contracted to supply configure and support their customizable off the shelf cots healthcare information system in cooperation with an in hospital project team given this contractual relationship for system delivery and support increasingly common and standard in uk healthcare we focus on the ways in which issues to do with the contract enter into and impinge on everyday design and deployment work as part of the process of delivering dependable systems contracts customizable off the shelf cots healthcare information systems ethnography project management,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
32,0,169,"Workload Analysis for the Scope of User Demand Prediction Model Evaluations in Cloud Environments Alongside the healthy development of the Cloud-based technologies across various application deployments, their associated energy consumptions incurred by the excess usage of Information and Communication Technology (ICT) resources, is one of the serious concerns demanding effective solutions with immediate effect. Effective auto scaling of the Cloud resources in accordance to the incoming user demand and thereby reducing the idle resources is one optimum solution which not only reduces the excess energy consumptions but also helps maintaining the Quality of Service (QoS). Whilst achieving such tasks, estimating the user demand in advance with reliable level of accuracy has become an integral and vital component. With this in mind, this research work is aimed at analyzing the Cloud workloads and further evaluating the performances of two widely used prediction techniques such as Markov modelling and Bayesian modelling with 7 hours of Google cluster data. An important outcome of this research work is the categorization and characterization of the Cloud workloads which will assist leading into the user demand prediction parameter modelling. modelling, pattern, prediction, workloads",workload analysis for the scope of user demand prediction model evaluations in cloud environments alongside the healthy development of the cloud based technologies across various application deployments their associated energy consumptions incurred by the excess usage of information and communication technology ict resources is one of the serious concerns demanding effective solutions with immediate effect effective auto scaling of the cloud resources in accordance to the incoming user demand and thereby reducing the idle resources is one optimum solution which not only reduces the excess energy consumptions but also helps maintaining the quality of service qos whilst achieving such tasks estimating the user demand in advance with reliable level of accuracy has become an integral and vital component with this in mind this research work is aimed at analyzing the cloud workloads and further evaluating the performances of two widely used prediction techniques such as markov modelling and bayesian modelling with 7 hours of google cluster data an important outcome of this research work is the categorization and characterization of the cloud workloads which will assist leading into the user demand prediction parameter modelling modelling pattern prediction workloads,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
33,0,230,"Workload Characterization for Operator-based Distributed Stream Processing Applications Operator-based programming languages provide an effective development model for large scale stream processing applications. A stream processing application consists of many runtime deployable software processing elements (PE) that work in flows to process incoming messages. Operators (OP) are logical building blocks hosted by PEs. One or more OPs can be fused into a PE at compile-time. Performance optimization for our streaming system includes compile-time fusion optimization and runtime PE-to-host deployment. One of the goals of an optimized stream application is to use minimal computing resource to sustain maximal message throughput. Characterizing the resource usage of PEs is critical for performance optimization. During compile-time optimization, OP-level resource usage is used to predict the resource usage of fused PEs. When starting an application, PE-level resource usage is used as an initial estimation by the scheduler. In this paper, we propose an efficient workload characterization approach for data stream processing systems. Our method includes the procedures for obtaining reusable OP-level resource usage information from profiling data and recomposing OP-level profiles to predict PE-level resource usage. We present several techniques to overcome measurement errors from the OP data collection. The impact of hardware speed and multi-threading contention on hyper-threading and multi-core machines are also studied. We show that our method can be applied to several streaming applications and the prediction of the PE CPU resource usage is within 15% of the actual CPU usage. profiling, stream processing system, workload characterization",workload characterization for operator based distributed stream processing applications operator based programming languages provide an effective development model for large scale stream processing applications a stream processing application consists of many runtime deployable software processing elements pe that work in flows to process incoming messages operators op are logical building blocks hosted by pes one or more ops can be fused into a pe at compile time performance optimization for our streaming system includes compile time fusion optimization and runtime pe to host deployment one of the goals of an optimized stream application is to use minimal computing resource to sustain maximal message throughput characterizing the resource usage of pes is critical for performance optimization during compile time optimization op level resource usage is used to predict the resource usage of fused pes when starting an application pe level resource usage is used as an initial estimation by the scheduler in this paper we propose an efficient workload characterization approach for data stream processing systems our method includes the procedures for obtaining reusable op level resource usage information from profiling data and recomposing op level profiles to predict pe level resource usage we present several techniques to overcome measurement errors from the op data collection the impact of hardware speed and multi threading contention on hyper threading and multi core machines are also studied we show that our method can be applied to several streaming applications and the prediction of the pe cpu resource usage is within 15 of the actual cpu usage profiling stream processing system workload characterization,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
34,0,57,"Worldsens: From Lab to Sensor Network Application Development and Deployment We present Worldsens,a prototyping and development work for wireless sensor protocols and applications.Our relies on two simulators,WSim and WSNet, and proposes a full simulation and performance estimation embedded platforms with instruction and radio byte curacy.During this demo,we propose to demonstrate the interest of the two simulators in stand-alone and in cooperative use. design, sensor networks, simulation",worldsens from lab to sensor network application development and deployment we present worldsens a prototyping and development work for wireless sensor protocols and applications our relies on two simulators wsim and wsnet and proposes a full simulation and performance estimation embedded platforms with instruction and radio byte curacy during this demo we propose to demonstrate the interest of the two simulators in stand alone and in cooperative use design sensor networks simulation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
35,0,217,"Wq: A Modular Framework for Collecting, Storing, and Utilizing Experiential VGI We present ""wq"", an open-source framework for developing robust applications allowing volunteers to collect geographic information (VGI) in the field. Successful VGI applications been deployed in various contexts, but much of the effort that is put into common programming tasks cannot be re-used, often because the application code is too closely tied to the problem domain. User-friendly campaign authoring tools are being explored as ways to facilitate the rapid deployment of VGI applications, but many of these tools necessarily enforce a restricted vocabulary of interface elements and database models, limiting their usefulness for more complex VGI project workflows. In contrast, we propose a highly modular, open-source approach that enables reuse of general-purpose components created to facilitate common design patterns -- without enforcing any hard limitations on the database model or interface. The framework builds off of and extends numerous existing open-source projects and leverages open standards (e.g. HTML5), which means it will work across all popular mobile devices as well as desktop browsers. The ideas behind wq arose from our ongoing efforts to generalize an existing data collection application initially created for community-based stream quality monitoring. In this paper we justify the design decisions made in creating wq and suggest general principles that should be taken into consideration when designing systems for collecting, storing, and utilizing VGI. HTML5, VGI, citizen science, crowdsourcing, open source",wq a modular framework for collecting storing and utilizing experiential vgi we present wq an open source framework for developing robust applications allowing volunteers to collect geographic information vgi in the field successful vgi applications been deployed in various contexts but much of the effort that is put into common programming tasks cannot be re used often because the application code is too closely tied to the problem domain user friendly campaign authoring tools are being explored as ways to facilitate the rapid deployment of vgi applications but many of these tools necessarily enforce a restricted vocabulary of interface elements and database models limiting their usefulness for more complex vgi project workflows in contrast we propose a highly modular open source approach that enables reuse of general purpose components created to facilitate common design patterns without enforcing any hard limitations on the database model or interface the framework builds off of and extends numerous existing open source projects and leverages open standards e g html5 which means it will work across all popular mobile devices as well as desktop browsers the ideas behind wq arose from our ongoing efforts to generalize an existing data collection application initially created for community based stream quality monitoring in this paper we justify the design decisions made in creating wq and suggest general principles that should be taken into consideration when designing systems for collecting storing and utilizing vgi html5 vgi citizen science crowdsourcing open source,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
36,0,202,"XCo: Explicit Coordination to Prevent Network Fabric Congestion in Cloud Computing Cluster Platforms Large cluster-based cloud computing platforms increasingly use commodity Ethernet technologies, such as Gigabit Ethernet, 10GigE, and Fibre Channel over Ethernet (FCoE), for intra-cluster communication. Traffic congestion can become a performance concern in the Ethernet due to consolidation of data, storage, and control traffic over a common layer-2 fabric, as well as consolidation of multiple virtual machines (VMs) over less physical hardware. Even as networking vendors race to develop switch-level hardware support for congestion management, we make the case that virtualization has opened up a complementary set of opportunities to reduce or even eliminate network congestion in cloud computing clusters. We present the design, implementation, and evaluation of a system called XCo, that performs explicit coordination of network transmissions over a shared Ethernet fabric to proactively prevent network congestion. XCo is a software-only distributed solution executing only in the end-nodes. A central controller uses explicit permissions to temporally separate (at millisecond granularity) the transmissions from competing senders through congested links. XCo is fully transparent to applications, presently deployable, and independent of any switch-level hardware support. We present a detailed evaluation of our XCo prototype across a number of network congestion scenarios, and demonstrate that XCo significantly improves network performance during periods of congestion. congestion, ethernet, virtualization",xco explicit coordination to prevent network fabric congestion in cloud computing cluster platforms large cluster based cloud computing platforms increasingly use commodity ethernet technologies such as gigabit ethernet 10gige and fibre channel over ethernet fcoe for intra cluster communication traffic congestion can become a performance concern in the ethernet due to consolidation of data storage and control traffic over a common layer 2 fabric as well as consolidation of multiple virtual machines vms over less physical hardware even as networking vendors race to develop switch level hardware support for congestion management we make the case that virtualization has opened up a complementary set of opportunities to reduce or even eliminate network congestion in cloud computing clusters we present the design implementation and evaluation of a system called xco that performs explicit coordination of network transmissions over a shared ethernet fabric to proactively prevent network congestion xco is a software only distributed solution executing only in the end nodes a central controller uses explicit permissions to temporally separate at millisecond granularity the transmissions from competing senders through congested links xco is fully transparent to applications presently deployable and independent of any switch level hardware support we present a detailed evaluation of our xco prototype across a number of network congestion scenarios and demonstrate that xco significantly improves network performance during periods of congestion congestion ethernet virtualization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
37,0,180,"XMLText: From XML Schema to Xtext A multitude of Domain-Specific Languages (DSLs) have been implemented with XML Schemas. While such DSLs are well adopted and flexible, they miss modern DSL editor functionality. Moreover, since XML is primarily designed as a machine-processible format, artifacts defined with XML-based DSLs lack comprehensibility and, therefore, maintainability. In order to tackle these shortcomings, we propose a bridge between the XML Schema Definition (XSD) language and text-based metamodeling languages. This bridge exploits existing seams between the technical spaces XMLware, modelware, and grammarware as well as closes identified gaps. The resulting approach is able to generate Xtext-based editors from XSDs providing powerful editor functionality, customization options for the textual concrete syntax style, and round-trip transformations enabling the exchange of data between the involved technical spaces. We evaluate our approach by a case study on TOSCA, which is an XML-based standard for defining Cloud deployments. The results show that our approach enables bridging XMLware with modelware and grammarware in several ways going beyond existing approaches and allows the automated generation of editors that are at least equivalent to editors manually built for XML-based languages. DSL, Language Engineering, Language Modernization, Markup Language, XSD, Xtext",xmltext from xml schema to xtext a multitude of domain specific languages dsls have been implemented with xml schemas while such dsls are well adopted and flexible they miss modern dsl editor functionality moreover since xml is primarily designed as a machine processible format artifacts defined with xml based dsls lack comprehensibility and therefore maintainability in order to tackle these shortcomings we propose a bridge between the xml schema definition xsd language and text based metamodeling languages this bridge exploits existing seams between the technical spaces xmlware modelware and grammarware as well as closes identified gaps the resulting approach is able to generate xtext based editors from xsds providing powerful editor functionality customization options for the textual concrete syntax style and round trip transformations enabling the exchange of data between the involved technical spaces we evaluate our approach by a case study on tosca which is an xml based standard for defining cloud deployments the results show that our approach enables bridging xmlware with modelware and grammarware in several ways going beyond existing approaches and allows the automated generation of editors that are at least equivalent to editors manually built for xml based languages dsl language engineering language modernization markup language xsd xtext,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
38,0,42,"YADDA2: Assemble Your Own Digital Library Application from LEGO Bricks YADDA2 is an open software platform which facilitates creation of digital library applications. It consists of versatile building blocks providing, among others: storage, relational and full-text indexing, process management, and asynchronous communication. Its loosely-coupled service-oriented architecture enables deployment of highly-scalable, distributed systems. scalability, service-oriented architecture, software platform",yadda2 assemble your own digital library application from lego bricks yadda2 is an open software platform which facilitates creation of digital library applications it consists of versatile building blocks providing among others storage relational and full text indexing process management and asynchronous communication its loosely coupled service oriented architecture enables deployment of highly scalable distributed systems scalability service oriented architecture software platform,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
39,0,149,"You Want Us to Support WHAT?!? Negotiation, Delivery and Cultivation: The Gateway to Excellent Service Deployment In 2009, the Campus Information Technologies and Educational Services (CITES) Help Desk at the University of Illinois at Urbana-Champaign began taking a more active role in supporting many of the services provided by CITES. The introduction of new services is inevitable. Providing excellent support requires collaboration from the early planning stages through implementation and beyond. The Help Desk experienced varied results during the integration of new services. Join us as we explore the importance of communication and collaboration between service managers and the Help Desk. We will discuss our successes and failures, past and present. Our approach contains requirements that must be satisfied before the Help Desk takes on support responsibilities. We are the face of the organization. We must be an advocate for our customers. We have a vision of how this organic process should work. Gaining trust and cooperation will be the foundation of a world-class institution. SLA, applications, customer service, help desk, knowledgebase, lifecycle, management, project management, services",you want us to support what negotiation delivery and cultivation the gateway to excellent service deployment in 2009 the campus information technologies and educational services cites help desk at the university of illinois at urbana champaign began taking a more active role in supporting many of the services provided by cites the introduction of new services is inevitable providing excellent support requires collaboration from the early planning stages through implementation and beyond the help desk experienced varied results during the integration of new services join us as we explore the importance of communication and collaboration between service managers and the help desk we will discuss our successes and failures past and present our approach contains requirements that must be satisfied before the help desk takes on support responsibilities we are the face of the organization we must be an advocate for our customers we have a vision of how this organic process should work gaining trust and cooperation will be the foundation of a world class institution sla applications customer service help desk knowledgebase lifecycle management project management services,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
40,0,119,"ZenCrowd: Leveraging Probabilistic Reasoning and Crowdsourcing Techniques for Large-scale Entity Linking We tackle the problem of entity linking for large collections of online pages; Our system, ZenCrowd, identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud. We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform. We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers. We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links, while limiting the amount of work performed by the crowd. crowdsourcing, entity linking, linked data, probabilistic reasoning",zencrowd leveraging probabilistic reasoning and crowdsourcing techniques for large scale entity linking we tackle the problem of entity linking for large collections of online pages our system zencrowd identifies entities from natural language text using state of the art techniques and automatically connects them to the linked open data cloud we show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro tasks on an online crowdsourcing platform we develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers we evaluate zencrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links while limiting the amount of work performed by the crowd crowdsourcing entity linking linked data probabilistic reasoning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
41,0,141,"Ziria: Wireless Programming for Hardware Dummies Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE. data and control paths, high performance, programming model, software-defined radio",ziria wireless programming for hardware dummies software defined radio sdr brings the flexibility of software to the domain of wireless protocol design promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware most existing sdr platforms require careful hand tuning of low level code to be useful in the real world in this talk i will describe ziria an sdr platform that is both easily programmable and performant ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks the model captures the inherent and important distinction between data and control paths in this domain i will describe the programming model give an overview of the execution model compiler optimizations and current work we have used ziria to produce an implementation of 802 11a g and a partial implementation of lte data and control paths high performance programming model software defined radio,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
42,0,191,"Zodiac: Organizing Large Deployment of Sensors to Create Reusable Applications for Buildings Large scale deployment of sensors is essential to practical applications in cyber physical systems. For instance, instrumenting a commercial building for 'smart energy' management requires deployment and operation of thousands of measurement and metering sensors and actuators that direct operation of the HVAC system. Each of these sensors need to be named consistently and constantly calibrated. Doing this process manually is not only time consuming but also error prone given the scale, heterogeneity and complexity of buildings as well as lack of uniform naming schemas. To address this challenge, we propose Zodiac - a framework for automatically classifying, naming and managing sensors based on active learning from sensor metadata. In contrast to prior work, Zodiac requires minimal user input in terms of labelling examples while being more accurate. To evaluate Zodiac, we deploy it across four real buildings on our campus and label the ground truth metadata for all the sensors in these buildings manually. Using a combination of hierarchical clustering and random forest classifiers we show that Zodiac can successfully classify sensors with an average accuracy of 98% with 28% fewer training examples when compared to a regular expression based method. active learning, automated naming, bacnet, building management systems, hvac, ontology, sensor metadata, smart buildings",zodiac organizing large deployment of sensors to create reusable applications for buildings large scale deployment of sensors is essential to practical applications in cyber physical systems for instance instrumenting a commercial building for smart energy management requires deployment and operation of thousands of measurement and metering sensors and actuators that direct operation of the hvac system each of these sensors need to be named consistently and constantly calibrated doing this process manually is not only time consuming but also error prone given the scale heterogeneity and complexity of buildings as well as lack of uniform naming schemas to address this challenge we propose zodiac a framework for automatically classifying naming and managing sensors based on active learning from sensor metadata in contrast to prior work zodiac requires minimal user input in terms of labelling examples while being more accurate to evaluate zodiac we deploy it across four real buildings on our campus and label the ground truth metadata for all the sensors in these buildings manually using a combination of hierarchical clustering and random forest classifiers we show that zodiac can successfully classify sensors with an average accuracy of 98 with 28 fewer training examples when compared to a regular expression based method active learning automated naming bacnet building management systems hvac ontology sensor metadata smart buildings,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
43,0,91,"Zooniverse: Observing the World's Largest Citizen Science Platform This paper introduces the Zooniverse citizen science project and software framework, outlining its structure from an observatory perspective: both as an observable web-based system in itself, and as an example of a platform iteratively developed according to real-world deployment and used at scale. We include details of the technical architecture of Zooniverse, including the mechanisms for data gathering across the Zooniverse operation, access, and analysis. We consider the lessons that can be drawn from the experience of designing and running Zooniverse, and how this might inform development of other web observatories. citizen science, crowdsourcing, web observatories",zooniverse observing the world s largest citizen science platform this paper introduces the zooniverse citizen science project and software framework outlining its structure from an observatory perspective both as an observable web based system in itself and as an example of a platform iteratively developed according to real world deployment and used at scale we include details of the technical architecture of zooniverse including the mechanisms for data gathering across the zooniverse operation access and analysis we consider the lessons that can be drawn from the experience of designing and running zooniverse and how this might inform development of other web observatories citizen science crowdsourcing web observatories,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
44,0,82,"A Biologically-inspired Intrabody Nanonetwork: Design Considerations n this position paper, we describe an architecture for intrabody nanonetworks, a new type of body area networks targeted to the molecular environment deep inside the human body. Our approach is to learn from biological systems (e.g., microbial organisms that establish a complex adaptive system in the body) to design an architecture for intrabody nanonetworks. Our initial thoughts on the architectural design as well as testbed are first described. Future challenges are then discussed toward the practical deployment of intrabody nanonetworks. bio-inspired approach, intrabody nanonetwork, microbial ecosystem, system architecture",a biologically inspired intrabody nanonetwork design considerations n this position paper we describe an architecture for intrabody nanonetworks a new type of body area networks targeted to the molecular environment deep inside the human body our approach is to learn from biological systems e g microbial organisms that establish a complex adaptive system in the body to design an architecture for intrabody nanonetworks our initial thoughts on the architectural design as well as testbed are first described future challenges are then discussed toward the practical deployment of intrabody nanonetworks bio inspired approach intrabody nanonetwork microbial ecosystem system architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
45,0,200,"A Biomorphic Model for Automated Cloud Adaptation Although there is an extensive amount of research covering in the area of Cloud computing, the field of bio-inspired cloud computing is underinvestigated when compared to the general research area. This study tries to find answers on how a biomorphic model can be implemented in the cloud in order to achieve adaptive cloud behaviour. The process of cellular differentiation where cells transform from one type to another, is chosen to be the foundation model for a developed technical model. We define analogies to the cloud where stem cells are blank servers and web servers are cells with a specific function. With a combination of configuration management, version control and cloud deployment systems, an imitation of this biological process is applied in the cloud. The use of automated cloud scaling as a case of adaptive behaviour is the main goal of the research. One approach has been developed for mapping the biological model to the cloud which consists of a prototype where the signal detection and node activation is being triggered by using the concept of random generated timers. The obtained performance results were varying, depending on the general timer distribution, providing new ideas for future improvements and different algorithm proposals. biomorphic model, cloud comptuing, nature inspired computing, web scaling",a biomorphic model for automated cloud adaptation although there is an extensive amount of research covering in the area of cloud computing the field of bio inspired cloud computing is underinvestigated when compared to the general research area this study tries to find answers on how a biomorphic model can be implemented in the cloud in order to achieve adaptive cloud behaviour the process of cellular differentiation where cells transform from one type to another is chosen to be the foundation model for a developed technical model we define analogies to the cloud where stem cells are blank servers and web servers are cells with a specific function with a combination of configuration management version control and cloud deployment systems an imitation of this biological process is applied in the cloud the use of automated cloud scaling as a case of adaptive behaviour is the main goal of the research one approach has been developed for mapping the biological model to the cloud which consists of a prototype where the signal detection and node activation is being triggered by using the concept of random generated timers the obtained performance results were varying depending on the general timer distribution providing new ideas for future improvements and different algorithm proposals biomorphic model cloud comptuing nature inspired computing web scaling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
46,0,154,"A Broker-based Framework for Multi-cloud Workflows Computational science workflows have been successfully run on traditional HPC systems like clusters and Grids for many years. Today, users are interested to execute their workflow applications in the Cloud to exploit the economic and technical benefits of this new emerging technology. The deployment and management of workflows over the current existing heterogeneous and not yet interoperable Cloud providers, however, is still a challenging task for the workflow developers. In this paper, we present a broker-based framework for running workflows in a multi-Cloud environment. The framework allows an automatic selection of the target Clouds, a uniform access to the Clouds, and workflow data management with respect to user Service Level Agreement (SLA) requirements. Following a simulation approach, we evaluated the framework with a real scientific workflow application in different deployment scenarios. The results show that our framework offers benefits to users by executing workflows with the expected performance and service quality at lowest cost. cloud broker, cloud computing, cloud workflow, intercloud computing, multi-cloud",a broker based framework for multi cloud workflows computational science workflows have been successfully run on traditional hpc systems like clusters and grids for many years today users are interested to execute their workflow applications in the cloud to exploit the economic and technical benefits of this new emerging technology the deployment and management of workflows over the current existing heterogeneous and not yet interoperable cloud providers however is still a challenging task for the workflow developers in this paper we present a broker based framework for running workflows in a multi cloud environment the framework allows an automatic selection of the target clouds a uniform access to the clouds and workflow data management with respect to user service level agreement sla requirements following a simulation approach we evaluated the framework with a real scientific workflow application in different deployment scenarios the results show that our framework offers benefits to users by executing workflows with the expected performance and service quality at lowest cost cloud broker cloud computing cloud workflow intercloud computing multi cloud,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
47,0,242,"A Call Control Driven MVC Programming Model for Mixing Web and Call or Multimedia Applications With the evolution of internet towards web 2.0 and real time communications and the adoption of IMS by telecommunications service providers, SIP becomes the next major protocol that application developer may want to rely on to develop their latest applications. As SIP specifications stabilize, SIP application server become well understood entities. In fact most application servers, like J2EE, now support SIP servlet containers. As a result it become essential to understand how such SIP applications should interact with other web or IT applications. In this paper, we describe a call control driven MVC programming model for converged applications, defined as application that mix Web and SIP. The programming model is then generalized to any call or media control situation. As most telecommunications networks, in particular mobile network evolve towards the IMS, such programming model is expected to become essential to the development of future IMS mobile applications. It is in fact not limited to mobile, but extends to any application on network deploying IP (internet, broadband, wired and wireless with or without IMS as well as converged applications that provides a same experience across multiple such access networks). The programming model presented in this paper can be extended to rely on the notion of enabler, thereby supporting the combination of call control with other mobile, voice and communication features and the deployment over any network (IMS, SIP and other SIP deployments as well as legacy networks (e.g. PSTN and IN)). IMS (IP multimedia subsystems), Parlay, SIP, call control, converged applications, media server control, multimedia applications, multimodal applications, real time communications (RTC), voice application, web 2.0",a call control driven mvc programming model for mixing web and call or multimedia applications with the evolution of internet towards web 2 0 and real time communications and the adoption of ims by telecommunications service providers sip becomes the next major protocol that application developer may want to rely on to develop their latest applications as sip specifications stabilize sip application server become well understood entities in fact most application servers like j2ee now support sip servlet containers as a result it become essential to understand how such sip applications should interact with other web or it applications in this paper we describe a call control driven mvc programming model for converged applications defined as application that mix web and sip the programming model is then generalized to any call or media control situation as most telecommunications networks in particular mobile network evolve towards the ims such programming model is expected to become essential to the development of future ims mobile applications it is in fact not limited to mobile but extends to any application on network deploying ip internet broadband wired and wireless with or without ims as well as converged applications that provides a same experience across multiple such access networks the programming model presented in this paper can be extended to rely on the notion of enabler thereby supporting the combination of call control with other mobile voice and communication features and the deployment over any network ims sip and other sip deployments as well as legacy networks e g pstn and in ims ip multimedia subsystems parlay sip call control converged applications media server control multimedia applications multimodal applications real time communications rtc voice application web 2 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
48,0,107,"A Case Study in Database Reliability: Component Types, Usage Profiles, and Testing Data management lies at the core of most modern information technology deployments. Accordingly, the reliability of the database management system (DBMS) is critical to the reputation and success of both its vendors and their clients. However, there is a dearth of work in the literature focused on the reliability of the DBMS. More specifically, research is yet to be focused on the variables that influence DBMS reliability and the relationships between these variables. We present an initial case study focused on the relationships between component type, usage profiles, component size, component changes, component usage, and defect yield. The system under study is a distributed enterprise relational DBMS. case study, database management system, testing, usage profiles",a case study in database reliability component types usage profiles and testing data management lies at the core of most modern information technology deployments accordingly the reliability of the database management system dbms is critical to the reputation and success of both its vendors and their clients however there is a dearth of work in the literature focused on the reliability of the dbms more specifically research is yet to be focused on the variables that influence dbms reliability and the relationships between these variables we present an initial case study focused on the relationships between component type usage profiles component size component changes component usage and defect yield the system under study is a distributed enterprise relational dbms case study database management system testing usage profiles,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
49,0,110,"A Case Study in Interoperable Support for Collaborative Community Healthcare This paper describes a two year case study in the engineering and deployment of a Clinical Information System (CIS) called Palliative Care Information System (PAL-IS) for managing and monitoring team-based community care of palliative patients. The case study followed the methodology, architecture and ontology proposed in previous work to address workflow, behavioral and technology issues for CIS that support collaborative, mobile, and accessible healthcare. Both PAL-IS and the methodology used in its development are evaluated. The results give fresh insight into interoperability issues which complicate CIS design. They also highlight the importance of reporting requirements as a major driver for investment in CIS and a critical factor in CIS design. clinical information system, collaboration, community care, interoperability, ontology, systems design",a case study in interoperable support for collaborative community healthcare this paper describes a two year case study in the engineering and deployment of a clinical information system cis called palliative care information system pal is for managing and monitoring team based community care of palliative patients the case study followed the methodology architecture and ontology proposed in previous work to address workflow behavioral and technology issues for cis that support collaborative mobile and accessible healthcare both pal is and the methodology used in its development are evaluated the results give fresh insight into interoperability issues which complicate cis design they also highlight the importance of reporting requirements as a major driver for investment in cis and a critical factor in cis design clinical information system collaboration community care interoperability ontology systems design,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
50,0,130,"A Case Study of Enterprise Identity Management System Adoption in an Insurance Organization This case study describes the adoption of an enterprise identity management(IdM) system in an insurance organization. We describe the state of the organization before deploying the IdM system, and point out the challenges in its IdM practices. We describe the organization's requirements for an IdM system, why a particular solution was chosen, issues in the deployment and configuration of the solution, the expected benefits, and the new challenges that arose from using the solution. Throughout, we identify practical problems that can be the focus of future research and development efforts. Our results confirm and elaborate upon the findings of previous research, contributing to an as-yet immature body of cases about IdM. Furthermore, our findings serve as a validation of our previously identified guidelines for IT security tools in general. case study, identity management, organizational factors, qualitative research, security tools",a case study of enterprise identity management system adoption in an insurance organization this case study describes the adoption of an enterprise identity management idm system in an insurance organization we describe the state of the organization before deploying the idm system and point out the challenges in its idm practices we describe the organization s requirements for an idm system why a particular solution was chosen issues in the deployment and configuration of the solution the expected benefits and the new challenges that arose from using the solution throughout we identify practical problems that can be the focus of future research and development efforts our results confirm and elaborate upon the findings of previous research contributing to an as yet immature body of cases about idm furthermore our findings serve as a validation of our previously identified guidelines for it security tools in general case study identity management organizational factors qualitative research security tools,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
51,0,150,"A Case Study of Post-deployment User Feedback Triage Many software requirements are identified only after a product is deployed, once users have had a chance to try the software and provide feedback. Unfortunately, addressing such feedback is not always straightforward, even when a team is fully invested in user-centered design. To investigate what constrains a teams evolution decisions, we performed a 6-month field study of a team employing iterative user-centered design methods to the design, deployment and evolution of a web application for a university community. Across interviews with the team, analyses of their bug reports, and further interviews with both users and non-adopters of the application, we found most of the constraints on addressing user feedback emerged from conflicts between users heterogeneous use of information and inflexible assumptions in the team's software architecture derived from earlier user research. These findings highlight the need for new approaches to expressing and validating assumptions from user research as software evolves. bug reports, bug triage, software evolution, user feedback",a case study of post deployment user feedback triage many software requirements are identified only after a product is deployed once users have had a chance to try the software and provide feedback unfortunately addressing such feedback is not always straightforward even when a team is fully invested in user centered design to investigate what constrains a teams evolution decisions we performed a 6 month field study of a team employing iterative user centered design methods to the design deployment and evolution of a web application for a university community across interviews with the team analyses of their bug reports and further interviews with both users and non adopters of the application we found most of the constraints on addressing user feedback emerged from conflicts between users heterogeneous use of information and inflexible assumptions in the team s software architecture derived from earlier user research these findings highlight the need for new approaches to expressing and validating assumptions from user research as software evolves bug reports bug triage software evolution user feedback,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
52,0,163,"A Cloud-based Framework for QoS-aware Service Selection Optimization In distributed, service-oriented systems, in which several concrete service instances need to be composed in order to respond to a request, it is important to select service deployments in an optimal and efficient way. Quality of Service attributes of deployments and network links are taken into account to decide between workflows that are identical in terms of their functionality. Several heuristic approaches have been proposed to solve the resulting QoS-aware service selection problem, known to be NP-hard. In our previous work, motivated by two concrete application scenarios, we proposed a blackboard and a genetic algorithm and compared them in terms of solution quality, performance and scalability. In order to seamlessly run and evaluate further approaches and parallel versions of the current algorithms in a distributed environment, a general framework for service selection optimization has been implemented using Cloud Computing resources. A performance study on sequential and parallel blackboard and genetic algorithms for solving service selection problems has been carried out in the Cloud. blackboard, genetic algorithm, optimization, service selection",a cloud based framework for qos aware service selection optimization in distributed service oriented systems in which several concrete service instances need to be composed in order to respond to a request it is important to select service deployments in an optimal and efficient way quality of service attributes of deployments and network links are taken into account to decide between workflows that are identical in terms of their functionality several heuristic approaches have been proposed to solve the resulting qos aware service selection problem known to be np hard in our previous work motivated by two concrete application scenarios we proposed a blackboard and a genetic algorithm and compared them in terms of solution quality performance and scalability in order to seamlessly run and evaluate further approaches and parallel versions of the current algorithms in a distributed environment a general framework for service selection optimization has been implemented using cloud computing resources a performance study on sequential and parallel blackboard and genetic algorithms for solving service selection problems has been carried out in the cloud blackboard genetic algorithm optimization service selection,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
53,0,162,"A Cloud-enabled Coordination Service for Internet-scale OMG DDS Applications The OMG Data Distribution Service (DDS), which is a standard specification for data-centric publish/subscribe communications, has shown promise for use in internet of things (IoT) applications because of its loosely coupled and scalable nature, and support for multiple QoS properties, such as reliable and real-time message delivery in dynamic environments. However, the current OMG DDS specification does not define coordination and discovery services for DDS message brokers, which are used in wide area network deployments of DDS. This paper describes preliminary research on a cloud-enabled coordination service for DDS message brokers, PubSubCoord, to overcome these limitations. Our approach provides a novel solution that brings together (a) ZooKeeper, which is used for the distributed coordination logic between message brokers, (b) DDS Routing Service, which is used to bridge DDS endpoints connected to different networks, and (c) BlueDove, which is used to provide a single-hop message delivery between brokers. Our design can support publishers and subscribers that dynamically join and leave their subnetworks. cloud computing, coordination, data distribution service, discovery, middleware, publish subscribe",a cloud enabled coordination service for internet scale omg dds applications the omg data distribution service dds which is a standard specification for data centric publish subscribe communications has shown promise for use in internet of things iot applications because of its loosely coupled and scalable nature and support for multiple qos properties such as reliable and real time message delivery in dynamic environments however the current omg dds specification does not define coordination and discovery services for dds message brokers which are used in wide area network deployments of dds this paper describes preliminary research on a cloud enabled coordination service for dds message brokers pubsubcoord to overcome these limitations our approach provides a novel solution that brings together a zookeeper which is used for the distributed coordination logic between message brokers b dds routing service which is used to bridge dds endpoints connected to different networks and c bluedove which is used to provide a single hop message delivery between brokers our design can support publishers and subscribers that dynamically join and leave their subnetworks cloud computing coordination data distribution service discovery middleware publish subscribe,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
54,0,71,"A Common Home for Features and Requirements: Retrofitting the House of Quality with Feature Models Quality function deployment (QFD) is a method for quality assurance developed for application in production processes. One prominent tool for implementing QFD is the House of Quality (HoQ), whose basic design principles have been left unchanged for the last decades. Modern concepts for handling product variability, most notably feature models, represent intuitive means for a refurbished roof construction of the HoQ, and thus more expressiveness in the definition of functional requirements. House of Quality,  Feature Models,  Requirements Engineering,  Variability Modeling,  Quality Function Deployment",a common home for features and requirements retrofitting the house of quality with feature models quality function deployment qfd is a method for quality assurance developed for application in production processes one prominent tool for implementing qfd is the house of quality hoq whose basic design principles have been left unchanged for the last decades modern concepts for handling product variability most notably feature models represent intuitive means for a refurbished roof construction of the hoq and thus more expressiveness in the definition of functional requirements house of quality feature models requirements engineering variability modeling quality function deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
55,0,197,"A Comparison of Mobile Rule Engines for Reasoning on Semantic Web Based Health Data Semantic Web technology is used extensively in the health domain, due to its ability to specify expressive, domain-specific data, as well as its capacity to facilitate data integration between heterogeneous, health-related sources. In the health domain, mobile devices are an essential part of patient self-management approaches, where local clinical decision support is applied to ensure that patients receive timely clinical findings. Currently, increases in mobile device capabilities have enabled the deployment of Semantic Web technologies on mobile platforms, enabling the consumption of rich, semantically described health data. To make this semantic health data available to local decision support as well, Semantic Web reasoning should be deployed on mobile platforms. However, there is currently a lack of software solutions and performance analysis of mobile, Semantic Web reasoning engines. This paper presents and compares the mobile benchmarks of 4 reasoning engines, applied on a dataset and rule set for patients with Atrial Fibrillation (AF). In particular, these benchmarks investigate the scalability of the mobile reasoning processes, and study reasoning performance for different process flows in decision support. For the purpose of these benchmarks, we extended a number of existing rule engines and RDF stores with Semantic Web reasoning capabilities. []",a comparison of mobile rule engines for reasoning on semantic web based health data semantic web technology is used extensively in the health domain due to its ability to specify expressive domain specific data as well as its capacity to facilitate data integration between heterogeneous health related sources in the health domain mobile devices are an essential part of patient self management approaches where local clinical decision support is applied to ensure that patients receive timely clinical findings currently increases in mobile device capabilities have enabled the deployment of semantic web technologies on mobile platforms enabling the consumption of rich semantically described health data to make this semantic health data available to local decision support as well semantic web reasoning should be deployed on mobile platforms however there is currently a lack of software solutions and performance analysis of mobile semantic web reasoning engines this paper presents and compares the mobile benchmarks of 4 reasoning engines applied on a dataset and rule set for patients with atrial fibrillation af in particular these benchmarks investigate the scalability of the mobile reasoning processes and study reasoning performance for different process flows in decision support for the purpose of these benchmarks we extended a number of existing rule engines and rdf stores with semantic web reasoning capabilities,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
56,0,165,"A Comparison of Two Iterations of a Software Studio Course Based on Continuous Integration In previous work we introduced a software studio course in which seventy students used continuous integration practices to collaborate on a common legacy code base. This enabled students to experience the issues of realistically sized software projects, and learn and apply appropriate techniques to overcome them, in a course without significant extra staffing. Although the course was broadly successful in its goals, it received a mixed response from students, and our paper noted several issues to overcome. This paper considers experimental changes to the course in light of our previous findings, and additional data from the official student surveys. Two iterations of the course and their respective results are compared. Whereas our previous paper addressed the feasibility of such a course, this paper considers how the student experience can be improved. The paper also considers how such a course can be adapted for more heterogeneous cohorts, such as the introduction of an unknown number of design and database students, or the introduction of online students. continuous integration, experience report, software engineering, studio course",a comparison of two iterations of a software studio course based on continuous integration in previous work we introduced a software studio course in which seventy students used continuous integration practices to collaborate on a common legacy code base this enabled students to experience the issues of realistically sized software projects and learn and apply appropriate techniques to overcome them in a course without significant extra staffing although the course was broadly successful in its goals it received a mixed response from students and our paper noted several issues to overcome this paper considers experimental changes to the course in light of our previous findings and additional data from the official student surveys two iterations of the course and their respective results are compared whereas our previous paper addressed the feasibility of such a course this paper considers how the student experience can be improved the paper also considers how such a course can be adapted for more heterogeneous cohorts such as the introduction of an unknown number of design and database students or the introduction of online students continuous integration experience report software engineering studio course,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
57,0,111,"A Component Based Approach for the Web of Things Model Driven Architectures are the holy grail of software engineering. Instead of writing code, developers draw models from the client's specification, which are then compiled into executable code (skeletons). We have taken this principle and applied it to the WoT. With the help of a meta-model tailored for the WoT we are able to build models to simultaneously take care of the physical and virtual aspects of smart devices. These models can then automatically be turned into code skeletons. The emphasis in the meta-model and its associated tools is reusability. Following the software engineering principle of independent reusable and deployable components, the outcome of the meta-model compiler are WoT compliant components. Web of Things, component based approach, meta-model, model driven architecture, software architecture, software development approach",a component based approach for the web of things model driven architectures are the holy grail of software engineering instead of writing code developers draw models from the client s specification which are then compiled into executable code skeletons we have taken this principle and applied it to the wot with the help of a meta model tailored for the wot we are able to build models to simultaneously take care of the physical and virtual aspects of smart devices these models can then automatically be turned into code skeletons the emphasis in the meta model and its associated tools is reusability following the software engineering principle of independent reusable and deployable components the outcome of the meta model compiler are wot compliant components web of things component based approach meta model model driven architecture software architecture software development approach,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
58,0,107,"A Comprehensive Trust Model for Component Software The growing importance of component software introduces special requirements on trust due to the nature of applications they provide, in particular when the system supports dynamic component deployment. This paper presents a comprehensive trust model in order to specify, evaluate and manage various trust relationships that exist among entities in a component software system. It contains a sub-model to present trust relationships among system entities, a sub-model to specify the information related to trust management for a software component, a sub-model for trust evaluation and a sub-model for trust management. This trust model supports trust management based on trust evaluation both at component download time and runtime. component software, security, trust, trust management, trust model",a comprehensive trust model for component software the growing importance of component software introduces special requirements on trust due to the nature of applications they provide in particular when the system supports dynamic component deployment this paper presents a comprehensive trust model in order to specify evaluate and manage various trust relationships that exist among entities in a component software system it contains a sub model to present trust relationships among system entities a sub model to specify the information related to trust management for a software component a sub model for trust evaluation and a sub model for trust management this trust model supports trust management based on trust evaluation both at component download time and runtime component software security trust trust management trust model,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
59,0,182,"A Computational Model for the Integration of Linked Data in Mobile Augmented Reality Applications Linked Data, Augmented Reality (AR), and technical advancements in mobile information technology lead to an increasing desire to exploit Linked Data for the integration and visualization in mobile AR applications. However, current approaches are either bound to existing client-server-based infrastructures or use closed data sources and proprietary data formats. Moreover, a number of related approaches are built upon content-based recognition algorithms that are both memory and processing-intensive, require a permanent connection to a host, and thus are inappropriate for a direct deployment onto mobile devices. In this work, we present a computational model that builds on a sensor-based tracking approach and maps proactively replicated Linked Data sets to a virtual representation of the user's vicinity computed by a mathematical model. We demonstrate the applicability of our approach through a proof-of-concept AR application that retrieves and aggregates mountain-specific data from a set of different sources and displays such data in a live-view interface. In consequence, our approach is resource-efficient, does not require a permanent network connection, is independent from existing server-based infrastructures, and allows to process Linked Data directly on a mobile device. augmented reality, linked data, mobile information systems, semantic web, visualization",a computational model for the integration of linked data in mobile augmented reality applications linked data augmented reality ar and technical advancements in mobile information technology lead to an increasing desire to exploit linked data for the integration and visualization in mobile ar applications however current approaches are either bound to existing client server based infrastructures or use closed data sources and proprietary data formats moreover a number of related approaches are built upon content based recognition algorithms that are both memory and processing intensive require a permanent connection to a host and thus are inappropriate for a direct deployment onto mobile devices in this work we present a computational model that builds on a sensor based tracking approach and maps proactively replicated linked data sets to a virtual representation of the user s vicinity computed by a mathematical model we demonstrate the applicability of our approach through a proof of concept ar application that retrieves and aggregates mountain specific data from a set of different sources and displays such data in a live view interface in consequence our approach is resource efficient does not require a permanent network connection is independent from existing server based infrastructures and allows to process linked data directly on a mobile device augmented reality linked data mobile information systems semantic web visualization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
60,0,240,"A Concept of Quality Assurance for Metrics in CASE-tools The quality of software has become more important to software companies in the past years. Software measurement is one of many approaches that is used to check the quality of software [18]. This may involve measuring some attributes of a software product or a software process and comparing these measurements to each other or to some desirable level. A software metric is ""any type of measurement that relates to a software system, process or related documentation"" [7]. Software metrics can help to improve the quality of the produced software. However, metrics and metrics tools are still not used in most software companies -- for example in the 3 companies where we cooperated in last 5 years. One reason is that there is lack of knowledge about metrics and hence software metrics are still unknown or difficult to use for some developers, and software measurement is still time-consuming for managers. Another reason is that, good metrics tools are still expensive for small and middle companies. In our opinion the effectiveness of metrics can be improved by simple organizational expedients. In this paper we present new/adapted requirements on metrics in CASE tools to define flexible product quality models. For this quality model we tried to use some standard techniques, e.g. metrics suite, metrics visualization or metrics filtering to show how metrics in CASE tools can be defined and how it can benefit different people who are involved in a software deployment. coding tools and techniques, design tools and techniques, management - software quality assurance, metrics",a concept of quality assurance for metrics in case tools the quality of software has become more important to software companies in the past years software measurement is one of many approaches that is used to check the quality of software 18 this may involve measuring some attributes of a software product or a software process and comparing these measurements to each other or to some desirable level a software metric is any type of measurement that relates to a software system process or related documentation 7 software metrics can help to improve the quality of the produced software however metrics and metrics tools are still not used in most software companies for example in the 3 companies where we cooperated in last 5 years one reason is that there is lack of knowledge about metrics and hence software metrics are still unknown or difficult to use for some developers and software measurement is still time consuming for managers another reason is that good metrics tools are still expensive for small and middle companies in our opinion the effectiveness of metrics can be improved by simple organizational expedients in this paper we present new adapted requirements on metrics in case tools to define flexible product quality models for this quality model we tried to use some standard techniques e g metrics suite metrics visualization or metrics filtering to show how metrics in case tools can be defined and how it can benefit different people who are involved in a software deployment coding tools and techniques design tools and techniques management software quality assurance metrics,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
61,0,296,"A Conceptual Framework for Delivering Cost Effective Business Intelligence Solutions As a Service Smart use of business intelligence (BI) can allow organizations to leverage the huge amounts of transactional data at their disposal and turn it into a powerful decision support mechanism that gives them competitive advantage. Despite the potential benefits of an effective BI system, the adoption and use of BI systems within the enterprise remains low, especially among smaller companies with resource constraints. This can partly be explained by the predominant deployment approach available today in which a firm needs to procure, install, configure and operate a BI system in-house. Barriers of high cost, complexity and lack of in-house expertise discourage many firms from adopting BI systems. This paper argues that adopting a cloud computing model, where BI is offered as a service over the Internet can lower these barriers and accelerate the pace of BI adoption. However, migrating BI systems from traditional on-premise environments to the cloud presents huge challenges. There are technical, economic, organizational and regulatory hurdles to overcome. Further, BI systems are multi-component (ETL, Data warehouse, data marts, OLAP, reporting, data mining etc.) and deciding which component(s) to move to the cloud, and which ones to leave on-premise needs careful consideration. In addition, the fact that cloud computing is still in its infancy means there is a general lack of conceptual and architectural frameworks to guide companies considering migrating enterprise systems to the cloud. This paper takes a closer look into traditional BI and proposes a conceptual framework that companies can use to chart an adoption path for cloud BI. The framework combines attributes of IT outsourcing, traditional BI, cloud computing as well as decision theory to present a consolidated view of cloud BI. The domain of South African Higher Education was chosen as the target in which the framework will be tested. BI as a service, SaaS, business intelligence, cloud BI, cloud computing, data warehouses",a conceptual framework for delivering cost effective business intelligence solutions as a service smart use of business intelligence bi can allow organizations to leverage the huge amounts of transactional data at their disposal and turn it into a powerful decision support mechanism that gives them competitive advantage despite the potential benefits of an effective bi system the adoption and use of bi systems within the enterprise remains low especially among smaller companies with resource constraints this can partly be explained by the predominant deployment approach available today in which a firm needs to procure install configure and operate a bi system in house barriers of high cost complexity and lack of in house expertise discourage many firms from adopting bi systems this paper argues that adopting a cloud computing model where bi is offered as a service over the internet can lower these barriers and accelerate the pace of bi adoption however migrating bi systems from traditional on premise environments to the cloud presents huge challenges there are technical economic organizational and regulatory hurdles to overcome further bi systems are multi component etl data warehouse data marts olap reporting data mining etc and deciding which component s to move to the cloud and which ones to leave on premise needs careful consideration in addition the fact that cloud computing is still in its infancy means there is a general lack of conceptual and architectural frameworks to guide companies considering migrating enterprise systems to the cloud this paper takes a closer look into traditional bi and proposes a conceptual framework that companies can use to chart an adoption path for cloud bi the framework combines attributes of it outsourcing traditional bi cloud computing as well as decision theory to present a consolidated view of cloud bi the domain of south african higher education was chosen as the target in which the framework will be tested bi as a service saas business intelligence cloud bi cloud computing data warehouses,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
62,0,95,"A Control Theory Based Approach for Self-healing of Un-handled Runtime Exceptions This work presents an approach to self-healing that deals with un-handled exceptions within an executing program. More precisely, we propose an approach based on control theory that automatically disables system functionalities that have led to runtime exceptions. This approach requires the system to be instrumented prior to deployment so that it can later interact with a supervisor. This supervisor encodes the only sequences of actions (method calls) of the system that are permitted. We describe an implementation that automatically generates instrumentation for Java systems and demonstrate the efficacy of this approach through a comprehensive example. self-healing, software control theory, softwaremaintenance.",a control theory based approach for self healing of un handled runtime exceptions this work presents an approach to self healing that deals with un handled exceptions within an executing program more precisely we propose an approach based on control theory that automatically disables system functionalities that have led to runtime exceptions this approach requires the system to be instrumented prior to deployment so that it can later interact with a supervisor this supervisor encodes the only sequences of actions method calls of the system that are permitted we describe an implementation that automatically generates instrumentation for java systems and demonstrate the efficacy of this approach through a comprehensive example self healing software control theory softwaremaintenance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
63,0,150,"A Cost-benefit Analysis of Data Processing Architectures for the Smart Grid Information and communication technology (ICT) infrastructure plays an important role to realize the full potential of Smart Grid applications. Smart grids utilize ICT entities to enhance efficiency, reliability and sustainability of power generation and distribution network. Majority of the architectures proposed hitherto focus only on a specific architectural aspect, like communication, storage, processing requirement, etc. Recent studies have shown that lack of knowledge on which architecture best satisfies certain information management requirements has hindered large scale smart grid deployments. In this paper, we investigate the cost-benefit analysis of four data processing architectures for various applications in smart grid. We introduce several key cost indicators to analyze hierarchical data processing architectures for the smart grid. In our evaluation, we consider realistic deployments for both dense and sparse environments. Results reported here are significant for smart grid designers, who can use them to discern the architecture that best fits the system requirements. data processing, distributed information systems, smart grid",a cost benefit analysis of data processing architectures for the smart grid information and communication technology ict infrastructure plays an important role to realize the full potential of smart grid applications smart grids utilize ict entities to enhance efficiency reliability and sustainability of power generation and distribution network majority of the architectures proposed hitherto focus only on a specific architectural aspect like communication storage processing requirement etc recent studies have shown that lack of knowledge on which architecture best satisfies certain information management requirements has hindered large scale smart grid deployments in this paper we investigate the cost benefit analysis of four data processing architectures for various applications in smart grid we introduce several key cost indicators to analyze hierarchical data processing architectures for the smart grid in our evaluation we consider realistic deployments for both dense and sparse environments results reported here are significant for smart grid designers who can use them to discern the architecture that best fits the system requirements data processing distributed information systems smart grid,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
64,0,189,"A Data-driven Synchronization Technique for Cyber-physical Systems In this paper, we present a method to synchronize data from multiple sensors in a cyber-physical system without any software or hardware modifications to the sensors. This method allows for synchronization of low-power embedded systems in heterogeneous sensor networks, regardless of accuracy of individual sensor clocks by using the events in the physical world to drive the synchronization in the cyber world. We propose two methods to select portions of sensor data streams to drive the synchronization: one leveraging the notion of known templates and the other using an information theoretic approach. Using the events as well as cues from the delay models, we determine alignment points between the data streams. These alignment points are used to synchronize the data. This novel approach is based solely on the sensor data for synchronization, and it can be applied post-deployment on systems of heterogeneous sensors that are not well designed and lack effective synchronization. Experiments show an average accuracy improvement from 12000ppm to 2400ppm for a template based-method and from 12000 to 277ppm and 445ppm for information theoretic methods when comparing the synchronized (corrected) clock data to an ideal clock source. alignment, cyber-physical systems, data-driven, sensor networks, synchronization",a data driven synchronization technique for cyber physical systems in this paper we present a method to synchronize data from multiple sensors in a cyber physical system without any software or hardware modifications to the sensors this method allows for synchronization of low power embedded systems in heterogeneous sensor networks regardless of accuracy of individual sensor clocks by using the events in the physical world to drive the synchronization in the cyber world we propose two methods to select portions of sensor data streams to drive the synchronization one leveraging the notion of known templates and the other using an information theoretic approach using the events as well as cues from the delay models we determine alignment points between the data streams these alignment points are used to synchronize the data this novel approach is based solely on the sensor data for synchronization and it can be applied post deployment on systems of heterogeneous sensors that are not well designed and lack effective synchronization experiments show an average accuracy improvement from 12000ppm to 2400ppm for a template based method and from 12000 to 277ppm and 445ppm for information theoretic methods when comparing the synchronized corrected clock data to an ideal clock source alignment cyber physical systems data driven sensor networks synchronization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
65,0,208,"A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically. Large-scale media streaming application, cloud, deployment optimization, local memory, multimedia big data",a deployment optimization scheme over multimedia big data for large scale media streaming application with the prosperity of media streaming applications over the internet in the past decades multimedia data has sharply increased categorized as multimedia big data which exerts more pressure on the infrastructure such as networking of the application provider in order to move this hurdle an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud with the elastic resource provisioning and centralized management of the cloud the operational costs of media streaming application providers can decrease dramatically however to the best of our knowledge existing migration solutions do not fully take viewer information such as hardware condition into consideration in this article we consider the deployment optimization problem named odp by leveraging local memories at each viewer considering the np hardness of calculating the optimal solution we turn to propose computationally tractable algorithms specifically we unfold the original problem into two interactive subproblems coarse grained migration subproblem and fine grained scheduling subproblem then the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given the results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically large scale media streaming application cloud deployment optimization local memory multimedia big data,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
66,0,166,"A DevOps Approach to Integration of Software Components in an EU Research Project We present a description of the development and deployment infrastructure being created to support the integration effort of HARNESS, an EU FP7 project. HARNESS is a multi-partner research project intended to bring the power of heterogeneous resources to the cloud. It consists of a number of different services and technologies that interact with the OpenStack cloud computing platform at various levels. Many of these components are being developed independently by different teams at different locations across Europe, and keeping the work fully integrated is a challenge. We use a combination of Vagrant based virtual machines, Docker containers, and Ansible playbooks to provide a consistent and up-to-date environment to each developer. The same playbooks used to configure local virtual machines are also used to manage a static testbed with heterogeneous compute and storage devices, and to automate ephemeral larger-scale deployments to Grid'5000. Access to internal projects is managed by GitLab, and automated testing of services within Docker-based environments and integrated deployments within virtual-machines is provided by Buildbot. Ansible, Automated Testing, BuildBot, Configuration Management, DevOps, Docker, GitLab, OpenStack, Vagrant",a devops approach to integration of software components in an eu research project we present a description of the development and deployment infrastructure being created to support the integration effort of harness an eu fp7 project harness is a multi partner research project intended to bring the power of heterogeneous resources to the cloud it consists of a number of different services and technologies that interact with the openstack cloud computing platform at various levels many of these components are being developed independently by different teams at different locations across europe and keeping the work fully integrated is a challenge we use a combination of vagrant based virtual machines docker containers and ansible playbooks to provide a consistent and up to date environment to each developer the same playbooks used to configure local virtual machines are also used to manage a static testbed with heterogeneous compute and storage devices and to automate ephemeral larger scale deployments to grid 5000 access to internal projects is managed by gitlab and automated testing of services within docker based environments and integrated deployments within virtual machines is provided by buildbot ansible automated testing buildbot configuration management devops docker gitlab openstack vagrant,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
67,0,109,"A Distributed Architecture for Intra- and Inter-cloud Data Management When envisioning ""The Cloud,"" one is often presented with an idyllic black box of functionality that seamlessly stores arbitrary amounts of data while providing endless CPU cycles. However, Cloud deployments are often partitioned along institutional and middleware boundaries that link computation and storage infrastructures. Decoupling storage from computation resource providers allows for greater flexibility in resource provisioning and new data storage paradigms to emerge. This paper proposes a decentralized data management architecture that facilitates interoperability between Clouds and other heterogeneous systems. The goals of this research are to augment the latent storage capacity within provisioned Cloud VMs with existing institutional resources to build low-cost Storage Clouds for scientific computing. attic, data management, p2p, peer-to-peer, science clouds",a distributed architecture for intra and inter cloud data management when envisioning the cloud one is often presented with an idyllic black box of functionality that seamlessly stores arbitrary amounts of data while providing endless cpu cycles however cloud deployments are often partitioned along institutional and middleware boundaries that link computation and storage infrastructures decoupling storage from computation resource providers allows for greater flexibility in resource provisioning and new data storage paradigms to emerge this paper proposes a decentralized data management architecture that facilitates interoperability between clouds and other heterogeneous systems the goals of this research are to augment the latent storage capacity within provisioned cloud vms with existing institutional resources to build low cost storage clouds for scientific computing attic data management p2p peer to peer science clouds,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
68,0,91,"A Distributed Test System Architecture for Open-source IoT Software In this paper, we discuss challenges that are specific to testing of open IoT software systems. The analysis reveals gaps compared to wireless sensor networks as well as embedded software. We propose a testing framework which (a) supports continuous integration techniques, (b) allows for the integration of project contributors to volunteer hardware and software resources to the test system, and (c) can function as a permanent distributed plugtest for network interoperability testing. The focus of this paper lies in open-source IoT development but many aspects are also applicable to closed-source projects. interoperability, open-source iot, test system architecture",a distributed test system architecture for open source iot software in this paper we discuss challenges that are specific to testing of open iot software systems the analysis reveals gaps compared to wireless sensor networks as well as embedded software we propose a testing framework which a supports continuous integration techniques b allows for the integration of project contributors to volunteer hardware and software resources to the test system and c can function as a permanent distributed plugtest for network interoperability testing the focus of this paper lies in open source iot development but many aspects are also applicable to closed source projects interoperability open source iot test system architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
69,0,205,"A Drift-tolerant Model for Data Management in Ocean Sensor Networks Traditional means of observing the ocean, like fixed mooring stations and radar systems, are difficult and expensive to deploy and provide coarse-grained and data measurements of currents and waves. In this paper, we explore the use of inexpensive wireless drifters as an alternative flexible infrastructure for fine-grained ocean monitoring. Surface drifters are designed specifically to move passively with the flow of water on the ocean surface and they are able to acquire sensor readings and GPS-generated positions at regular intervals. We view the fleet of drifters as a wireless ad-hoc sensor network with two types of nodes:i) a few powerful drifters with satellite connectivity, acting as mobile base-stations, and ii)a large number of low-power drifters with short-range acoustic or radio connectivity. Using real datasets from the Gulf of Maine (US) and the Liverpool Bay (UK), we study connectivity and uniformity properties of the ad-hoc mobile sensor network. We investigate the effect of deployment strategy, weather conditions as well as seasonal changes on the ability of drifters to relay readings to the end-users,and to provide sufficient sensing coverage of the monitored area. Our empirical study provides useful insights on how to design distributed routing and in-network processing algorithms tailored for ocean-monitoring sensor networks. ad-hoc sensor networks, drifters, geosensor networks, mobility, oceanography",a drift tolerant model for data management in ocean sensor networks traditional means of observing the ocean like fixed mooring stations and radar systems are difficult and expensive to deploy and provide coarse grained and data measurements of currents and waves in this paper we explore the use of inexpensive wireless drifters as an alternative flexible infrastructure for fine grained ocean monitoring surface drifters are designed specifically to move passively with the flow of water on the ocean surface and they are able to acquire sensor readings and gps generated positions at regular intervals we view the fleet of drifters as a wireless ad hoc sensor network with two types of nodes i a few powerful drifters with satellite connectivity acting as mobile base stations and ii a large number of low power drifters with short range acoustic or radio connectivity using real datasets from the gulf of maine us and the liverpool bay uk we study connectivity and uniformity properties of the ad hoc mobile sensor network we investigate the effect of deployment strategy weather conditions as well as seasonal changes on the ability of drifters to relay readings to the end users and to provide sufficient sensing coverage of the monitored area our empirical study provides useful insights on how to design distributed routing and in network processing algorithms tailored for ocean monitoring sensor networks ad hoc sensor networks drifters geosensor networks mobility oceanography,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
70,0,101,"A DSL for Executable 'How to' Manuals 'How to' manuals help potential users with deploying and understanding software technologies such as web applications or servers. In a domain analysis, we survey existing 'how to' manuals to assess the feasibility of making the manuals executable and to derive a suggestion for domain-specific language support. We realize a DSL for executable 'how to' manuals and refer to the approach as 'literate deployment scripting', as it is inspired by literate programming in that all the code for deployment and configuration is embedded into its documentation. This includes code (or commands) for an initial deployment or changes to a deployed software system. executable how to, executable manual, install scripts, literate deployment, literate programming, software deployment",a dsl for executable how to manuals how to manuals help potential users with deploying and understanding software technologies such as web applications or servers in a domain analysis we survey existing how to manuals to assess the feasibility of making the manuals executable and to derive a suggestion for domain specific language support we realize a dsl for executable how to manuals and refer to the approach as literate deployment scripting as it is inspired by literate programming in that all the code for deployment and configuration is embedded into its documentation this includes code or commands for an initial deployment or changes to a deployed software system executable how to executable manual install scripts literate deployment literate programming software deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
71,0,107,"A Dynamic and Distributed Key Management Scheme for Wireless Sensor Networks Ensuring key management in Wireless Sensor Networks has a vital importance, especially when sensor nodes have to communicate in hostile environments. In this paper, we propose a new simple and resource-aware key management scheme. The scheme is based on the idea that the initial pre-distributed key is not pre-loaded in all nodes to improve its resilience to node compromising attacks. Each node has to store an initial key, a set of prime number groups and a pseudo-random function. This pre-distributed secret information is used to establish pairwise keys between adjacent nodes after their deployment. Moreover, the proposed scheme is dynamic since it allows a flexible key refresh. Distributed algorithms, Key management, Security, WSNs",a dynamic and distributed key management scheme for wireless sensor networks ensuring key management in wireless sensor networks has a vital importance especially when sensor nodes have to communicate in hostile environments in this paper we propose a new simple and resource aware key management scheme the scheme is based on the idea that the initial pre distributed key is not pre loaded in all nodes to improve its resilience to node compromising attacks each node has to store an initial key a set of prime number groups and a pseudo random function this pre distributed secret information is used to establish pairwise keys between adjacent nodes after their deployment moreover the proposed scheme is dynamic since it allows a flexible key refresh distributed algorithms key management security wsns,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
72,0,304,"A Dynamic Ontology for a Dynamic Reference Work The successful deployment of digital technologies by humanities scholars presents computer scientists with a number of unique scientific and technological challenges. The task seems particularly daunting because issues in the humanities are presented in abstract language demanding the kind of subtle interpretation often thought to be beyond the scope of artificial intelligence, and humanities scholars themselves often disagree about the structure of their disciplines. The future of humanities computing depends on having tools for automatically discovering complex semantic relationships among different parts of a corpus. Digital library tools for the humanities will need to be capable of dynamically tracking the introduction of new ideas and interpretations and applying them to older texts in ways that support the needs of scholars and students. This paper describes the design of new algorithms and the adjustment of existing algorithms to support the automated and semi-automated management of domain-rich metadata for an established digital humanities project, the Stanford Encyclopedia of Philosophy. Our approach starts with a ""hand-built"" formal ontology that is modified and extended by a combination of automated and semi-automated methods, thus becoming a ""dynamic ontology"". We assess the suitability of current information retrieval and information extraction methods for the task of automatically maintaining the ontology. We describe a novel measure of term-relatedness that appears to be particularly helpful for predicting hierarchical relationships in the ontology. We believe that our project makes a further contribution to information science by being the first to harness the collaboration inherent in a expert-maintained dynamic reference work to the task of maintaining and verifying a formal ontology. We place special emphasis on the task of bringing domain expertise to bear on all phases of the development and deployment of the system, from the initial design of the software and ontology to its dynamic use in a fully operational digital reference work. digital humanities, dynamic ontology, formal ontology, information extraction, information retrieval, link mining, metadata",a dynamic ontology for a dynamic reference work the successful deployment of digital technologies by humanities scholars presents computer scientists with a number of unique scientific and technological challenges the task seems particularly daunting because issues in the humanities are presented in abstract language demanding the kind of subtle interpretation often thought to be beyond the scope of artificial intelligence and humanities scholars themselves often disagree about the structure of their disciplines the future of humanities computing depends on having tools for automatically discovering complex semantic relationships among different parts of a corpus digital library tools for the humanities will need to be capable of dynamically tracking the introduction of new ideas and interpretations and applying them to older texts in ways that support the needs of scholars and students this paper describes the design of new algorithms and the adjustment of existing algorithms to support the automated and semi automated management of domain rich metadata for an established digital humanities project the stanford encyclopedia of philosophy our approach starts with a hand built formal ontology that is modified and extended by a combination of automated and semi automated methods thus becoming a dynamic ontology we assess the suitability of current information retrieval and information extraction methods for the task of automatically maintaining the ontology we describe a novel measure of term relatedness that appears to be particularly helpful for predicting hierarchical relationships in the ontology we believe that our project makes a further contribution to information science by being the first to harness the collaboration inherent in a expert maintained dynamic reference work to the task of maintaining and verifying a formal ontology we place special emphasis on the task of bringing domain expertise to bear on all phases of the development and deployment of the system from the initial design of the software and ontology to its dynamic use in a fully operational digital reference work digital humanities dynamic ontology formal ontology information extraction information retrieval link mining metadata,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
73,0,182,"A Feature-based Approach to System Deployment and Adaptation Building large scale systems involves many design decisions, both at specification and implementation levels. This is due to numerous variants in the description of the task to achieve and its execution context as well as in the assembly of software components. We have modeled variability for large scale systems using feature diagrams, a formalism well suited for modeling variablility. These models are built with a clear separation of concerns between specification and implementation aspects. They are used at design and deployment time as well as at execution time. Our test application domain is video surveillance systems, from a software engineering perspective. These are good candidates to put model driven engineering to the test, because of the huge variability in both the surveillance tasks and the video analysis algorithms. They are also dynamically adaptive systems, thus suitable for models at run time approaches. We propose techniques and tools to define the models, to operate on them, and to transform specification requirements into an effective implementation of a processing chain. We also define a run time architecture to integrate models into the adaptation loop. feature model, models at run time, software variability, video surveillance",a feature based approach to system deployment and adaptation building large scale systems involves many design decisions both at specification and implementation levels this is due to numerous variants in the description of the task to achieve and its execution context as well as in the assembly of software components we have modeled variability for large scale systems using feature diagrams a formalism well suited for modeling variablility these models are built with a clear separation of concerns between specification and implementation aspects they are used at design and deployment time as well as at execution time our test application domain is video surveillance systems from a software engineering perspective these are good candidates to put model driven engineering to the test because of the huge variability in both the surveillance tasks and the video analysis algorithms they are also dynamically adaptive systems thus suitable for models at run time approaches we propose techniques and tools to define the models to operate on them and to transform specification requirements into an effective implementation of a processing chain we also define a run time architecture to integrate models into the adaptation loop feature model models at run time software variability video surveillance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
74,0,105,"A Fine-grained and Flexible Version Control for Software Artifacts Version control is an activity very important for high-quality software production. The structure used by version control systems is the same used by file systems, but in general the abstraction level made by software developers considers the file contents and its internal structure, including details as classes, methods, control blocks and others. Fine-grained version control tools can provide a more detailed version control. However traditional tools and models provide very low flexibility and present high cost and impact of deployment in software development environments. In this paper, there are presented a model and a tool which aim at providing support to fine-grained version control activities. software configuration management, version control",a fine grained and flexible version control for software artifacts version control is an activity very important for high quality software production the structure used by version control systems is the same used by file systems but in general the abstraction level made by software developers considers the file contents and its internal structure including details as classes methods control blocks and others fine grained version control tools can provide a more detailed version control however traditional tools and models provide very low flexibility and present high cost and impact of deployment in software development environments in this paper there are presented a model and a tool which aim at providing support to fine grained version control activities software configuration management version control,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
75,0,194,"A Fragment-based Approach for Efficiently Creating Dynamic Web Content This article presents a publishing system for efficiently creating dynamic Web content. Complex Web pages are constructed from simpler fragments. Fragments may recursively embed other fragments. Relationships between Web pages and fragments are represented by object dependence graphs. We present algorithms for efficiently detecting and updating Web pages affected after one or more fragments change. We also present algorithms for publishing sets of Web pages consistently; different algorithms are used depending upon the consistency requirements.Our publishing system provides an easy method for Web site designers to specify and modify inclusion relationships among Web pages and fragments. Users can update content on multiple Web pages by modifying a template. The system then automatically updates all Web pages affected by the change. Our system accommodates both content that must be proofread before publication and is typically from humans as well as content that has to be published immediately and is typically from automated feeds.We discuss some of our experiences with real deployments of our system as well as its performance. We also quantitatively present characteristics of fragments used at a major deployment of our publishing system including fragment sizes, update frequencies, and inclusion relationships. Caching, Web, Web performance, dynamic content, fragments, publishing",a fragment based approach for efficiently creating dynamic web content this article presents a publishing system for efficiently creating dynamic web content complex web pages are constructed from simpler fragments fragments may recursively embed other fragments relationships between web pages and fragments are represented by object dependence graphs we present algorithms for efficiently detecting and updating web pages affected after one or more fragments change we also present algorithms for publishing sets of web pages consistently different algorithms are used depending upon the consistency requirements our publishing system provides an easy method for web site designers to specify and modify inclusion relationships among web pages and fragments users can update content on multiple web pages by modifying a template the system then automatically updates all web pages affected by the change our system accommodates both content that must be proofread before publication and is typically from humans as well as content that has to be published immediately and is typically from automated feeds we discuss some of our experiences with real deployments of our system as well as its performance we also quantitatively present characteristics of fragments used at a major deployment of our publishing system including fragment sizes update frequencies and inclusion relationships caching web web performance dynamic content fragments publishing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
76,0,152,"A Framework for Instantiating Native Mobile Multimedia Learning Applications on Android Platform This paper proposes an extendable Native Mobile Multimedia Learning Application (NMMLA) Framework for instantiating mobile learning applications for various educational subjects and courses on the Android platform. The framework supports a scalable number of components, which include Learn, Evaluate, Simulate, Resources, Chat, e-Quiz etc. It is a one-page-setup and do-it-yourself library that will facilitate the development and deployment of NMMLAs. Thus, it supports four major types of multimedia learning content---images, Hypertext Markup Language (HTML), audio and video---aimed at meeting different learning preferences. Moreover, it supports active and face-toface collaborative learning, such as simulations, chatting, and application/content sharing via Bluetooth, email and on social networks. Above all, it offers key application features, such as theme, course and quiz menus; listview/tabview presentational modes; and Search and Help utilities. This work will benefit practitioners in the m-learning field by providing a content flow algorithm tree that will prevent reinventing the wheel. M-learning,  learning framework,  multimedia,  Android platform",a framework for instantiating native mobile multimedia learning applications on android platform this paper proposes an extendable native mobile multimedia learning application nmmla framework for instantiating mobile learning applications for various educational subjects and courses on the android platform the framework supports a scalable number of components which include learn evaluate simulate resources chat e quiz etc it is a one page setup and do it yourself library that will facilitate the development and deployment of nmmlas thus it supports four major types of multimedia learning content images hypertext markup language html audio and video aimed at meeting different learning preferences moreover it supports active and face toface collaborative learning such as simulations chatting and application content sharing via bluetooth email and on social networks above all it offers key application features such as theme course and quiz menus listview tabview presentational modes and search and help utilities this work will benefit practitioners in the m learning field by providing a content flow algorithm tree that will prevent reinventing the wheel m learning learning framework multimedia android platform,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
77,0,187,"A Framework for Rapid Integration of Presentation Components The development of user interfaces (UIs) is one of the most time-consuming aspects in software development. In this context, the lack of proper reuse mechanisms for UIs is increasingly becoming manifest, especially as software development is more and more moving toward composite applications. In this paper we propose a framework for the integration of stand-alone modules or applications, where integration occurs at the presentation layer. Hence, the final goal is to reduce the effort required for UI development by maximizing reus. The design of the framework is inspired by lessons learned from application integration, appropriately modified to account for the specificity of the UI integration problem. We provide an abstract component model to specify characteristics and behaviors of presentation components and propose an event-based composition model to specify the composition logic. Components and composition are described by means of a simple XML-based language, which is interpreted by a runtime middleware for the execution of the resulting composite application. A proof-of-concept prototype allows us to show that the proposed component model can also easily be applied to existing presentation components, built with different languages and/or component technologies. XPIL, component model, presentation component, presentation composition, presentation integration, user interface (UI)",a framework for rapid integration of presentation components the development of user interfaces uis is one of the most time consuming aspects in software development in this context the lack of proper reuse mechanisms for uis is increasingly becoming manifest especially as software development is more and more moving toward composite applications in this paper we propose a framework for the integration of stand alone modules or applications where integration occurs at the presentation layer hence the final goal is to reduce the effort required for ui development by maximizing reus the design of the framework is inspired by lessons learned from application integration appropriately modified to account for the specificity of the ui integration problem we provide an abstract component model to specify characteristics and behaviors of presentation components and propose an event based composition model to specify the composition logic components and composition are described by means of a simple xml based language which is interpreted by a runtime middleware for the execution of the resulting composite application a proof of concept prototype allows us to show that the proposed component model can also easily be applied to existing presentation components built with different languages and or component technologies xpil component model presentation component presentation composition presentation integration user interface ui,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
78,0,210,"A Framework to Leverage Cloud for Modernization of Indian Agricultural Produce Marketing System In India, Information and Communication technology (ICT) is being leveraged as a modernization tool in almost every sector of economy such as health, education, and transportation. But when we consider the agricultural scenario in the Indian context, we realise that the ICT remains to be exploited to accrue its invaluable benefits. In recent times, the Government of India has introduced several initiatives to promote the application of ICT in agriculture sector. But when we compare the scale of ICT application in Indian agriculture sector with other developing countries like China, Brazil, etc., we find that application of ICT in Indian agriculture is yet to be applied on a significant magnitude. In this paper, we propose a cloud deployment model ""Agri-Bridge"", which provides access to agricultural market related information to farmers facing market connectivity constraints and acute capital shortage. Also, this model will operate as a bridge between the farmers and consumers within the existing agricultural produce marketing chain. This model utilizes the existing Government services, Agricultural Produce Marketing Committee (APMC) databases, retail market sources besides leveraging cloud computing, mobile phone services and Internet services to provide a solution to the problem of lack of access to real-time market information to the farmers, hence modernising the Indian agricultural produce marketing system. AGRI-CLOUD, Agmarknet, Agri-Bridge, Cloud Deployment, ICT, e-Agriculture, e-Choupal",a framework to leverage cloud for modernization of indian agricultural produce marketing system in india information and communication technology ict is being leveraged as a modernization tool in almost every sector of economy such as health education and transportation but when we consider the agricultural scenario in the indian context we realise that the ict remains to be exploited to accrue its invaluable benefits in recent times the government of india has introduced several initiatives to promote the application of ict in agriculture sector but when we compare the scale of ict application in indian agriculture sector with other developing countries like china brazil etc we find that application of ict in indian agriculture is yet to be applied on a significant magnitude in this paper we propose a cloud deployment model agri bridge which provides access to agricultural market related information to farmers facing market connectivity constraints and acute capital shortage also this model will operate as a bridge between the farmers and consumers within the existing agricultural produce marketing chain this model utilizes the existing government services agricultural produce marketing committee apmc databases retail market sources besides leveraging cloud computing mobile phone services and internet services to provide a solution to the problem of lack of access to real time market information to the farmers hence modernising the indian agricultural produce marketing system agri cloud agmarknet agri bridge cloud deployment ict e agriculture e choupal,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
79,0,121,"A Further Study in the Data Partitioning Approach for Frequent Itemsets Mining Frequent itemsets mining is well explored for various data types, and its computational complexity is well understood. Based on our previous work by Nguyen and Orlowska (2005), this paper shows the extension of the data pre-processing approach to further improve the performance of frequent itemsets computation. The methods focus on potential reduction of the size of the input data required for deployment of the partitioning based algorithms.We have made a series of the data pre-processing methods such that the final step of the Partition algorithm, where a combination of all local candidate sets must be processed, is executed on substantially smaller input data. Moreover, we have made a comparison among these methods based on the experiments with particular data sets. algorithm, data mining, frequent itemset, partition, performance",a further study in the data partitioning approach for frequent itemsets mining frequent itemsets mining is well explored for various data types and its computational complexity is well understood based on our previous work by nguyen and orlowska 2005 this paper shows the extension of the data pre processing approach to further improve the performance of frequent itemsets computation the methods focus on potential reduction of the size of the input data required for deployment of the partitioning based algorithms we have made a series of the data pre processing methods such that the final step of the partition algorithm where a combination of all local candidate sets must be processed is executed on substantially smaller input data moreover we have made a comparison among these methods based on the experiments with particular data sets algorithm data mining frequent itemset partition performance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
80,0,187,"A Gamification Approach for Distributed Agile Delivery Large organizations need to be nimble in delivering software solutions for meeting rapidly changing business requirements and technology landscape. Following Agile principles of software development is a natural choice. However, to truly leverage the power of Agile, big organizations need to be able to utilize distributed teams effectively. Agile relies hugely on shared context and awareness among team members and this can become a stumbling block among such geographically dispersed teams. Moreover, in such large projects there is a need for incentivizing quick delivery of user stories so that the teams have a constructive sense of competition and are recognized in-process. Here, we describe a gamification based approach which promotes quicker completion and acceptance of user stories in such distributed Agile projects. Our approach captures important events from the development environment and then helps create project-wide awareness regarding the progress of different teams. A model of earning revenue for faster delivery of user stories is used to determine the leading team at the end of each sprint. This approach has been implemented in an Agile process guidance and awareness workbench that we are piloting within our organization. agile gamification, distributed agile, software delivery",a gamification approach for distributed agile delivery large organizations need to be nimble in delivering software solutions for meeting rapidly changing business requirements and technology landscape following agile principles of software development is a natural choice however to truly leverage the power of agile big organizations need to be able to utilize distributed teams effectively agile relies hugely on shared context and awareness among team members and this can become a stumbling block among such geographically dispersed teams moreover in such large projects there is a need for incentivizing quick delivery of user stories so that the teams have a constructive sense of competition and are recognized in process here we describe a gamification based approach which promotes quicker completion and acceptance of user stories in such distributed agile projects our approach captures important events from the development environment and then helps create project wide awareness regarding the progress of different teams a model of earning revenue for faster delivery of user stories is used to determine the leading team at the end of each sprint this approach has been implemented in an agile process guidance and awareness workbench that we are piloting within our organization agile gamification distributed agile software delivery,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
81,0,77,"A General Approach for a Trusted Deployment of a Business Process in Clouds It is recognized that the most important obstacle to the development of the cloud is the variety of new security threats which requests new methods and mechanisms. This is even truer for those who want to deploy business processes, because of the critical knowledge they encapsulate in terms of know-how and data. This paper proposes an approach combining modeling techniques and cloud selection for a trusted deployment of a security risk-aware business process in security constrained clouds. business process, cloud, security",a general approach for a trusted deployment of a business process in clouds it is recognized that the most important obstacle to the development of the cloud is the variety of new security threats which requests new methods and mechanisms this is even truer for those who want to deploy business processes because of the critical knowledge they encapsulate in terms of know how and data this paper proposes an approach combining modeling techniques and cloud selection for a trusted deployment of a security risk aware business process in security constrained clouds business process cloud security,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
82,0,191,"A Generic Component Model for Building Systems Software Component-based software structuring principles are now commonplace at the application level; but componentization is far less established when it comes to building low-level systems software. Although there have been pioneering efforts in applying componentization to systems-building, these efforts have tended to target specific application domains (e.g., embedded systems, operating systems, communications systems, programmable networking environments, or middleware platforms). They also tend to be targeted at specific deployment environments (e.g., standard personal computer (PC) environments, network processors, or microcontrollers). The disadvantage of this narrow targeting is that it fails to maximize the genericity and abstraction potential of the component approach. In this article, we argue for the benefits and feasibility of a generic yet tailorable approach to component-based systems-building that offers a uniform programming model that is applicable in a wide range of systems-oriented target domains and deployment environments. The component model, called OpenCom, is supported by a reflective runtime architecture that is itself built from components. After describing OpenCom and evaluating its performance and overhead characteristics, we present and evaluate two case studies of systems we have built using OpenCom technology, thus illustrating its benefits and its general applicability. Component-based software, computer systems implementation",a generic component model for building systems software component based software structuring principles are now commonplace at the application level but componentization is far less established when it comes to building low level systems software although there have been pioneering efforts in applying componentization to systems building these efforts have tended to target specific application domains e g embedded systems operating systems communications systems programmable networking environments or middleware platforms they also tend to be targeted at specific deployment environments e g standard personal computer pc environments network processors or microcontrollers the disadvantage of this narrow targeting is that it fails to maximize the genericity and abstraction potential of the component approach in this article we argue for the benefits and feasibility of a generic yet tailorable approach to component based systems building that offers a uniform programming model that is applicable in a wide range of systems oriented target domains and deployment environments the component model called opencom is supported by a reflective runtime architecture that is itself built from components after describing opencom and evaluating its performance and overhead characteristics we present and evaluate two case studies of systems we have built using opencom technology thus illustrating its benefits and its general applicability component based software computer systems implementation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
83,0,117,"A Generic Mechanism for Cloud Service Governance and Quality Control With the pervasion of cloud computing the enterprise IT environment is progressively transformed into an ecosystem of highly distributed, task-oriented, modular, and collaborative cloud services. In order to deal effectively with the complexity inherent in such ecosystems, future enterprises are anticipated to increasingly rely on cloud service brokerage (CSB). This paper presents a CSB mechanism which offers capabilities with respect to the Quality Assurance dimension of CSB. The proposed mechanism evaluates the compliance of cloud services with pre-specified policies concerning the technical, and mainly the business aspects, of service deployment and delivery. By relying on a declarative representation of both services and policies, the proposed mechanism is kept generic and orthogonal to any underlying cloud delivery platform. cloud computing, cloud service brokerage, linked USDL, policy-based governance, quality control, service description languages, service governance",a generic mechanism for cloud service governance and quality control with the pervasion of cloud computing the enterprise it environment is progressively transformed into an ecosystem of highly distributed task oriented modular and collaborative cloud services in order to deal effectively with the complexity inherent in such ecosystems future enterprises are anticipated to increasingly rely on cloud service brokerage csb this paper presents a csb mechanism which offers capabilities with respect to the quality assurance dimension of csb the proposed mechanism evaluates the compliance of cloud services with pre specified policies concerning the technical and mainly the business aspects of service deployment and delivery by relying on a declarative representation of both services and policies the proposed mechanism is kept generic and orthogonal to any underlying cloud delivery platform cloud computing cloud service brokerage linked usdl policy based governance quality control service description languages service governance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
84,0,139,"A Geo-Adaptive JavaScript DASH Player In Dynamic Adaptive Streaming over HTTP (DASH), the video bitrate is adapted to the network's condition based on client's feedback. Typically, a high bitrate is selected for next video chunk when the average of previously observed bandwidth samples is high and vice versa. An unexpected drop in the network throughput causes video freezing as the adaptation is done based on network's status in previous moment. In a vehicular environment, this problem is more likely to occur due to rapid movements of client to new locations where the network throughput can be very volatile. In this paper, we present a modified JavaScript DASH player that enables the streaming client to optimize the streaming performance by taking advantage of pre-collected bandwidth statistics in different locations. In this video player, Markov Decision Process (MDP) has been considered as the underlying optimization framework. adaptive video streaming, bandwidth map",a geo adaptive javascript dash player in dynamic adaptive streaming over http dash the video bitrate is adapted to the network s condition based on client s feedback typically a high bitrate is selected for next video chunk when the average of previously observed bandwidth samples is high and vice versa an unexpected drop in the network throughput causes video freezing as the adaptation is done based on network s status in previous moment in a vehicular environment this problem is more likely to occur due to rapid movements of client to new locations where the network throughput can be very volatile in this paper we present a modified javascript dash player that enables the streaming client to optimize the streaming performance by taking advantage of pre collected bandwidth statistics in different locations in this video player markov decision process mdp has been considered as the underlying optimization framework adaptive video streaming bandwidth map,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
85,0,696,"A Global Collaboration to Deploy Help to China A firsthand account of an international team effort to install the Sahana disaster-management system in Chengdu, Sichuan after an earthquake. On Monday May 12, 2008, an earthquake measuring 7.9 on the Richter scale struck in Sichuan Province in southwestern China, destroying homes, schools, hospitals, roads, and vital power and communication infrastructure. More than 45 million people were affected---tens of thousands were killed, hundreds of thousands injured, millions of people were evacuated and left homeless, and millions of buildings were destroyed. When the earthquake hit, several members of what became an international, volunteer, disaster-management IT team were attending a workshop in Washington, D.C. The workshop was organized by the IBM Office of Corporate Citizenship and Corporate Affairs department to train IBM personnel and others in the use and deployment of Sahana, a free and open source software (FOSS) disaster management system. Sahana, which means relief in Sinhalese, is a Web-based collaboration tool that helps manage information resources during a disaster recovery effort. It supports a wide range of relief efforts from finding missing persons, to managing volunteers, tracking resources, and coordinating refugee camps. Sahana enables government groups, non-governmental organizations (NGOs) and the victims themselves to work together during a disaster recovery effort. Over the next several weeks, the team members, distributed among several cities (Beijing and Chengdu in China, Hartford and New York in the U.S., and Colombo in Sri Lanka), worked together over global communication channels to configure and deploy Sahana in Chengdu, in order to support the disaster recovery effort there. The organizations involved in the collaboration included: * The Lanka Software Foundation (LSF), developers of the Sahana system. Three LSF members, led by the second author, were conducting the training workshop. * Various departments of IBM, including Business Continuity and Resiliency Services. Ten employees led by the fourth author, who organized the workshop, were receiving instruction in how to deploy and use Sahana. * The Humanitarian FOSS Project (H-FOSS), an NSF-funded effort aimed at revitalizing undergraduate computing education. Four students and their mentors, the first and third authors, were attending the workshop as developers and undergraduate members of the Sahana community. * IBM China. Initially, local teams in Beijing and Chengdu consisting of corporate citizenship, government relations, and technical professionals led in demonstrating Sahana to local officials, securing buy-in, and establishing channels to proceed. Then a large team of developers, language specialists, and others, including a team based in Chengdu, Sichuan, eventually took charge of the deployment effort in Chengdu. The fifth author was a member of the China development team. Almost immediately after the earthquake, discussions were held between IBM, IBM China, China's Ministry of Civil Affairs, and the Chengdu city government in Sichuan province. Once the Chengdu government expressed real interest in deploying Sahana, a team was formed to begin the process of localizing Sahana---that is, translating its user interface into simplified Chinese. The team was led by executives and software developers from IBM-China and assisted by Sahana team members in Colombo and student H-FOSS volunteers in Hartford. The team's organizational structure followed the normal procedure involved in previous Sahana deployments---a local group in close proximity to the incident supported by volunteers from the global Sahana community. In this case IBM-China, including some who were directly affected by the disaster, took the lead in deploying Sahana over an intensive three-week period. The decision by the Chengdu government to proceed with the deployment was taken on May 21, 2008 and a revised and localized version of Sahana was deployed in Chengdu on May 25. On June 12 we learned that 42 families had been reunited with the help of Sahana. This article provides an inside look at the deployment effort. It describes how a diverse, multidisciplinary team---professional programmers, software engineers, executives from a large global enterprise, students, faculty, and humanitarian IT specialists from a global FOSS community---worked together to assist the earthquake recovery effort. The success of the collaboration illustrates the power of virtual communities working across international boundaries using a variety of electronic communication software. It also demonstrates that the Internet has truly made us all neighbors and is constantly forcing us to redefine our concept of community. []",a global collaboration to deploy help to china a firsthand account of an international team effort to install the sahana disaster management system in chengdu sichuan after an earthquake on monday may 12 2008 an earthquake measuring 7 9 on the richter scale struck in sichuan province in southwestern china destroying homes schools hospitals roads and vital power and communication infrastructure more than 45 million people were affected tens of thousands were killed hundreds of thousands injured millions of people were evacuated and left homeless and millions of buildings were destroyed when the earthquake hit several members of what became an international volunteer disaster management it team were attending a workshop in washington d c the workshop was organized by the ibm office of corporate citizenship and corporate affairs department to train ibm personnel and others in the use and deployment of sahana a free and open source software foss disaster management system sahana which means relief in sinhalese is a web based collaboration tool that helps manage information resources during a disaster recovery effort it supports a wide range of relief efforts from finding missing persons to managing volunteers tracking resources and coordinating refugee camps sahana enables government groups non governmental organizations ngos and the victims themselves to work together during a disaster recovery effort over the next several weeks the team members distributed among several cities beijing and chengdu in china hartford and new york in the u s and colombo in sri lanka worked together over global communication channels to configure and deploy sahana in chengdu in order to support the disaster recovery effort there the organizations involved in the collaboration included the lanka software foundation lsf developers of the sahana system three lsf members led by the second author were conducting the training workshop various departments of ibm including business continuity and resiliency services ten employees led by the fourth author who organized the workshop were receiving instruction in how to deploy and use sahana the humanitarian foss project h foss an nsf funded effort aimed at revitalizing undergraduate computing education four students and their mentors the first and third authors were attending the workshop as developers and undergraduate members of the sahana community ibm china initially local teams in beijing and chengdu consisting of corporate citizenship government relations and technical professionals led in demonstrating sahana to local officials securing buy in and establishing channels to proceed then a large team of developers language specialists and others including a team based in chengdu sichuan eventually took charge of the deployment effort in chengdu the fifth author was a member of the china development team almost immediately after the earthquake discussions were held between ibm ibm china china s ministry of civil affairs and the chengdu city government in sichuan province once the chengdu government expressed real interest in deploying sahana a team was formed to begin the process of localizing sahana that is translating its user interface into simplified chinese the team was led by executives and software developers from ibm china and assisted by sahana team members in colombo and student h foss volunteers in hartford the team s organizational structure followed the normal procedure involved in previous sahana deployments a local group in close proximity to the incident supported by volunteers from the global sahana community in this case ibm china including some who were directly affected by the disaster took the lead in deploying sahana over an intensive three week period the decision by the chengdu government to proceed with the deployment was taken on may 21 2008 and a revised and localized version of sahana was deployed in chengdu on may 25 on june 12 we learned that 42 families had been reunited with the help of sahana this article provides an inside look at the deployment effort it describes how a diverse multidisciplinary team professional programmers software engineers executives from a large global enterprise students faculty and humanitarian it specialists from a global foss community worked together to assist the earthquake recovery effort the success of the collaboration illustrates the power of virtual communities working across international boundaries using a variety of electronic communication software it also demonstrates that the internet has truly made us all neighbors and is constantly forcing us to redefine our concept of community,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
86,0,210,"A Graph Model for False Negative Handling in Indoor RFID Tracking Data The Radio Frequency Identification (RFID) emerges to be one of the key technologies to modernize object tracking and monitoring systems in indoor environments, e.g., airport baggage tracking. Although RFID has advantages over alternative identification technologies, the raw RFID data produced is inherently uncertain and contains errors. The dirty nature of raw RFID data hinders the progress of applying meaningful high-level applications that range from querying to analyzing. Therefore, cleansing RFID data is a high necessity. In this paper, we focus on handling one of the main aspects of raw RFID data, namely, false negatives, which occurs when a moving object passes the detection range of an RFID reader but the reader fails to produce any readings. We investigate the topology of indoor spaces as well as the deployment of RFID readers, and propose the transition probabilities that capture how likely objects move from one RFID reader to another. We organize such probabilities, together with the characteristics of indoor topology and RFID readers, into a probabilistic distance-aware graph model. Further, we evaluate the effectiveness and efficiency of devised graph model in recovering the false negatives using real dataset. The experimental results show that the devised graph model is effective and efficient in handling false negatives in indoor RFID tracking data. RFID, data cleansing, indoor spaces",a graph model for false negative handling in indoor rfid tracking data the radio frequency identification rfid emerges to be one of the key technologies to modernize object tracking and monitoring systems in indoor environments e g airport baggage tracking although rfid has advantages over alternative identification technologies the raw rfid data produced is inherently uncertain and contains errors the dirty nature of raw rfid data hinders the progress of applying meaningful high level applications that range from querying to analyzing therefore cleansing rfid data is a high necessity in this paper we focus on handling one of the main aspects of raw rfid data namely false negatives which occurs when a moving object passes the detection range of an rfid reader but the reader fails to produce any readings we investigate the topology of indoor spaces as well as the deployment of rfid readers and propose the transition probabilities that capture how likely objects move from one rfid reader to another we organize such probabilities together with the characteristics of indoor topology and rfid readers into a probabilistic distance aware graph model further we evaluate the effectiveness and efficiency of devised graph model in recovering the false negatives using real dataset the experimental results show that the devised graph model is effective and efficient in handling false negatives in indoor rfid tracking data rfid data cleansing indoor spaces,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
87,0,134,"A Graphical Interface for Private Cloud and Cluster Management Management of cloud and cluster systems can be a daunting task, and becomes increasingly complicated as the organization grows and requires more computing resources. Organizations that have a need for a large amount of computing resources have two major deployment decisions: native cluster architecture or a local cloud based architecture. Regardless of the underlying architecture, the deployment process should remain universal. Many different deployment considerations are available. Automated deployment is desirable because typically these systems are headless, and thus difficult to manage individually. The tool, which we have developed allows users to deploy filesystem images on computer systems within the same local network. Additionally, the tool presents the user with node management tools for controlling the power state of the systems, as well as information such as IP addresses, MAC addresses, and power state. ACM proceedings, latex, text tagging",a graphical interface for private cloud and cluster management management of cloud and cluster systems can be a daunting task and becomes increasingly complicated as the organization grows and requires more computing resources organizations that have a need for a large amount of computing resources have two major deployment decisions native cluster architecture or a local cloud based architecture regardless of the underlying architecture the deployment process should remain universal many different deployment considerations are available automated deployment is desirable because typically these systems are headless and thus difficult to manage individually the tool which we have developed allows users to deploy filesystem images on computer systems within the same local network additionally the tool presents the user with node management tools for controlling the power state of the systems as well as information such as ip addresses mac addresses and power state acm proceedings latex text tagging,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
88,0,161,"A Hardware and Software Computational Platform for the HiPerDNO (High Performance Distribution Network Operation) Project The HiPerDNO project aims to develop new applications to enhance the operational capabilities of Distribution Network Operators (DNO). Their delivery requires an advanced computational strategy. This paper describes a High Performance Computing (HPC) platform developed for these applications whilst also being flexible enough to accommodate new ones emerging from the gradual introduction of smart metering in the Low Voltage (LV) networks (AMI: Advanced Metering Infrastructures). Security and reliability requirements for both data and computations are very stringent. Our proposed architecture would allow the deployment of computations and data access as services, thus achieving independence on the actual hardware and software technologies deployed, and hardening against malicious as well as accidental corruptions. Cost containment and reliance on proven technologies are also of paramount importance to DNOs. We suggest an architecture that fulfills these needs, which includes the following components for the HPC and Data Storage systems: Hadoop Distributed File System, a federation of loosely coupled computational clusters, the PELICAN computational application framework high performance computing applications, smart grid, systems design",a hardware and software computational platform for the hiperdno high performance distribution network operation project the hiperdno project aims to develop new applications to enhance the operational capabilities of distribution network operators dno their delivery requires an advanced computational strategy this paper describes a high performance computing hpc platform developed for these applications whilst also being flexible enough to accommodate new ones emerging from the gradual introduction of smart metering in the low voltage lv networks ami advanced metering infrastructures security and reliability requirements for both data and computations are very stringent our proposed architecture would allow the deployment of computations and data access as services thus achieving independence on the actual hardware and software technologies deployed and hardening against malicious as well as accidental corruptions cost containment and reliance on proven technologies are also of paramount importance to dnos we suggest an architecture that fulfills these needs which includes the following components for the hpc and data storage systems hadoop distributed file system a federation of loosely coupled computational clusters the pelican computational application framework high performance computing applications smart grid systems design,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
89,0,155,"A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future. Fog computing, big data analysis, distributed computing architecture, pipeline safety monitoring, smart city",a hierarchical distributed fog computing architecture for big data analysis in smart cities the ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support internet of things iot services and applications and big data analysis fog computing which extends cloud computing to the edge of network fits this need in this paper we present a hierarchical distributed fog computing architecture to support the integration of massive number of infrastructure components and services in future smart cities to secure future communities it is necessary to build large scale geospatial sensing networks perform big data analysis identify anomalous and hazardous events and offer optimal responses in real time we analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety a working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events these experimental results demonstrate the feasibility of the system s city wide implementation in the future fog computing big data analysis distributed computing architecture pipeline safety monitoring smart city,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
90,0,105,"A Hierarchical Parallel Storage System Based on Distributed Memory for Large Scale Systems This paper presents the design and implementation of a storage system for high performance systems based on a multiple level I/O caching architecture. The solution relies on Memcached as a parallel storage system, preserving its powerful capacities such as transparency, quick deployment, and scalability. The designed parallel storage system targets to reduce the I/O latency in data-intensive high performance applications. The proposed solution consists of a user-level library and extended Memcached servers. The solution aims to be hierarchical by deploying Memcached-based I/O servers across all the infrastructure data path. Our experiments demonstrate that our solution is up to 40% faster than PVFS2. distributed cache, memcached, parallel storage system",a hierarchical parallel storage system based on distributed memory for large scale systems this paper presents the design and implementation of a storage system for high performance systems based on a multiple level i o caching architecture the solution relies on memcached as a parallel storage system preserving its powerful capacities such as transparency quick deployment and scalability the designed parallel storage system targets to reduce the i o latency in data intensive high performance applications the proposed solution consists of a user level library and extended memcached servers the solution aims to be hierarchical by deploying memcached based i o servers across all the infrastructure data path our experiments demonstrate that our solution is up to 40 faster than pvfs2 distributed cache memcached parallel storage system,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
91,0,212,"A High Throughput Processing Engine for Taxi-generated Data Streams The ACM DEBS Grand Challenge 2015 focuses on real-time analytics over a high volume geospatial data stream composed of taxi trip reports from New York City. The goal of the challenge is to provide a solution which continuously identifies the most frequent routes (query 1) and most profitable areas (query 2) for taxis in New York City. The solution needs to process the incoming data stream in near real-time to provide valid information about taxi positions to end-users in a real-world deployment. We propose a modular processing engine design which is configured to offer efficient performance with a high data throughput and low processing latency. It consists of three main components: an input processor which pre-processes data objects to detect outliers, and two independent query processors tailored to the requirements of challenge queries. To efficiently compute query results, query processors use algorithms customized to the distribution of the taxi-generated data stream. Our experimental evaluation shows that the system can on average process 350,000 input events per second in a distributed mode, while achieving an average latency of less than 1 ms for both queries. Due to their excellent performance, the proposed algorithms are well suited for efficient tracking of a large number of vehicles that are present in modern urban areas. ACM DEBS grand challenge, complex event processing, smart city, trafic monitoring",a high throughput processing engine for taxi generated data streams the acm debs grand challenge 2015 focuses on real time analytics over a high volume geospatial data stream composed of taxi trip reports from new york city the goal of the challenge is to provide a solution which continuously identifies the most frequent routes query 1 and most profitable areas query 2 for taxis in new york city the solution needs to process the incoming data stream in near real time to provide valid information about taxi positions to end users in a real world deployment we propose a modular processing engine design which is configured to offer efficient performance with a high data throughput and low processing latency it consists of three main components an input processor which pre processes data objects to detect outliers and two independent query processors tailored to the requirements of challenge queries to efficiently compute query results query processors use algorithms customized to the distribution of the taxi generated data stream our experimental evaluation shows that the system can on average process 350 000 input events per second in a distributed mode while achieving an average latency of less than 1 ms for both queries due to their excellent performance the proposed algorithms are well suited for efficient tracking of a large number of vehicles that are present in modern urban areas acm debs grand challenge complex event processing smart city trafic monitoring,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
92,0,127,"A Holistic Service Provisioning Solution for Federated Cloud Infrastructures Cloud Computing builds on the latest achievements of diverse research areas, such as Grid Computing, Service-oriented computing, business process modeling and virtualization. As this new computing paradigm was mostly lead by companies, several proprietary systems arisen. Recently, alongside these commercial systems, several smaller-scale privately owned systems are maintained and developed. In this paper we present our research results performed within the S-Cube European FP7 NoE project to enable automated service provisioning for users on a highly dynamic infrastructure consisting of multiple Cloud providers. We developed a Federated Cloud Management architecture that provides unified access to a federated Cloud that aggregates multiple heterogeneous IaaS Cloud providers in a transparent manner. We have also incorporated an integrated monitoring approach that enables more reliable provider selection in these heterogeneous environments. cloud brokering, cloud computing, on-demand deployment, service monitoring",a holistic service provisioning solution for federated cloud infrastructures cloud computing builds on the latest achievements of diverse research areas such as grid computing service oriented computing business process modeling and virtualization as this new computing paradigm was mostly lead by companies several proprietary systems arisen recently alongside these commercial systems several smaller scale privately owned systems are maintained and developed in this paper we present our research results performed within the s cube european fp7 noe project to enable automated service provisioning for users on a highly dynamic infrastructure consisting of multiple cloud providers we developed a federated cloud management architecture that provides unified access to a federated cloud that aggregates multiple heterogeneous iaas cloud providers in a transparent manner we have also incorporated an integrated monitoring approach that enables more reliable provider selection in these heterogeneous environments cloud brokering cloud computing on demand deployment service monitoring,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
93,0,144,"A Knowledge-assisted Framework to Bridge Functional and Architecturally Significant Requirements The disciplines of requirements engineering (RE) and software architecture (SA) are fundamental to the success of software projects. The synergistic relationship between these two disciplines has long been acknowledged by both academicians and practitioners alike. To build successful and cost-effective software systems, we must understand and leverage the linkages between functional and architectural requirements. We discuss a knowledge-assisted approach that establishes traceability between functional and architectural requirements. The approach classifies requirements into the problem context (functional) and a solution (architectural) context. The functional context is called Functional Requirement Viewpoint (FRV). The architectural context is further categorized into three sub-contexts namely the Functional Architecture Viewpoint (FAV), the Technical Architecture Viewpoint (TAV) and the Deployment Architecture Viewpoint (DAV). Though the approach separates the problem domain and the solution domain explicitly; it facilitates development of requirements and architectural specifications concurrently; appreciating the necessary interplay between the two. Requirements Engineering, Software Engineering, Software and Systems Architecture, Twin Peaks model",a knowledge assisted framework to bridge functional and architecturally significant requirements the disciplines of requirements engineering re and software architecture sa are fundamental to the success of software projects the synergistic relationship between these two disciplines has long been acknowledged by both academicians and practitioners alike to build successful and cost effective software systems we must understand and leverage the linkages between functional and architectural requirements we discuss a knowledge assisted approach that establishes traceability between functional and architectural requirements the approach classifies requirements into the problem context functional and a solution architectural context the functional context is called functional requirement viewpoint frv the architectural context is further categorized into three sub contexts namely the functional architecture viewpoint fav the technical architecture viewpoint tav and the deployment architecture viewpoint dav though the approach separates the problem domain and the solution domain explicitly it facilitates development of requirements and architectural specifications concurrently appreciating the necessary interplay between the two requirements engineering software engineering software and systems architecture twin peaks model,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
94,0,87,"A Lightweight Rule-based Al Engine for Mobile Games The growth of the mobile gaming market offers considerable potential for the deployment of engaging and compelling games constructed using AI components and techniques. This paper discusses a rule-based approach for constructing lightweight Game AI systems for deployment on mobile devices. The development environment and the mimosa programming language for constructing Game AI components are outlined. A prototype game of Texas Hold'em Poker implemented using this environment is described. Ideas for future work, including the development of games mentors for deployment on mobile devices are briefly presented. game AI, mobile gaming, rule-based languages, tools",a lightweight rule based al engine for mobile games the growth of the mobile gaming market offers considerable potential for the deployment of engaging and compelling games constructed using ai components and techniques this paper discusses a rule based approach for constructing lightweight game ai systems for deployment on mobile devices the development environment and the mimosa programming language for constructing game ai components are outlined a prototype game of texas hold em poker implemented using this environment is described ideas for future work including the development of games mentors for deployment on mobile devices are briefly presented game ai mobile gaming rule based languages tools,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
95,0,96,"A Limited-data Model of Building Energy Consumption We present a model targeted at practical, wide-scale deployment which produces an ongoing breakdown of building energy consumption. We argue that wide-scale deployment is practical due to its reliance only on commonly available sensor information and crowd-sourced inventory data. The results for our own building over the previous 10 months show many of the trends seen in the building's true, me-tered energy consumption and we find our model predicts long term averages within 10% of the true value in some scenarios. We further use our model to estimate the potential impact of some energy saving scenarios. personal energy meter",a limited data model of building energy consumption we present a model targeted at practical wide scale deployment which produces an ongoing breakdown of building energy consumption we argue that wide scale deployment is practical due to its reliance only on commonly available sensor information and crowd sourced inventory data the results for our own building over the previous 10 months show many of the trends seen in the building s true me tered energy consumption and we find our model predicts long term averages within 10 of the true value in some scenarios we further use our model to estimate the potential impact of some energy saving scenarios personal energy meter,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
96,0,138,"A Longitudinal Study of Vibration-based Water Flow Sensing We present a long-term and cross-sectional study of a vibration-based water flow rate monitoring system in practical environments and scenarios. In our earlier research, we proved that a water flow monitoring system with vibration sensors is feasible by deploying and evaluating it in a small-scale laboratory setting. To validate the proposed system, the system was deployed in existing environments---two houses and a public restroom---and in two different laboratory test settings. With the collected data, we first demonstrate various aspects of the system's performance, including sensing stability, sensor node lifetime, the stability of autonomous sensor calibration, time to adaptation, and deployment complexity. We then discuss the practical challenges and lessons from the full-scale deployments. The evaluation results show that our water monitoring solution is a practical, quick-to-deploy system with a less than 5% average flow estimation error. Application of sensor networks, adaptive sensor calibration, nonintrusive and spatially distributed sensing, parameter estimation via numerical optimization",a longitudinal study of vibration based water flow sensing we present a long term and cross sectional study of a vibration based water flow rate monitoring system in practical environments and scenarios in our earlier research we proved that a water flow monitoring system with vibration sensors is feasible by deploying and evaluating it in a small scale laboratory setting to validate the proposed system the system was deployed in existing environments two houses and a public restroom and in two different laboratory test settings with the collected data we first demonstrate various aspects of the system s performance including sensing stability sensor node lifetime the stability of autonomous sensor calibration time to adaptation and deployment complexity we then discuss the practical challenges and lessons from the full scale deployments the evaluation results show that our water monitoring solution is a practical quick to deploy system with a less than 5 average flow estimation error application of sensor networks adaptive sensor calibration nonintrusive and spatially distributed sensing parameter estimation via numerical optimization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
97,0,84,"A Method for Analyzing Software Product Line Ecosystems The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example. software ecosystem, software product line",a method for analyzing software product line ecosystems the ecosystem for a software product line includes all of the entities with which the software product line organization interacts information artifacts customers money and products move among these entities as a part of the planning development and deployment processes in this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities the result of the analysis is data that is used to evaluate and structure the organization we illustrate with an example software ecosystem software product line,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
98,0,139,"A Method to Support Multiple Interfaces Mobile Nodes in PMIPv6 Domain In this paper, we propose a method to support handover between heterogeneous access technologies to multiple interface (MIF) mobile node (MN) in the Proxy Mobile IPv6 (PMIPv6) domain. PMIPv6 enables network-based mobility for a regular IPv6 mobile (MIPv6) with no mobility management protocol. But, there have been some issues identified with supporting a host with multiple interfaces attaching to the PMIPv6 domain. So we propose the method using NAT (Network Address Translation) in MAG (Mobile Access Gateway) which can continuously delivery packets although handover between heterogeneous access technologies occur. This paper analyzes the reduction of handover signaling cost in the proposed MAT (MAG Address Translation) method. It compares the handover signaling cost with PMIPv6 assisted by IEEE 802.21(MIH) and with the proposed method (i.e. PMIPv6-MAT) to show how much handover signaling cost reduction can be achieved. PMIPv6, handover, mobility, multiple interfaces, signaling cost",a method to support multiple interfaces mobile nodes in pmipv6 domain in this paper we propose a method to support handover between heterogeneous access technologies to multiple interface mif mobile node mn in the proxy mobile ipv6 pmipv6 domain pmipv6 enables network based mobility for a regular ipv6 mobile mipv6 with no mobility management protocol but there have been some issues identified with supporting a host with multiple interfaces attaching to the pmipv6 domain so we propose the method using nat network address translation in mag mobile access gateway which can continuously delivery packets although handover between heterogeneous access technologies occur this paper analyzes the reduction of handover signaling cost in the proposed mat mag address translation method it compares the handover signaling cost with pmipv6 assisted by ieee 802 21 mih and with the proposed method i e pmipv6 mat to show how much handover signaling cost reduction can be achieved pmipv6 handover mobility multiple interfaces signaling cost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
99,0,180,"A Middleware for Fast and Flexible Sensor Network Deployment A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN's central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from http://globalsn.sourceforge.net/. []",a middleware for fast and flexible sensor network deployment a key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task to minimize the unnecessary and repetitive implementation of identical functionalities for different platforms we present our global sensor networks gsn middleware which supports the flexible integration and discovery of sensor networks and sensor data enables fast deployment and addition of new platforms provides distributed querying filtering and combination of sensor data and supports the dynamic adaption of the system configuration during operation gsn s central concept is the virtual sensor abstraction which enables the user to declaratively specify xml based deployment descriptors in combination with the possibility to integrate sensor network data through plain sql queries over local and remote sensor data sources in this demonstration we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system to add new sensor networks on the fly and to monitor the effects of the changes via a graphical interface the gsn implementation is available from http globalsn sourceforge net,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
100,0,225,"A Middleware for Reflective Web Service Choreographies on the Cloud Web service composition is a commonly used solution to build distributed systems on the cloud. Choreographies are one specific kind of service composition in which the responsibilities for the execution of the system are shared by its service components without a central point of coordination. Due to the distributed nature of these systems, a manual approach to resource usage monitoring and allocation to maintain the expected Quality of Service (QoS) is not only inefficient but also does not scale. In this paper, we present an open source choreography enactment middleware that is capable of automatically deploying and executing a composition. Additionally, it also monitors the composition execution to perform automatic resource provisioning and dynamic service reconfiguration based on pre-defined Service Level Agreement (SLA) constraints. To achieve that, it keeps a meta-level representation of the compositions, which contains their specifications, deployment statuses, and QoS attributes. Application developers can write specific rules that take into account these meta-data to reason about the performance of the composition and change its behavior. Our middleware was evaluated on Amazon EC2 and our results demonstrate that, with little effort from the choreography developer or deployer, the middleware is able to maintain the established SLA using both horizontal and vertical scaling when faced with varying levels of load. Additionally, it also reduces operational costs by using as little computational resources as possible. QoS, SOA, middleware, reflection",a middleware for reflective web service choreographies on the cloud web service composition is a commonly used solution to build distributed systems on the cloud choreographies are one specific kind of service composition in which the responsibilities for the execution of the system are shared by its service components without a central point of coordination due to the distributed nature of these systems a manual approach to resource usage monitoring and allocation to maintain the expected quality of service qos is not only inefficient but also does not scale in this paper we present an open source choreography enactment middleware that is capable of automatically deploying and executing a composition additionally it also monitors the composition execution to perform automatic resource provisioning and dynamic service reconfiguration based on pre defined service level agreement sla constraints to achieve that it keeps a meta level representation of the compositions which contains their specifications deployment statuses and qos attributes application developers can write specific rules that take into account these meta data to reason about the performance of the composition and change its behavior our middleware was evaluated on amazon ec2 and our results demonstrate that with little effort from the choreography developer or deployer the middleware is able to maintain the established sla using both horizontal and vertical scaling when faced with varying levels of load additionally it also reduces operational costs by using as little computational resources as possible qos soa middleware reflection,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
101,0,154,"A Middleware-based Approach for QoS-aware Deployment of Service Choreography in the Cloud Non-functional requirements specification for an application and definition of required resources for its deployment are activities commonly performed at ad hoc, which can cause quality of service degradation due to inefficient use of resources. This problem is even greater with applications formed by the composition of multiple services. Cloud computing emerges as an alternative to provision of the needed resources, which is usually done using local infrastructure in conjunction with external providers. However, efficient resource management mechanisms and policies for offering quality services in these environments are necessary. This work is a PhD proposal, where is presented a middleware-based approach for cloud resources management and their provisioning according with non-funcional requirements and with greater usability by the deployer. Use of models at run-time and choreography of services for higher abstraction, adaptability and decentralization of the running system is proposed. Using this approach applications running in cloud environments may work based on their immediate needs. choreography, cloud computing, models@runtime, non-functional requirement",a middleware based approach for qos aware deployment of service choreography in the cloud non functional requirements specification for an application and definition of required resources for its deployment are activities commonly performed at ad hoc which can cause quality of service degradation due to inefficient use of resources this problem is even greater with applications formed by the composition of multiple services cloud computing emerges as an alternative to provision of the needed resources which is usually done using local infrastructure in conjunction with external providers however efficient resource management mechanisms and policies for offering quality services in these environments are necessary this work is a phd proposal where is presented a middleware based approach for cloud resources management and their provisioning according with non funcional requirements and with greater usability by the deployer use of models at run time and choreography of services for higher abstraction adaptability and decentralization of the running system is proposed using this approach applications running in cloud environments may work based on their immediate needs choreography cloud computing models runtime non functional requirement,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
102,0,206,"A Mobile Computing Middleware for Location- and Context-aware Internet Data Services The widespread diffusion of mobile computing calls for novel services capable of providing results that depend on both the current physical position of users (location) and the logical set of accessible resources, subscribed services, preferences, and requirements (context). Leaving the burden of location/context management to applications complicates service design and development. In addition, traditional middleware solutions tend to hide location/context visibility to the application level and are not suitable for supporting novel adaptive services for mobile computing scenarios. The article proposes a flexible middleware for the development and deployment of location/context-aware services for heterogeneous data access in the Internet. A primary design choice is to exploit a high-level policy framework to simplify the specification of services that the middleware dynamically adapts to the client location/context. In addition, the middleware adopts the mobile agent technology to effectively support autonomous, asynchronous, and local access to data resources, and is particularly suitable for temporarily disconnected clients. The article also presents the case study of a museum guide assistant service that provides visitors with location/context-dependent artistic data. The case study points out the flexibility and usability of the proposed middleware that permits automatic service reconfiguration with no impact on the implementation of the application logic. Mobile computing, adaptive services, context awareness, location awareness, middleware, mobile agents, policies",a mobile computing middleware for location and context aware internet data services the widespread diffusion of mobile computing calls for novel services capable of providing results that depend on both the current physical position of users location and the logical set of accessible resources subscribed services preferences and requirements context leaving the burden of location context management to applications complicates service design and development in addition traditional middleware solutions tend to hide location context visibility to the application level and are not suitable for supporting novel adaptive services for mobile computing scenarios the article proposes a flexible middleware for the development and deployment of location context aware services for heterogeneous data access in the internet a primary design choice is to exploit a high level policy framework to simplify the specification of services that the middleware dynamically adapts to the client location context in addition the middleware adopts the mobile agent technology to effectively support autonomous asynchronous and local access to data resources and is particularly suitable for temporarily disconnected clients the article also presents the case study of a museum guide assistant service that provides visitors with location context dependent artistic data the case study points out the flexibility and usability of the proposed middleware that permits automatic service reconfiguration with no impact on the implementation of the application logic mobile computing adaptive services context awareness location awareness middleware mobile agents policies,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
103,0,121,"A Mobile Crowdsensing System Enhanced by Cloud-based Social Networking Services This paper presents TripleS, a novel mobile crowdsensing system enhanced by social networking services, which enables mobile users to participate and perform mobile crowdsensing tasks in an efficient manner. TripleS provides a flexible and universal architecture across mobile devices and cloud computing platforms by integrating the service-oriented architecture with multi-agent frameworks for mobile crowdsensing, with extensive supports to application developers and end users. The customized platform of TripleS enables dynamic deployments and collaborations of services and tasks during run-time of mobile devices. Our practical experiments show that TripleS performs its tasks with a considerable computation efficiency, and low computation and communication overhead on mobile devices. Also, the mobile crowdsensing application developed on TripleS demonstrates the functionalities and practical usage of TripleS. cloud, mobile crowdsensing, social networks, system architecture",a mobile crowdsensing system enhanced by cloud based social networking services this paper presents triples a novel mobile crowdsensing system enhanced by social networking services which enables mobile users to participate and perform mobile crowdsensing tasks in an efficient manner triples provides a flexible and universal architecture across mobile devices and cloud computing platforms by integrating the service oriented architecture with multi agent frameworks for mobile crowdsensing with extensive supports to application developers and end users the customized platform of triples enables dynamic deployments and collaborations of services and tasks during run time of mobile devices our practical experiments show that triples performs its tasks with a considerable computation efficiency and low computation and communication overhead on mobile devices also the mobile crowdsensing application developed on triples demonstrates the functionalities and practical usage of triples cloud mobile crowdsensing social networks system architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
104,0,102,"A Mobile Voice Communication System in Medical Setting: Love It or Hate It? Hospital work coordination and collaboration often requires mobility for acquiring proper information and resources. In turn, the spatial distribution and the mobility of clinicians can curtail the opportunities for effective communications making collaboration difficult. In this situation, a mobile hands-free voice communication system, Vocera, was introduced to enhance communication. It supports quick and impromptu conversations among coworkers for work coordination and collaboration anytime and anywhere. We study this deployment and present our findings concerning the impact of this communication system on the information flow. Our information flow framework's communication strategies help contrast the information processes before and after the deployment of Vocera. communication strategy, healthcare, information flow, mobile, observational study, vocera, voice communication",a mobile voice communication system in medical setting love it or hate it hospital work coordination and collaboration often requires mobility for acquiring proper information and resources in turn the spatial distribution and the mobility of clinicians can curtail the opportunities for effective communications making collaboration difficult in this situation a mobile hands free voice communication system vocera was introduced to enhance communication it supports quick and impromptu conversations among coworkers for work coordination and collaboration anytime and anywhere we study this deployment and present our findings concerning the impact of this communication system on the information flow our information flow framework s communication strategies help contrast the information processes before and after the deployment of vocera communication strategy healthcare information flow mobile observational study vocera voice communication,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
105,0,45,"A Model Driven Method to Deploy Auto-scaling Configuration for Cloud Services Vendor lock-in is the issues in auto-scaling configuration; scaling configuration of a service cannot automatically transfer when the service is migrated from one cloud to another cloud. To facilitate fast service deployment, there is a need to automate the operations of auto-scaling configuration and deployment. Auto-scaling, Cloud Computing, DevOps, Model-driven",a model driven method to deploy auto scaling configuration for cloud services vendor lock in is the issues in auto scaling configuration scaling configuration of a service cannot automatically transfer when the service is migrated from one cloud to another cloud to facilitate fast service deployment there is a need to automate the operations of auto scaling configuration and deployment auto scaling cloud computing devops model driven,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
106,0,158,"A Multi-criteria Approach for Assessing Cloud Deployment Options Based on Non-functional Requirements Cloud computing is a recent computing paradigm that is changing software engineering. It offers scalable virtual compute resources at low prices, thus attracting many software developers interested in reducing their infrastructure and operational costs. Even though using cloud solutions is simple, with many providers and resource types available, a common difficulty developers face is how to best configure their applications using a myriad of cloud services, specially when considering different attributes such as cost, scalability, performance and others. A wrong architectural decision can lead to a significant cost increase or a deployment option that does not meet the minimum required performance. This work presents an approach that relies on non-functional requirements as key drivers for assessing and selecting, based on a multi-criteria optimization method, the best architectural options for deploying applications in the cloud. Results from a real application (WordPress) deployed in a popular cloud provider (Amazon) are discussed to illustrate the use and benefits of the approach. AHP, architectural analysis, cloud deployment, multi-criteria",a multi criteria approach for assessing cloud deployment options based on non functional requirements cloud computing is a recent computing paradigm that is changing software engineering it offers scalable virtual compute resources at low prices thus attracting many software developers interested in reducing their infrastructure and operational costs even though using cloud solutions is simple with many providers and resource types available a common difficulty developers face is how to best configure their applications using a myriad of cloud services specially when considering different attributes such as cost scalability performance and others a wrong architectural decision can lead to a significant cost increase or a deployment option that does not meet the minimum required performance this work presents an approach that relies on non functional requirements as key drivers for assessing and selecting based on a multi criteria optimization method the best architectural options for deploying applications in the cloud results from a real application wordpress deployed in a popular cloud provider amazon are discussed to illustrate the use and benefits of the approach ahp architectural analysis cloud deployment multi criteria,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
107,0,101,"A Multi-objective ACS Algorithm to Optimize Cost, Performance, and Reliability in the Cloud In this paper, we present a novel Multi-Objective Ant Colony System algorithm to optimize Cost, Performance, and Reliability (MOACS-CoPeR) in the cloud. The proposed algorithm provides a metaheuristic-based approach for the multi-objective cloud-based software component deployment problem. MOACS-CoPeR explores the search-space of architecture design alternatives with respect to several architectural degrees of freedom and produces a set of Pareto-optimal deployment configurations. We also present a Java-based implementation of our proposed algorithm and compare its results with the Non-dominated Sorting Genetic Algorithm II (NSGA-II). We evaluate the two algorithms against a cloud-based storage service, which is loosely based on a real system. []",a multi objective acs algorithm to optimize cost performance and reliability in the cloud in this paper we present a novel multi objective ant colony system algorithm to optimize cost performance and reliability moacs coper in the cloud the proposed algorithm provides a metaheuristic based approach for the multi objective cloud based software component deployment problem moacs coper explores the search space of architecture design alternatives with respect to several architectural degrees of freedom and produces a set of pareto optimal deployment configurations we also present a java based implementation of our proposed algorithm and compare its results with the non dominated sorting genetic algorithm ii nsga ii we evaluate the two algorithms against a cloud based storage service which is loosely based on a real system,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
108,0,142,"A Multi-site Field Study of Crowdsourced Contextual Help: Usage and Perspectives of End Users and Software Teams We present a multi-site field study to evaluate LemonAid, a crowdsourced contextual help approach that allows users to retrieve relevant questions and answers by making selections within the interface. We deployed LemonAid on 4 different web sites used by thousands of users and collected data over several weeks, gathering over 1,200 usage logs, 168 exit surveys, and 36 one-on-one interviews. Our results indicate that over 70% of users found LemonAid to be helpful, intuitive, and desirable for reuse. Software teams found LemonAid easy to integrate with their sites and found the analytics data aggregated by LemonAid a novel way of learning about users' popular questions. Our work provides the first holistic picture of the adoption and use of a crowdsourced contextual help system and offers several insights into the social and organizational dimensions of implementing such help systems for real-world applications. contextual help, crowdsourced help, field deployments, field studies, help systems, software support=.",a multi site field study of crowdsourced contextual help usage and perspectives of end users and software teams we present a multi site field study to evaluate lemonaid a crowdsourced contextual help approach that allows users to retrieve relevant questions and answers by making selections within the interface we deployed lemonaid on 4 different web sites used by thousands of users and collected data over several weeks gathering over 1 200 usage logs 168 exit surveys and 36 one on one interviews our results indicate that over 70 of users found lemonaid to be helpful intuitive and desirable for reuse software teams found lemonaid easy to integrate with their sites and found the analytics data aggregated by lemonaid a novel way of learning about users popular questions our work provides the first holistic picture of the adoption and use of a crowdsourced contextual help system and offers several insights into the social and organizational dimensions of implementing such help systems for real world applications contextual help crowdsourced help field deployments field studies help systems software support,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
109,0,156,"A Network-state Management Service We present Statesman, a network-state management service that allows multiple network management applications to operate independently, while maintaining network-wide safety and performance invariants. Network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic. Statesman uses three views of the network state. In observed state, it maintains an up-to-date view of the actual network state. Applications read this state and propose state changes based on their individual goals. Using a model of dependencies among state variables, Statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants. It then updates the network to the target state. Statesman has been deployed in ten Microsoft Azure datacenters for several months, and three distinct applications have been built on it. We use the experience from this deployment to demonstrate how Statesman enables each application to meet its goals, while maintaining network-wide invariants. datacenter network, network state, software-defined networking",a network state management service we present statesman a network state management service that allows multiple network management applications to operate independently while maintaining network wide safety and performance invariants network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic statesman uses three views of the network state in observed state it maintains an up to date view of the actual network state applications read this state and propose state changes based on their individual goals using a model of dependencies among state variables statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants it then updates the network to the target state statesman has been deployed in ten microsoft azure datacenters for several months and three distinct applications have been built on it we use the experience from this deployment to demonstrate how statesman enables each application to meet its goals while maintaining network wide invariants datacenter network network state software defined networking,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
110,0,156,"A Network-state Management Service We present Statesman, a network-state management service that allows multiple network management applications to operate independently, while maintaining network-wide safety and performance invariants. Network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic. Statesman uses three views of the network state. In observed state, it maintains an up-to-date view of the actual network state. Applications read this state and propose state changes based on their individual goals. Using a model of dependencies among state variables, Statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants. It then updates the network to the target state. Statesman has been deployed in ten Microsoft Azure datacenters for several months, and three distinct applications have been built on it. We use the experience from this deployment to demonstrate how Statesman enables each application to meet its goals, while maintaining network-wide invariants. datacenter network, network state, software-defined networking",a network state management service we present statesman a network state management service that allows multiple network management applications to operate independently while maintaining network wide safety and performance invariants network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic statesman uses three views of the network state in observed state it maintains an up to date view of the actual network state applications read this state and propose state changes based on their individual goals using a model of dependencies among state variables statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants it then updates the network to the target state statesman has been deployed in ten microsoft azure datacenters for several months and three distinct applications have been built on it we use the experience from this deployment to demonstrate how statesman enables each application to meet its goals while maintaining network wide invariants datacenter network network state software defined networking,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
111,0,329,"A New Paradigm for the Development of Future Medical Software Systems In many areas of human life, computer-based Information Technology (IT) has prevailed and become essential for the coordinated and efficient organization of workflows. Especially in the field of health care, interaction between human beings and IT is a sensitive subject. Inevitably, people working in the field of health care will have to make use of the potentials of IT in order to meet the enormous demands on e.g. patient management. When we look at the situation of current medical software systems, we can find major advances in the performance capacities of modern software systems, but must also note their rapid penetration into almost every facet of the daily hospital routine. Today medical software systems can be characterized as a vast number of (networked) software which each fulfills specific functions (e.g., hospital information system, Picture Archiving and Communication System) and which is used by different user groups (e.g., physicians, nurses, medical secretaries) with diverse user tasks [4]. Yet, for almost two decades, graphical user interfaces have dominated the interaction of medical software systems in most cases. In the future, a broader range of paradigms will emerge, allowing for multi-modal interaction. But also the growing number of heterogeneous platforms (operating systems, graphical libraries) and devices utilized complementarily (e.g., PC, Toughbook, Smartphone, PDA) demand for the development of congeneric user interfaces for a plethora of target platforms. Especially ""mobile healthcare computing devices (MHCDs) are rapidly becoming an integral part of hospital information systems. Deployment of these devices is becoming an important IT strategy designed to assist in improving quality of care, enhancing patient services, increasing productivity, lowering costs, improving cash flow, as well as facilitating other critical delivery processes"" [3]. For MHCDs aspects like getting the right information at the right time in the right place (context sensitivity) is important. To be able to cope with requirements like interoperability, context-sensitivity and device-independent usage a model-based approach for the development of user interfaces (MBUID) appears to be favourable [2]. []",a new paradigm for the development of future medical software systems in many areas of human life computer based information technology it has prevailed and become essential for the coordinated and efficient organization of workflows especially in the field of health care interaction between human beings and it is a sensitive subject inevitably people working in the field of health care will have to make use of the potentials of it in order to meet the enormous demands on e g patient management when we look at the situation of current medical software systems we can find major advances in the performance capacities of modern software systems but must also note their rapid penetration into almost every facet of the daily hospital routine today medical software systems can be characterized as a vast number of networked software which each fulfills specific functions e g hospital information system picture archiving and communication system and which is used by different user groups e g physicians nurses medical secretaries with diverse user tasks 4 yet for almost two decades graphical user interfaces have dominated the interaction of medical software systems in most cases in the future a broader range of paradigms will emerge allowing for multi modal interaction but also the growing number of heterogeneous platforms operating systems graphical libraries and devices utilized complementarily e g pc toughbook smartphone pda demand for the development of congeneric user interfaces for a plethora of target platforms especially mobile healthcare computing devices mhcds are rapidly becoming an integral part of hospital information systems deployment of these devices is becoming an important it strategy designed to assist in improving quality of care enhancing patient services increasing productivity lowering costs improving cash flow as well as facilitating other critical delivery processes 3 for mhcds aspects like getting the right information at the right time in the right place context sensitivity is important to be able to cope with requirements like interoperability context sensitivity and device independent usage a model based approach for the development of user interfaces mbuid appears to be favourable 2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
112,0,84,"A Next Generation Smart Energy Technology This paper has focused on the integration of renewable energy, specifically the solar energy resources into conventional electric grid and deployment of smart architecture of hybrid energy system in the user-centric pervasive computing concept in the context of Kyoto Protocol for sustainable development of the rural and urban sector. A concept of next generation mobile smart-grid city for efficient real-time collaborative use of renewable and non-renewable energy sources at smart user-centric device for sustainable green environment in the context of climate change is proposed Kyoto protocol, NASA surface meteorology and solar energy data on solar energy resources, SAP-SOA energy optimization model, acceptance index, energy service companies, enterprise SAP-SOA net weaver architecture, enterprise resource planning, green house gas, market potential mappings, next generation smart-grid through pervasive computing, service-oriented-architecture, systems applications products in data processing",a next generation smart energy technology this paper has focused on the integration of renewable energy specifically the solar energy resources into conventional electric grid and deployment of smart architecture of hybrid energy system in the user centric pervasive computing concept in the context of kyoto protocol for sustainable development of the rural and urban sector a concept of next generation mobile smart grid city for efficient real time collaborative use of renewable and non renewable energy sources at smart user centric device for sustainable green environment in the context of climate change is proposed kyoto protocol nasa surface meteorology and solar energy data on solar energy resources sap soa energy optimization model acceptance index energy service companies enterprise sap soa net weaver architecture enterprise resource planning green house gas market potential mappings next generation smart grid through pervasive computing service oriented architecture systems applications products in data processing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
113,0,303,"A Paradigm Comparison for Collecting TV Channel Statistics from High-volume Channel Zap Events The current approach used to obtain official television channel statistics is based on polls combined with specialized reporting hardware. These are deployed only on a small scale and batch processed every 24 hours. With the enhanced capabilities of present-day IPTV set-top-boxes, network operators can track channel popularity and usage patterns with a degree of precision and sophistication not possible with existing methods. One such network operator, Altibox, is the largest provider of IPTV in Norway with a deployment of over 320,000 set top-boxes. By tapping into the high-volume stream of channel zap events sent from these set-top boxes, very accurate viewership can be obtained and presented in near real-time. In this paper, we examine two programming paradigms for implementing applications to compute viewership based on channel zap events. One based on a general-purpose programming language (Java) and the other based on a highly specialized event stream processing language (EPL). An important characteristic of this application is stateful event processing. We are interested in exploring the trade-offs between these two implementations, to determine their suitability for such applications. Specifically, we are interested in the performance trade-off and the program complexity of each implementation. Our results show that a pure Java implementation has a significant edge over EPL in terms of performance. Although, our numbers cannot be used to draw a general conclusion, it seems indicative that an event stream processing engine would suffer more than a general-purpose language as query complexity grows. We conjecture that this is because it is easier to construct custom data structures for the specific problem in a general-purpose language like Java. In terms of program complexity, EPL has a slight edge in all metrics, and a significant edge when event streams can be reused to perform more complex processing, indicating that less effort is necessary to extend functionality. stream processing, tv viewership statistics",a paradigm comparison for collecting tv channel statistics from high volume channel zap events the current approach used to obtain official television channel statistics is based on polls combined with specialized reporting hardware these are deployed only on a small scale and batch processed every 24 hours with the enhanced capabilities of present day iptv set top boxes network operators can track channel popularity and usage patterns with a degree of precision and sophistication not possible with existing methods one such network operator altibox is the largest provider of iptv in norway with a deployment of over 320 000 set top boxes by tapping into the high volume stream of channel zap events sent from these set top boxes very accurate viewership can be obtained and presented in near real time in this paper we examine two programming paradigms for implementing applications to compute viewership based on channel zap events one based on a general purpose programming language java and the other based on a highly specialized event stream processing language epl an important characteristic of this application is stateful event processing we are interested in exploring the trade offs between these two implementations to determine their suitability for such applications specifically we are interested in the performance trade off and the program complexity of each implementation our results show that a pure java implementation has a significant edge over epl in terms of performance although our numbers cannot be used to draw a general conclusion it seems indicative that an event stream processing engine would suffer more than a general purpose language as query complexity grows we conjecture that this is because it is easier to construct custom data structures for the specific problem in a general purpose language like java in terms of program complexity epl has a slight edge in all metrics and a significant edge when event streams can be reused to perform more complex processing indicating that less effort is necessary to extend functionality stream processing tv viewership statistics,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
114,0,81,"A Pattern for Network Functions Virtualization Cloud computing has brought a large variety of services available to potential consumers. A recent type of services are the provision of network functions using virtualization. Network Functions Virtualization (NFV) is a network architecture where network node functions such as load balancers, firewalls, IDS, and accelerators are built in software and offered as services. This approach results in reduced complexity in network design, better scalability and agility, as well as faster deployment. We present here a pattern for the NFV architecture. architecture patterns, cloud computing, network functions, security patterns, telecommunications, virtualization",a pattern for network functions virtualization cloud computing has brought a large variety of services available to potential consumers a recent type of services are the provision of network functions using virtualization network functions virtualization nfv is a network architecture where network node functions such as load balancers firewalls ids and accelerators are built in software and offered as services this approach results in reduced complexity in network design better scalability and agility as well as faster deployment we present here a pattern for the nfv architecture architecture patterns cloud computing network functions security patterns telecommunications virtualization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
115,0,229,"A Pattern Language for Release and Deployment Management A release is a collection of authorized software changes that include new functionality, changed functionality, or both, that are introduced into the production environment. Agile software development results in many small releases delivered as needed, as opposed to big-bang releases that deploy large amounts of functionality at regularly determined intervals. Agile software development without a detailed standard process model encompassing the overall release management activity can result in an emergent release schedule (which is almost as bad as not having one), potential bottlenecks for resources, and problems handling genuine emergency releases. Changes to software systems must occur in a disciplined and controlled way. Otherwise lack of control over the delivered changes will lead to deteriorated system quality. Change Management is often in conflict with Agile in that Change Management requires a big, heavy, schedule, dates, content, and deployment resources specified well in advance. This is a major attraction of big-bang releases; that their deployments seem to be easier to manage. In a fast changing Agile environment the survival of the software organization may be affected. There is no reason to abandon Agile development for the sake of a disciplined and controlled release process. Our proposed pattern language captures the expertise necessary to create an orderly, well thought out, end-to-end release management process that should help deliver software releases in a disciplined and controlled way within an Agile development environment. compliance, intellectual property, patterns, release management, software maintenance, software risk",a pattern language for release and deployment management a release is a collection of authorized software changes that include new functionality changed functionality or both that are introduced into the production environment agile software development results in many small releases delivered as needed as opposed to big bang releases that deploy large amounts of functionality at regularly determined intervals agile software development without a detailed standard process model encompassing the overall release management activity can result in an emergent release schedule which is almost as bad as not having one potential bottlenecks for resources and problems handling genuine emergency releases changes to software systems must occur in a disciplined and controlled way otherwise lack of control over the delivered changes will lead to deteriorated system quality change management is often in conflict with agile in that change management requires a big heavy schedule dates content and deployment resources specified well in advance this is a major attraction of big bang releases that their deployments seem to be easier to manage in a fast changing agile environment the survival of the software organization may be affected there is no reason to abandon agile development for the sake of a disciplined and controlled release process our proposed pattern language captures the expertise necessary to create an orderly well thought out end to end release management process that should help deliver software releases in a disciplined and controlled way within an agile development environment compliance intellectual property patterns release management software maintenance software risk,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
116,0,161,"A Perspective on Scientific Cloud Computing Cloud computing has the potential for tremendous benefits, but wide scale adoption has a range of challenges that must be met. We review these challenges and how they relate to scientific computing. To achieve the portability, interoperability, and economies of scale that clouds offer, it is clear that common design principles must be widely adopted in both the user community and marketplace. To this end, we argue that a private-to-public cloud deployment trajectory will be very common, if not dominant. This trajectory can be used to define a progression of needed common practices and standards which, in turn, can be used to define deployment, development and fundamental research agendas. We then survey the cloud standards landscape and how the standards process could be driven by major stakeholders, e.g., large user groups, vendors, and governments, to achieve scientific and national objectives. We conclude with a call to action for stakeholders to actively engage in driving this process to a successful conclusion. cloud computing, deployment trajectory, standardization",a perspective on scientific cloud computing cloud computing has the potential for tremendous benefits but wide scale adoption has a range of challenges that must be met we review these challenges and how they relate to scientific computing to achieve the portability interoperability and economies of scale that clouds offer it is clear that common design principles must be widely adopted in both the user community and marketplace to this end we argue that a private to public cloud deployment trajectory will be very common if not dominant this trajectory can be used to define a progression of needed common practices and standards which in turn can be used to define deployment development and fundamental research agendas we then survey the cloud standards landscape and how the standards process could be driven by major stakeholders e g large user groups vendors and governments to achieve scientific and national objectives we conclude with a call to action for stakeholders to actively engage in driving this process to a successful conclusion cloud computing deployment trajectory standardization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
117,0,126,"A Petri Net Semantics for Web Service Choreography Many Web service standards of orchestration and choreography are designed to reduce their inherent complexity of composing Web services. Existing standards all remain at the descriptive level, without providing any formal semantics and method for verifying important properties. Web Service Choreography Interface (WSCI) describes the flow of messages exchanged by a Web service which participates in choreographed interactions with other services. Due to many advantages of Petri nets, an extended one is used to formalize WSCI in this paper. We show several nets to represent the activity, process and interface respectively. Our formal model remarkably focuses on the WSCI concept of message exchange and the context which describes the environment. This paper proposes some properties and introduces technique for checking them to ensure its correct deployment. WSCI, formal semantics, petri net, web service",a petri net semantics for web service choreography many web service standards of orchestration and choreography are designed to reduce their inherent complexity of composing web services existing standards all remain at the descriptive level without providing any formal semantics and method for verifying important properties web service choreography interface wsci describes the flow of messages exchanged by a web service which participates in choreographed interactions with other services due to many advantages of petri nets an extended one is used to formalize wsci in this paper we show several nets to represent the activity process and interface respectively our formal model remarkably focuses on the wsci concept of message exchange and the context which describes the environment this paper proposes some properties and introduces technique for checking them to ensure its correct deployment wsci formal semantics petri net web service,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
118,0,94,"A Proposal for Secure Vehicular Communications It is well established now within the vehicular Communication (VC) community the fact that security and protection of privacy are a prerequisite for the deployment of the technology. It is also understood that without the integration of strong and practical security and privacy enhancing mechanisms, VC systems could be disrupted or disabled, even by relatively unsophisticated attackers. The paper introduces the widely accepted solution known as the ""pseudonymous authentication"" approach and discusses the limitations of some of the schemes proposed. The paper introduces a new proposal that helps to solve some of these limitations. authentication, digital signatures, privacy, pseudonyms, vehicular communication (VC)",a proposal for secure vehicular communications it is well established now within the vehicular communication vc community the fact that security and protection of privacy are a prerequisite for the deployment of the technology it is also understood that without the integration of strong and practical security and privacy enhancing mechanisms vc systems could be disrupted or disabled even by relatively unsophisticated attackers the paper introduces the widely accepted solution known as the pseudonymous authentication approach and discusses the limitations of some of the schemes proposed the paper introduces a new proposal that helps to solve some of these limitations authentication digital signatures privacy pseudonyms vehicular communication vc,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
119,0,125,"A Proposed Design of Learning Objects This article presents a proposal for Learning Object Design problem-based and well-defined features in the design of the problem, graphical interface design, the pattern of software architecture, the self-assessment process and structure of metadata are geared to the learning style and deployment platforms defined in non-functional requirements. The stage engineering design software Learning Object (LO), is an extension of the analysis model and requires the support of teachers with extensive knowledge of the issues associated with interactive content. In this design model specifies the system to reach the final program and here we propose the use of techniques in the life cycle for software design, appropriate to the design of Learning Objects to the proposed integrated engineering software Learning Objects based on known problems ISDOA. learning objects, software design, software engineering, software life cycle",a proposed design of learning objects this article presents a proposal for learning object design problem based and well defined features in the design of the problem graphical interface design the pattern of software architecture the self assessment process and structure of metadata are geared to the learning style and deployment platforms defined in non functional requirements the stage engineering design software learning object lo is an extension of the analysis model and requires the support of teachers with extensive knowledge of the issues associated with interactive content in this design model specifies the system to reach the final program and here we propose the use of techniques in the life cycle for software design appropriate to the design of learning objects to the proposed integrated engineering software learning objects based on known problems isdoa learning objects software design software engineering software life cycle,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
120,0,148,"A Proposed Model for Data Warehouse User Behaviour Using Intrusion Detection System Data Warehouse (DW) systems maintain sensitive and crucial information, which is integrated from various heterogenous sources of organization. With the ever increasing deployment and usage of networks, these systems are becoming more vulnerable to malicious attacks. With the increased number of attacks, intrusion detection has become vital part of Information Security. In this paper, we have proposed a model for analyzing and detecting anomalous events based on user behavior analysis through usage patterns, user profiles and session management. After monitoring the events in the system, if any intrusion activity occurs, then alerts are issued to system administrators. Since a user profile is not necessarily fixed but rather it evolves with changing time, so a dynamic user behavior modeling is represented as a sequence of events and combination of fact and dimension tables accessed by the users. In this way, DW systems may be protected by the malicious attacks. data warehouse, information security, intrusion detection system, user behaviour",a proposed model for data warehouse user behaviour using intrusion detection system data warehouse dw systems maintain sensitive and crucial information which is integrated from various heterogenous sources of organization with the ever increasing deployment and usage of networks these systems are becoming more vulnerable to malicious attacks with the increased number of attacks intrusion detection has become vital part of information security in this paper we have proposed a model for analyzing and detecting anomalous events based on user behavior analysis through usage patterns user profiles and session management after monitoring the events in the system if any intrusion activity occurs then alerts are issued to system administrators since a user profile is not necessarily fixed but rather it evolves with changing time so a dynamic user behavior modeling is represented as a sequence of events and combination of fact and dimension tables accessed by the users in this way dw systems may be protected by the malicious attacks data warehouse information security intrusion detection system user behaviour,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
121,0,194,"A Purely Functional Approach to Packet Processing Today's rapidly evolving network ecosystem, characterized by increasing traffic volumes, service heterogeneity and mutating cyber-threats, calls for new approaches to packet processing to address key issues such as scalability, flexibility, programmability and fast deployment. To this aim, this paper explores a new direction to packet processing by pushing forward functional programming principles in the definition of a ''software defined networking'' paradigm. This result is achieved by introducing PFQ-Lang, an extensible functional language which can be used to process, analyze and forward packets captured on modern multi-queue NICs (for example, it allows to quickly develop the early stage of monitoring applications). An implementation of PFQ-Lang, embedded into high level programming languages as an eDSL (embedded Domain Specific Language) is also presented. The proposed approach allows an easy development by leveraging the intuitive functional composition and, at the same time, allows to exploit multi-queue NICs and multi-core architectures to process high-speed network traffic. Experimental results are provided to prove that the presented implementation reaches line rate performance on a 10Gb line card. To demonstrate the effectiveness and expressiveness of PFQ-Lang, the paper also presents a few use-cases ranging from forwarding, firewalling and monitoring of real traffic. pfq, software defined networking",a purely functional approach to packet processing today s rapidly evolving network ecosystem characterized by increasing traffic volumes service heterogeneity and mutating cyber threats calls for new approaches to packet processing to address key issues such as scalability flexibility programmability and fast deployment to this aim this paper explores a new direction to packet processing by pushing forward functional programming principles in the definition of a software defined networking paradigm this result is achieved by introducing pfq lang an extensible functional language which can be used to process analyze and forward packets captured on modern multi queue nics for example it allows to quickly develop the early stage of monitoring applications an implementation of pfq lang embedded into high level programming languages as an edsl embedded domain specific language is also presented the proposed approach allows an easy development by leveraging the intuitive functional composition and at the same time allows to exploit multi queue nics and multi core architectures to process high speed network traffic experimental results are provided to prove that the presented implementation reaches line rate performance on a 10gb line card to demonstrate the effectiveness and expressiveness of pfq lang the paper also presents a few use cases ranging from forwarding firewalling and monitoring of real traffic pfq software defined networking,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
122,0,95,"A Quantitative Measure for Preventive Maintenance in Software Several techniques have been developed to identify and fix defects in software before its deployment. However, the challenge is to quantify how well these techniques prevent defects from occurring in the field from a holistic perspective. Therefore, we propose a novel software quality metric called ""The Preventability Metric"" that measures the preventability of defects in software. The metric is derived from a composite quantitative evaluation of the efficiency and effectiveness of the individual preventive techniques employed on software before its deployment. It provides a confidence on how well prevention of defects is handled before deployment. preventive maintenance software metrics, quantitative techniques, software engineering, software maintenance",a quantitative measure for preventive maintenance in software several techniques have been developed to identify and fix defects in software before its deployment however the challenge is to quantify how well these techniques prevent defects from occurring in the field from a holistic perspective therefore we propose a novel software quality metric called the preventability metric that measures the preventability of defects in software the metric is derived from a composite quantitative evaluation of the efficiency and effectiveness of the individual preventive techniques employed on software before its deployment it provides a confidence on how well prevention of defects is handled before deployment preventive maintenance software metrics quantitative techniques software engineering software maintenance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
123,0,172,"A Real-Time IVR Platform for Community Radio Interactive Voice Response (IVR) platforms have been widely deployed in resource-limited settings. These systems tend to afford asynchronous push interactions, and within the context of health, provide medication reminders, descriptions of symptoms and tips on self-management. Here, we present the development of an IVR system for resource-limited settings that enables real-time, synchronous interaction. Inspired by community radio, and calls for health systems that are truly local, we developed ""Sehat ki Vaani"". Sehat ki Vaani is a real-time IVR platform that enables hosting and participation in radio chat shows on community-led topics. We deployed Sehat ki Vaani with two communities in North India on topics related to the management of Type 2 diabetes and maternal health. Our deployments highlight the potential for synchronous IVR systems to offer community connection and localised sharing of experience, while also highlighting the complexity of producing, hosting and participating in radio shows in real time through IVR. We discuss the relative strengths and weaknesses of synchronous IVR systems, and highlight lessons learnt for interaction design in this area. community, design, hci4d, ictd, india, ivr, mhealth, resource-limited settings, user experience",a real time ivr platform for community radio interactive voice response ivr platforms have been widely deployed in resource limited settings these systems tend to afford asynchronous push interactions and within the context of health provide medication reminders descriptions of symptoms and tips on self management here we present the development of an ivr system for resource limited settings that enables real time synchronous interaction inspired by community radio and calls for health systems that are truly local we developed sehat ki vaani sehat ki vaani is a real time ivr platform that enables hosting and participation in radio chat shows on community led topics we deployed sehat ki vaani with two communities in north india on topics related to the management of type 2 diabetes and maternal health our deployments highlight the potential for synchronous ivr systems to offer community connection and localised sharing of experience while also highlighting the complexity of producing hosting and participating in radio shows in real time through ivr we discuss the relative strengths and weaknesses of synchronous ivr systems and highlight lessons learnt for interaction design in this area community design hci4d ictd india ivr mhealth resource limited settings user experience,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
124,0,211,"A Reference Architecture for Providing Tools As a Service to Support Global Software Development Global Software Development (GSD) teams encounter challenges that are associated with distribution of software development activities across multiple geographic regions. The limited support for performing collaborative development and engineering activities and lack of sufficient support for maintaining and resolving dependencies and traceability across heterogeneous tools are major challenges for GSD teams. The lack of insufficient support for cross platform tools integration also makes it hard to address the stated challenges using existing paradigms that are based upon desktop and web-based solutions. The restricted ability of the organizations to have desired alignment of tools with software engineering and development processes results in administrative and managerial overhead that incur increased development cost and poor product quality. Moreover, stakeholders involved in the projects have specific constraints regarding availability and deployments of the tools. The artifacts and data produced or consumed by the tools need to be governed according to the constraints and corresponding quality of service (QoS) parameters. In this paper, we present the research agenda to leverage cloud-computing paradigm for addressing above-mentioned issues by providing a framework to select appropriate tools as well as associated services and reference architecture of the cloud-enabled middleware platform that allows on demand provisioning of software engineering Tools as a Service (TaaS) with focus on integration of tools. cloud computing, global software development (GSD), infrastructure as a service (IaaS), software as a service (SaaS), software engineering (SE), tools as a service (TaaS)",a reference architecture for providing tools as a service to support global software development global software development gsd teams encounter challenges that are associated with distribution of software development activities across multiple geographic regions the limited support for performing collaborative development and engineering activities and lack of sufficient support for maintaining and resolving dependencies and traceability across heterogeneous tools are major challenges for gsd teams the lack of insufficient support for cross platform tools integration also makes it hard to address the stated challenges using existing paradigms that are based upon desktop and web based solutions the restricted ability of the organizations to have desired alignment of tools with software engineering and development processes results in administrative and managerial overhead that incur increased development cost and poor product quality moreover stakeholders involved in the projects have specific constraints regarding availability and deployments of the tools the artifacts and data produced or consumed by the tools need to be governed according to the constraints and corresponding quality of service qos parameters in this paper we present the research agenda to leverage cloud computing paradigm for addressing above mentioned issues by providing a framework to select appropriate tools as well as associated services and reference architecture of the cloud enabled middleware platform that allows on demand provisioning of software engineering tools as a service taas with focus on integration of tools cloud computing global software development gsd infrastructure as a service iaas software as a service saas software engineering se tools as a service taas,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
125,0,138,"A Scalable Multi-datacenter Layer-2 Network Architecture Cloud today is evolving towards multi-datacenter deployment, with each datacenter serving customers in different geographical areas. The independence between datacenters, however, prohibits effective inter-datacenter resource sharing and flexible management of the infrastructure. In this paper, we propose WL2, a Software-Defined Networking (SDN) solution to an Internet-scale Layer-2 network across multiple datacenters. In WL2, a logically centralized controller handles control-plane communication and configuration in each datacenter. We achieve scalability in three ways: (1) eliminating Layer-2 broadcast by rerouting control-plane traffic to the controller; (2) introducing a layered addressing scheme for aggregate Layer-2 routing; and (3) creating an overlay abstraction on top of physical topology for fast flow setup. WL2 is fault-tolerant against controller and gateway failures. We deployed and evaluated WL2 in a 2,250-VM testbed across three datacenters. The results indicate high performance and robustness of the system. fault-tolerance, layer-2 networking, multiple datacenters, scalability, software-defined networking",a scalable multi datacenter layer 2 network architecture cloud today is evolving towards multi datacenter deployment with each datacenter serving customers in different geographical areas the independence between datacenters however prohibits effective inter datacenter resource sharing and flexible management of the infrastructure in this paper we propose wl2 a software defined networking sdn solution to an internet scale layer 2 network across multiple datacenters in wl2 a logically centralized controller handles control plane communication and configuration in each datacenter we achieve scalability in three ways 1 eliminating layer 2 broadcast by rerouting control plane traffic to the controller 2 introducing a layered addressing scheme for aggregate layer 2 routing and 3 creating an overlay abstraction on top of physical topology for fast flow setup wl2 is fault tolerant against controller and gateway failures we deployed and evaluated wl2 in a 2 250 vm testbed across three datacenters the results indicate high performance and robustness of the system fault tolerance layer 2 networking multiple datacenters scalability software defined networking,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
126,0,188,"A SciDB-based Framework for Efficient Satellite Data Storage and Query Based on Dynamic Atmospheric Event Trajectory Current research in climate informatics focuses mainly on the development of novel (machine learning, data mining, or statistical) techniques to analyze climate data (e.g. model, in-situ, or satellite) or to make prediction based on these climate data. One important component missing from this analysis workflow is data management that allows efficient and flexible data retrieval, (ease of) reproducibility, and the (ease of) techniques reuse on user-defined data subsets or other data. In this paper, we describe our preliminary investigation on the utilization of the distributed array-based database management system, SciDB, to support data-driven climate science research. We focus on modeling and generating indices that allow effective execution of various spatiotemporal queries on satellite data. Moreover, we demonstrate fast and accurate data retrieval based on user-specified trajectories from the SciDB database containing tropical cyclone trajectories and the complete ten-year QuikSCAT ocean surface wind fields satellite data. Our preliminary work indicates the feasibility of the array-based technology for multiple satellite data storage, query, and analysis. Towards this end, a successful deployment of SciDB-based data storage can facilitate the use of data from multiple satellites for climate and weather research. QuikSCAT, SciDB, arrays, index, indexing, multidimensional arrays, satellite data, scientific database, spatiotemporal",a scidb based framework for efficient satellite data storage and query based on dynamic atmospheric event trajectory current research in climate informatics focuses mainly on the development of novel machine learning data mining or statistical techniques to analyze climate data e g model in situ or satellite or to make prediction based on these climate data one important component missing from this analysis workflow is data management that allows efficient and flexible data retrieval ease of reproducibility and the ease of techniques reuse on user defined data subsets or other data in this paper we describe our preliminary investigation on the utilization of the distributed array based database management system scidb to support data driven climate science research we focus on modeling and generating indices that allow effective execution of various spatiotemporal queries on satellite data moreover we demonstrate fast and accurate data retrieval based on user specified trajectories from the scidb database containing tropical cyclone trajectories and the complete ten year quikscat ocean surface wind fields satellite data our preliminary work indicates the feasibility of the array based technology for multiple satellite data storage query and analysis towards this end a successful deployment of scidb based data storage can facilitate the use of data from multiple satellites for climate and weather research quikscat scidb arrays index indexing multidimensional arrays satellite data scientific database spatiotemporal,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
127,0,222,"A Secure Architecture for Australia's Index Based e-Health Environment This paper proposes a security architecture for the basic cross indexing systems emerging as foundational structures in current health information systems. In these systems unique identifiers are issued to healthcare providers and consumers. In most cases, such numbering schemes are national in scope and must therefore necessarily be used via an indexing system to identify records contained in pre-existing local, regional or national health information systems. Most large scale electronic health record systems envisage that such correlation between national healthcare identifiers and pre-existing identifiers will be performed by some centrally administered cross referencing, or index system. This paper is concerned with the security architecture for such indexing servers and the manner in which they interface with pre-existing health systems (including both workstations and servers). The paper proposes two required structures to achieve the goal of a national scale, and secure exchange of electronic health information, including: (a) the employment of high trust computer systems to perform an indexing function, and (b) the development and deployment of an appropriate high trust interface module, a Healthcare Interface Processor (HIP), to be integrated into the connected workstations or servers of healthcare service providers. This proposed architecture is specifically oriented toward requirements identified in the Connectivity Architecture for Australia's e-health scheme as outlined by NEHTA and the national e-health strategy released by the Australian Health Ministers. HL7, architecture of health information systems, health informatics, indexing based system for e-health regime, network security for health systems, security for health information systems, trusted system",a secure architecture for australia s index based e health environment this paper proposes a security architecture for the basic cross indexing systems emerging as foundational structures in current health information systems in these systems unique identifiers are issued to healthcare providers and consumers in most cases such numbering schemes are national in scope and must therefore necessarily be used via an indexing system to identify records contained in pre existing local regional or national health information systems most large scale electronic health record systems envisage that such correlation between national healthcare identifiers and pre existing identifiers will be performed by some centrally administered cross referencing or index system this paper is concerned with the security architecture for such indexing servers and the manner in which they interface with pre existing health systems including both workstations and servers the paper proposes two required structures to achieve the goal of a national scale and secure exchange of electronic health information including a the employment of high trust computer systems to perform an indexing function and b the development and deployment of an appropriate high trust interface module a healthcare interface processor hip to be integrated into the connected workstations or servers of healthcare service providers this proposed architecture is specifically oriented toward requirements identified in the connectivity architecture for australia s e health scheme as outlined by nehta and the national e health strategy released by the australian health ministers hl7 architecture of health information systems health informatics indexing based system for e health regime network security for health systems security for health information systems trusted system,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
128,0,122,"A Secure Ubiquitous Sensor Network with Dragon The creation of wireless sensor networks provides a capable solution for a range of ubiquitous information services. Its challenges like, data security and secrecy due to its hostile deployment nature of being flat to physical attacks with restricted source available. In order to competition the security pressure that sensor networks are exposed to, a cryptography algorithm is implemented at sensor nodes for node-to-node encryption, between nodes considering the data redundancy, energy constraint, and security requirement. In this paper, we analyze Dragon stream cipher and propose a new D-MAC secure data scheme which supports node-to-node encryption using Dragon based-privacy [3] for sensor networks. Our procedure regarded the entity verification and message authentication through the performance of authenticated encryption scheme in wireless sensor nodes. Dragon stream cipher, Telos B sensor mote, authenticated encryption, eSTREAM's project, sensor network security",a secure ubiquitous sensor network with dragon the creation of wireless sensor networks provides a capable solution for a range of ubiquitous information services its challenges like data security and secrecy due to its hostile deployment nature of being flat to physical attacks with restricted source available in order to competition the security pressure that sensor networks are exposed to a cryptography algorithm is implemented at sensor nodes for node to node encryption between nodes considering the data redundancy energy constraint and security requirement in this paper we analyze dragon stream cipher and propose a new d mac secure data scheme which supports node to node encryption using dragon based privacy 3 for sensor networks our procedure regarded the entity verification and message authentication through the performance of authenticated encryption scheme in wireless sensor nodes dragon stream cipher telos b sensor mote authenticated encryption estream s project sensor network security,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
129,0,148,"A Security Metric Based on Security Arguments Software security metrics that facilitate decision making at the enterprise design and operations levels are a topic of active research and debate. These metrics are desirable to support deployment decisions, upgrade decisions, and so on; however, no single metric or set of metrics is known to provide universally effective and appropriate measurements. Instead, engineers must choose, for each software system, what to measure, how and how much to measure, and must be able to justify the rationale for how these measurements are mapped to stakeholder security goals. An assurance argument for security (i.e., a security argument) provides comprehensive documentation of all evidence and rationales for justifying belief in a security claim about a software system. In this work, we motivate the need for security arguments to facilitate meaningful and comprehensive security metrics, and present a novel framework for assessing security arguments to generate and interpret security metrics. Assurance Case, Confidence, Security Metrics",a security metric based on security arguments software security metrics that facilitate decision making at the enterprise design and operations levels are a topic of active research and debate these metrics are desirable to support deployment decisions upgrade decisions and so on however no single metric or set of metrics is known to provide universally effective and appropriate measurements instead engineers must choose for each software system what to measure how and how much to measure and must be able to justify the rationale for how these measurements are mapped to stakeholder security goals an assurance argument for security i e a security argument provides comprehensive documentation of all evidence and rationales for justifying belief in a security claim about a software system in this work we motivate the need for security arguments to facilitate meaningful and comprehensive security metrics and present a novel framework for assessing security arguments to generate and interpret security metrics assurance case confidence security metrics,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
130,0,116,"A Self-adaptation of Software Component Structures in Ubiquitous Environments The creation of applications able to be executed in ubiquitous environments, involves a better consideration of the execution context in order to ensure service continuity. In component-based software engineering, applications are built by assembling existing components. For deploying such applications in ubiquitous environments, its components must be able to adapt themselves to the current context. To deal with this issue, we propose in this paper an approach aiming at reconfiguring the component structure to allow a flexible deployment of its services according to its use context. This adaptation focusing on the service continuity, consists of determining a structure adapted to the execution context. Then, the current structure is automatically reconfigured and the generated components are redeployed. clustering, context-awareness, deployment, restructuring, self-adaptation, software component, ubiquitous systems",a self adaptation of software component structures in ubiquitous environments the creation of applications able to be executed in ubiquitous environments involves a better consideration of the execution context in order to ensure service continuity in component based software engineering applications are built by assembling existing components for deploying such applications in ubiquitous environments its components must be able to adapt themselves to the current context to deal with this issue we propose in this paper an approach aiming at reconfiguring the component structure to allow a flexible deployment of its services according to its use context this adaptation focusing on the service continuity consists of determining a structure adapted to the execution context then the current structure is automatically reconfigured and the generated components are redeployed clustering context awareness deployment restructuring self adaptation software component ubiquitous systems,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
131,0,129,"A Self-adaptive Deployment Framework for Service-oriented Systems Deploying components of a service-oriented system in a network of machines is often a complex and labourious process. Usually the environment in which such systems are deployed is dynamic: any machine in the network may crash, network links may temporarily fail, and so on. Such events may render the system partially or completely unusable. If an event occurs, it is difficult and expensive to redeploy the system to the take the new circumstances into account. In this paper we present a self-adaptive deployment framework built on top of Disnix, a model-driven distributed deployment tool for service-oriented systems. This framework dynamically discovers machines in the network and generates a mapping of components to machines based on non-functional properties. Disnix is then invoked to automatically, reliably and efficiently redeploy the system. service-oriented systems, software deployment",a self adaptive deployment framework for service oriented systems deploying components of a service oriented system in a network of machines is often a complex and labourious process usually the environment in which such systems are deployed is dynamic any machine in the network may crash network links may temporarily fail and so on such events may render the system partially or completely unusable if an event occurs it is difficult and expensive to redeploy the system to the take the new circumstances into account in this paper we present a self adaptive deployment framework built on top of disnix a model driven distributed deployment tool for service oriented systems this framework dynamically discovers machines in the network and generates a mapping of components to machines based on non functional properties disnix is then invoked to automatically reliably and efficiently redeploy the system service oriented systems software deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
132,0,122,"A Semantic Web Application Framework for Health Systems Interoperability Relevant biomedical advances happen daily, and the medical profession relies on this evolution to deliver an improved patient care. In addition, the growing magnitude of data generated by biomedical software and hardware since the initial discovery of the human genome is remarkable in size and variety. Hence, best-of-breed software solutions are at best a couple years behind clinical practice demands. In this paper we detail an innovative Semantic Web interoperability framework, which provides developers with a complete software stack for semantic application deployment. Interoperability is the defining feature of this framework. On the one hand, new instances are able to integrate several types of distributed and heterogeneous data. On the other hand, collected data are made available through a public SPARQL endpoint. bioinformatics, healthcare interoperability, semantic exploration, semantic integration, semantic web",a semantic web application framework for health systems interoperability relevant biomedical advances happen daily and the medical profession relies on this evolution to deliver an improved patient care in addition the growing magnitude of data generated by biomedical software and hardware since the initial discovery of the human genome is remarkable in size and variety hence best of breed software solutions are at best a couple years behind clinical practice demands in this paper we detail an innovative semantic web interoperability framework which provides developers with a complete software stack for semantic application deployment interoperability is the defining feature of this framework on the one hand new instances are able to integrate several types of distributed and heterogeneous data on the other hand collected data are made available through a public sparql endpoint bioinformatics healthcare interoperability semantic exploration semantic integration semantic web,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
133,0,211,"A Service Creation Environment Based on End to End Composition of Web Services The demand for quickly delivering new applications is increasingly becoming a business imperative today. Application development is often done in an ad hoc manner, without standard frameworks or libraries, thus resulting in poor reuse of software assets. Web services have received much interest in industry due to their potential in facilitating seamless business-to-business or enterprise application integration. A web services composition tool can help automate the process, from creating business process functionality, to developing executable workflows, to deploying them on an execution environment. However, we find that the main approaches taken thus far to standardize and compose web services are piecemeal and insufficient. The business world has adopted a (distributed) programming approach in which web service instances are described using WSDL, composed into flows with a language like BPEL and invoked with the SOAP protocol. Academia has propounded the AI approach of formally representing web service capabilities in ontologies, and reasoning about their composition using goal-oriented inferencing techniques from planning. We present the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the above approaches. We describe a prototype service creation environment along with a use-case scenario, and demonstrate how it can significantly speed up the time-to-market for new services. Web services composition, planning, semantic Web",a service creation environment based on end to end composition of web services the demand for quickly delivering new applications is increasingly becoming a business imperative today application development is often done in an ad hoc manner without standard frameworks or libraries thus resulting in poor reuse of software assets web services have received much interest in industry due to their potential in facilitating seamless business to business or enterprise application integration a web services composition tool can help automate the process from creating business process functionality to developing executable workflows to deploying them on an execution environment however we find that the main approaches taken thus far to standardize and compose web services are piecemeal and insufficient the business world has adopted a distributed programming approach in which web service instances are described using wsdl composed into flows with a language like bpel and invoked with the soap protocol academia has propounded the ai approach of formally representing web service capabilities in ontologies and reasoning about their composition using goal oriented inferencing techniques from planning we present the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the above approaches we describe a prototype service creation environment along with a use case scenario and demonstrate how it can significantly speed up the time to market for new services web services composition planning semantic web,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
134,0,147,"A Service-Oriented Blueprint for COTS Integration: The Hidden Part of the Iceberg The use of commercial off-the-shelf (COTS) software can greatly reduce the development cost and effort for complex software systems. Reusing software can also improve the general quality of a system by leveraging already proven implementations. One of the limiting factors in the adoption of COTS software is the complexity of integrating it with the rest of the system under development. Often, requirements do not entirely match the functionalities available in COTS components, increasing the complexity of the glue software that needs to be written. In this paper, we present the blueprint of a service-oriented architecture that can guide the engineer both in specifying the functionalities of a complex software system and as a deployment architecture to seamlessly integrate COTS components implementing such functionalities. The COTS integration concern, typically a deployment issue, is addressed in the service architecture, and is treated as first-class citizen of the development process. []",a service oriented blueprint for cots integration the hidden part of the iceberg the use of commercial off the shelf cots software can greatly reduce the development cost and effort for complex software systems reusing software can also improve the general quality of a system by leveraging already proven implementations one of the limiting factors in the adoption of cots software is the complexity of integrating it with the rest of the system under development often requirements do not entirely match the functionalities available in cots components increasing the complexity of the glue software that needs to be written in this paper we present the blueprint of a service oriented architecture that can guide the engineer both in specifying the functionalities of a complex software system and as a deployment architecture to seamlessly integrate cots components implementing such functionalities the cots integration concern typically a deployment issue is addressed in the service architecture and is treated as first class citizen of the development process,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
135,0,127,"A Software Architecture for Physical Layer Wireless Network Emulation Despite their widespread deployment, many aspects of wireless network performance are poorly understood, and there is great room from improvement in wireless network reliability and performance. A key obstacle to understanding and improving wireless networks has been the lack of a realistic yet flexible experimental methodology. Physical layer wireless network emulation promises to achieve much of the flexibility of wireless simulators while maintaining much of the realism of real wireless networks. We have developed a software architecture that tames the complexity of physical layer wireless network emulation, and presents users with a powerful yet ease-to-use interface. We present several case studies showing how this software architecture allows complex wireless experiments to be conducted in an efficient manner while still enabling novice users to quickly run simple experiments. emulation, simulation, software architecture, wireless networks",a software architecture for physical layer wireless network emulation despite their widespread deployment many aspects of wireless network performance are poorly understood and there is great room from improvement in wireless network reliability and performance a key obstacle to understanding and improving wireless networks has been the lack of a realistic yet flexible experimental methodology physical layer wireless network emulation promises to achieve much of the flexibility of wireless simulators while maintaining much of the realism of real wireless networks we have developed a software architecture that tames the complexity of physical layer wireless network emulation and presents users with a powerful yet ease to use interface we present several case studies showing how this software architecture allows complex wireless experiments to be conducted in an efficient manner while still enabling novice users to quickly run simple experiments emulation simulation software architecture wireless networks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
136,0,149,"A Software Architecture for the Deployment of Executable Transformation Models Emergency management involves collaboration among different operators (e.g. policemen, firemen, medics) on critical and dangerous situations (e.g. fires, floods). Real-time elaborations of a large amount of information and knowledge are needed to automate control and decision making processes. In this scenario raw data are captured and processed through a chain of activities (filtering, fusion, classification) generating abstract information that is selected, diffused and presented to interested users. These activities can be turned into the interpretation of executable transformation models, abstract representations of input-output transformations which can be coded on a calculator. Executable transformation models might not be static; but they may be dynamically generated by observing and processing incoming data. From the architectural point of view the dynamic model generation implies the existence of mechanisms for the dynamic deployment of the executable transformation models. In this paper we present a software architecture aimed at providing these mechanisms. adaptivity, deployment, software architecture",a software architecture for the deployment of executable transformation models emergency management involves collaboration among different operators e g policemen firemen medics on critical and dangerous situations e g fires floods real time elaborations of a large amount of information and knowledge are needed to automate control and decision making processes in this scenario raw data are captured and processed through a chain of activities filtering fusion classification generating abstract information that is selected diffused and presented to interested users these activities can be turned into the interpretation of executable transformation models abstract representations of input output transformations which can be coded on a calculator executable transformation models might not be static but they may be dynamically generated by observing and processing incoming data from the architectural point of view the dynamic model generation implies the existence of mechanisms for the dynamic deployment of the executable transformation models in this paper we present a software architecture aimed at providing these mechanisms adaptivity deployment software architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
137,0,144,"A Software Architecture-based Framework for Highly Distributed and Data Intensive Scientific Applications Modern scientific research is increasingly conducted by virtual communities of scientists distributed around the world. The data volumes created by these communities are extremely large, and growing rapidly. The management of the resulting highly distributed, virtual data systems is a complex task, characterized by a number of formidable technical challenges, many of which are of a software engineering nature. In this paper we describe our experience over the past seven years in constructing and deploying OODT, a software framework that supports large, distributed, virtual scientific communities. We outline the key software engineering challenges that we faced, and addressed, along the way. We argue that a major contributor to the success of OODT was its explicit focus on software architecture. We describe several large-scale, real-world deployments of OODT, and the manner in which OODT helped us to address the domain-specific challenges induced by each deployment. OODT, data management, software architecture",a software architecture based framework for highly distributed and data intensive scientific applications modern scientific research is increasingly conducted by virtual communities of scientists distributed around the world the data volumes created by these communities are extremely large and growing rapidly the management of the resulting highly distributed virtual data systems is a complex task characterized by a number of formidable technical challenges many of which are of a software engineering nature in this paper we describe our experience over the past seven years in constructing and deploying oodt a software framework that supports large distributed virtual scientific communities we outline the key software engineering challenges that we faced and addressed along the way we argue that a major contributor to the success of oodt was its explicit focus on software architecture we describe several large scale real world deployments of oodt and the manner in which oodt helped us to address the domain specific challenges induced by each deployment oodt data management software architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
138,0,90,"A Software Defined Approach to Unified IPv6 Transition The IPv6 transition has been an ongoing process throughout the world due to the exhaustion of the IPv4 address space. However, this transition leads to costly end-to-end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols. Recognizing these difficulties, we propose an software defined approach to unifying the deployment of IPv6 in a cost-effective, flexible manner. Our deployment and experiments demonstrate significant benefits of this approach, including low complexity, low cost and high flexibility of adopting different existing transition mechanisms. ipv6 transition, software defined network",a software defined approach to unified ipv6 transition the ipv6 transition has been an ongoing process throughout the world due to the exhaustion of the ipv4 address space however this transition leads to costly end to end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols recognizing these difficulties we propose an software defined approach to unifying the deployment of ipv6 in a cost effective flexible manner our deployment and experiments demonstrate significant benefits of this approach including low complexity low cost and high flexibility of adopting different existing transition mechanisms ipv6 transition software defined network,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
139,0,90,"A Software Defined Approach to Unified IPv6 Transition The IPv6 transition has been an ongoing process throughout the world due to the exhaustion of the IPv4 address space. However, this transition leads to costly end-to-end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols. Recognizing these difficulties, we propose an software defined approach to unifying the deployment of IPv6 in a cost-effective, flexible manner. Our deployment and experiments demonstrate significant benefits of this approach, including low complexity, low cost and high flexibility of adopting different existing transition mechanisms. ipv6 transition, software defined network",a software defined approach to unified ipv6 transition the ipv6 transition has been an ongoing process throughout the world due to the exhaustion of the ipv4 address space however this transition leads to costly end to end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols recognizing these difficulties we propose an software defined approach to unifying the deployment of ipv6 in a cost effective flexible manner our deployment and experiments demonstrate significant benefits of this approach including low complexity low cost and high flexibility of adopting different existing transition mechanisms ipv6 transition software defined network,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
140,0,127,"A Software System for Buzz-based Recommendations In this paper, we present an outline of a software system for buzz-based recommendations. This system is based on a large source of queries in an eCommerce application. The buzz events are detected based on query bursts linked to external entities like news and inventory information. A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood. The system follows the paradigm of limited quantity merchandizing, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity and promoting user activity and stickiness. The system demonstrates the deployment of an interesting application based on KDD principles applied to a high volume industrial context. large-scale buzz detection, semantic relatedness, surprise a day",a software system for buzz based recommendations in this paper we present an outline of a software system for buzz based recommendations this system is based on a large source of queries in an ecommerce application the buzz events are detected based on query bursts linked to external entities like news and inventory information a semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood the system follows the paradigm of limited quantity merchandizing in the sense that on a per day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity and promoting user activity and stickiness the system demonstrates the deployment of an interesting application based on kdd principles applied to a high volume industrial context large scale buzz detection semantic relatedness surprise a day,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
141,0,231,"A Speech Mashup Framework for Multimodal Mobile Services Amid today's proliferation of Web content and mobile phones with broadband data access, interacting with small-form factor devices is still cumbersome. Spoken interaction could overcome the input limitations of mobile devices, but running an automatic speech recognizer with the limited computational capabilities of a mobile device becomes an impossible challenge when large vocabularies for speech recognition must often be updated with dynamic content. One popular option is to move the speech processing resources into the network by concentrating the heavy computation load onto server farms. Although successful services have exploited this approach, it is unclear how such a model can be generalized to a large range of mobile applications and how to scale it for large deployments. To address these challenges we introduce the AT&T speech mashup architecture, a novel approach to speech services that leverages web services and cloud computing to make it easier to combine web content and speech processing. We show that this new compositional method is suitable for integrating automatic speech recognition and text-to-speech synthesis resources into real multimodal mobile services. The generality of this method allows researchers and speech practitioners to explore a countless variety of mobile multimodal services with a finer grain of control and richer multimedia interfaces. Moreover, we demonstrate that the speech mashup is scalable and particularly optimized to minimize round trips in the mobile network, reducing latency for better user experience. mashups, multimodal, speech mashup, speech services, speech system architecture, web services",a speech mashup framework for multimodal mobile services amid today s proliferation of web content and mobile phones with broadband data access interacting with small form factor devices is still cumbersome spoken interaction could overcome the input limitations of mobile devices but running an automatic speech recognizer with the limited computational capabilities of a mobile device becomes an impossible challenge when large vocabularies for speech recognition must often be updated with dynamic content one popular option is to move the speech processing resources into the network by concentrating the heavy computation load onto server farms although successful services have exploited this approach it is unclear how such a model can be generalized to a large range of mobile applications and how to scale it for large deployments to address these challenges we introduce the at t speech mashup architecture a novel approach to speech services that leverages web services and cloud computing to make it easier to combine web content and speech processing we show that this new compositional method is suitable for integrating automatic speech recognition and text to speech synthesis resources into real multimodal mobile services the generality of this method allows researchers and speech practitioners to explore a countless variety of mobile multimodal services with a finer grain of control and richer multimedia interfaces moreover we demonstrate that the speech mashup is scalable and particularly optimized to minimize round trips in the mobile network reducing latency for better user experience mashups multimodal speech mashup speech services speech system architecture web services,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
142,0,132,"A Study of Network Stack Latency for Game Servers Latency has a high impact on the user satisfaction in real-time multiplayer games. The network between the client and server is, in most cases, the main cause for latency and out of the scope of the game's developer. But the developer should avoid introducing unnecessary delays within his responsibility; i.e. with respect to development and operating costs he should wisely choose a network stack and deployment model for the server in order to reduce latency. In this paper we present latency measurements of the Linux network stack and effects of virtualization. We show that a worst-case scenario can cause a latency in the millisecond range when running a game server. We discuss how latencies can be reduced by using the packet processing framework DPDK to replace the operating system's network stack. intel DPDK, latency, measurement, virtualization",a study of network stack latency for game servers latency has a high impact on the user satisfaction in real time multiplayer games the network between the client and server is in most cases the main cause for latency and out of the scope of the game s developer but the developer should avoid introducing unnecessary delays within his responsibility i e with respect to development and operating costs he should wisely choose a network stack and deployment model for the server in order to reduce latency in this paper we present latency measurements of the linux network stack and effects of virtualization we show that a worst case scenario can cause a latency in the millisecond range when running a game server we discuss how latencies can be reduced by using the packet processing framework dpdk to replace the operating system s network stack intel dpdk latency measurement virtualization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
143,0,183,"A System for Intrusion Prediction in Cloud Computing Modern critical infrastructures have to process significantly large data sets. Intrusion-detection systems and unified threat management systems have the role of keeping critical infrastructures secure against cyber-attacks. However, in the world of big data, these systems are struggling to cope with overload and often become the bottle neck in the data network. To overcome this, our research investigates the use of deploying intrusion-detection and intrusion-prediction techniques in a cloud environment. Consequently, in this paper, a survey of existing intrusion-detection systems is presented and a discussion on how their deployment can enhance current security techniques in a cloud computing environment is put forward. A novel technique for intrusion prediction system is also put forward in this paper. Predictive statistical methods are used for proving the concepts put forward. The initial results show the necessity for using evolving statistical methods in prediction solutions; and the insufficiency of 'single-technique models' for building general solutions to predict intrusions. Furthermore, as this research shows, the concept of integrating multiple methods, such as game theory concepts and risk assessment methods, facilitates the development of a more efficient prediction model. Cyber-Attack, Intrusion Prediction, Security, cloud computing",a system for intrusion prediction in cloud computing modern critical infrastructures have to process significantly large data sets intrusion detection systems and unified threat management systems have the role of keeping critical infrastructures secure against cyber attacks however in the world of big data these systems are struggling to cope with overload and often become the bottle neck in the data network to overcome this our research investigates the use of deploying intrusion detection and intrusion prediction techniques in a cloud environment consequently in this paper a survey of existing intrusion detection systems is presented and a discussion on how their deployment can enhance current security techniques in a cloud computing environment is put forward a novel technique for intrusion prediction system is also put forward in this paper predictive statistical methods are used for proving the concepts put forward the initial results show the necessity for using evolving statistical methods in prediction solutions and the insufficiency of single technique models for building general solutions to predict intrusions furthermore as this research shows the concept of integrating multiple methods such as game theory concepts and risk assessment methods facilitates the development of a more efficient prediction model cyber attack intrusion prediction security cloud computing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
144,0,16,"A Systematic Approach in Managing Post-deployment System Changes In current IT practices, the task of managing post-deployment system changes often falls in no-man's land. []",a systematic approach in managing post deployment system changes in current it practices the task of managing post deployment system changes often falls in no man s land,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
145,0,229,"A Systematic Literature Review of Traceability Approaches Between Software Architecture and Source Code The links between the software architecture and the source code of a software system should be based on solid traceability mechanisms in order to effectively perform quality control and maintenance of the software system. There are several primary studies on traceability between software architecture and source code but so far no systematic literature review (SLR) has been undertaken. This study presents an SLR which has been carried out to discover the existing traceability approaches and tools between software architecture and source code, as well as the empirical evidence for these approaches, their benefits and liabilities, their relations to software architecture understanding, and issues, barriers, and challenges of the approaches. In our SLR the ACM Guide to Computing Literature has been electronically searched to accumulate the biggest share of relevant scientific bibliographic citations from the major publishers in computing. The search strategy identified 742 citations, out of which 11 have been included in our study, dated from 1999 to July, 2013, after applying our inclusion and exclusion criteria. Our SLR resulted in the identification of the current state-of-the-art of traceability approaches and tools between software architecture and source code, as well as gaps and pointers for further research. Moreover, the classification scheme developed in this paper can serve as a guide for researchers and practitioners to find a specific approach or set of approaches that is of interest to them. software architecture, source code, systematic literature review, traceability",a systematic literature review of traceability approaches between software architecture and source code the links between the software architecture and the source code of a software system should be based on solid traceability mechanisms in order to effectively perform quality control and maintenance of the software system there are several primary studies on traceability between software architecture and source code but so far no systematic literature review slr has been undertaken this study presents an slr which has been carried out to discover the existing traceability approaches and tools between software architecture and source code as well as the empirical evidence for these approaches their benefits and liabilities their relations to software architecture understanding and issues barriers and challenges of the approaches in our slr the acm guide to computing literature has been electronically searched to accumulate the biggest share of relevant scientific bibliographic citations from the major publishers in computing the search strategy identified 742 citations out of which 11 have been included in our study dated from 1999 to july 2013 after applying our inclusion and exclusion criteria our slr resulted in the identification of the current state of the art of traceability approaches and tools between software architecture and source code as well as gaps and pointers for further research moreover the classification scheme developed in this paper can serve as a guide for researchers and practitioners to find a specific approach or set of approaches that is of interest to them software architecture source code systematic literature review traceability,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
146,0,184,"End to End Automation on Cloud with Build Pipeline: The Case for DevOps in Insurance Industry, Continuous Integration, Continuous Testing, and Continuous Delivery In modern environment, delivering innovative idea in a fast and reliable manner is extremely significant for any organizations. In the existing scenario, Insurance industry need to better respond to dynamic market requirements, faster time to market for new initiatives and services, and support innovative ways of customer interaction. In past few years, the transition to cloud platforms has given benefits such as agility, scalability, and lower capital costs but the application lifecycle management practices are slow with this disruptive change. DevOps culture extends the agile methodology to rapidly create applications and deliver them across environment in automated manner to improve performance and quality assurance. Continuous Integration (CI) and Continuous delivery (CD) has emerged as a boon for traditional application development and release management practices to provide the capability to release quality artifacts continuously to customers with continuously integrated feedback. The objective of the paper is to create a proof of concept for designing an effective framework for continuous integration, continuous testing, and continuous delivery to automate the source code compilation, code analysis, test execution, packaging, infrastructure provisioning, deployment, and notifications using build pipeline concept. cloud computing, insurance data processing, program diagnostics, program testing, software prototyping, source code (software), build pipeline, software deployment, agile methodology, cloud computing, notifications, infrastructure provisioning, packaging, test execution, code analysis, source code compilation, continuous delivery, continuous testing, continuous integration, insurance industry, DevOps, end to end automation, Cloud computing, Pipelines, Automation, Servers, Testing, Configuration management, Insurance, Cloud Computing, DevOps, Continuous Integration, Continuous Testing, Configuration Management, Continuous Delivery, Automation",end to end automation on cloud with build pipeline the case for devops in insurance industry continuous integration continuous testing and continuous delivery in modern environment delivering innovative idea in a fast and reliable manner is extremely significant for any organizations in the existing scenario insurance industry need to better respond to dynamic market requirements faster time to market for new initiatives and services and support innovative ways of customer interaction in past few years the transition to cloud platforms has given benefits such as agility scalability and lower capital costs but the application lifecycle management practices are slow with this disruptive change devops culture extends the agile methodology to rapidly create applications and deliver them across environment in automated manner to improve performance and quality assurance continuous integration ci and continuous delivery cd has emerged as a boon for traditional application development and release management practices to provide the capability to release quality artifacts continuously to customers with continuously integrated feedback the objective of the paper is to create a proof of concept for designing an effective framework for continuous integration continuous testing and continuous delivery to automate the source code compilation code analysis test execution packaging infrastructure provisioning deployment and notifications using build pipeline concept cloud computing insurance data processing program diagnostics program testing software prototyping source code software build pipeline software deployment agile methodology cloud computing notifications infrastructure provisioning packaging test execution code analysis source code compilation continuous delivery continuous testing continuous integration insurance industry devops end to end automation cloud computing pipelines automation servers testing configuration management insurance cloud computing devops continuous integration continuous testing configuration management continuous delivery automation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
147,0,181,"Understanding DevOps bridging the gap from continuous integration to continuous delivery As part of Agile transformation in past few years we have seen IT organizations adopting continuous integration principles in their software delivery lifecycle, which has improved the efficiency of development teams. With the time it has been realized that this optimization as part of continuous integration - alone - is just not helping to make the entire delivery lifecycle efficient or is not driving the organization efficiency. Unless all the pieces of a software delivery lifecycle work like a well oiled machine - efficiency of organization to optimize the delivery lifecycle can not be met. This is the problem which DevOps tries to address. This paper tries to cover all aspects of Devops applicable to various phases of SDLC and specifically talks about business need, ways to move from continuous integration to continuous delivery and its benefits. Continuous delivery transformation in this paper is explained with a real life case study that how infrastructure can be maintained just in form of code (IAAC). Finally this paper touches upon various considerations one must evaluate before adopting DevOps and what kind of benefits one can expect. integrated software, software prototyping, DevOps, continuous integration, continuous delivery, agile transformation, software delivery lifecycle, SDLC, IAAC, Software, Organizations, Testing, Automation, Optimization, Production, DevOps, Continuous Integration, Continuous Delivery, Infrastructure as a Code (IAAC)",understanding devops bridging the gap from continuous integration to continuous delivery as part of agile transformation in past few years we have seen it organizations adopting continuous integration principles in their software delivery lifecycle which has improved the efficiency of development teams with the time it has been realized that this optimization as part of continuous integration alone is just not helping to make the entire delivery lifecycle efficient or is not driving the organization efficiency unless all the pieces of a software delivery lifecycle work like a well oiled machine efficiency of organization to optimize the delivery lifecycle can not be met this is the problem which devops tries to address this paper tries to cover all aspects of devops applicable to various phases of sdlc and specifically talks about business need ways to move from continuous integration to continuous delivery and its benefits continuous delivery transformation in this paper is explained with a real life case study that how infrastructure can be maintained just in form of code iaac finally this paper touches upon various considerations one must evaluate before adopting devops and what kind of benefits one can expect integrated software software prototyping devops continuous integration continuous delivery agile transformation software delivery lifecycle sdlc iaac software organizations testing automation optimization production devops continuous integration continuous delivery infrastructure as a code iaac,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
148,0,60,"An experimental continuous delivery framework for SmartX-mini IoT-cloud playground Internet of Things (IoT), one of the hottest research targets, needs the systematic leverage from cloud-centric ICT infrastructure. Among several challenges for fast and economic IoT-Cloud service enablement, the continuous integration and delivery (CI/CD) framework is one of commonly wanted capabilities. In this paper, we introduce early experience with deploying an experimental continuous delivery framework for SmartX-mini IoT-Cloud Playground. cloud computing, Internet of Things, experimental continuous delivery framework, SmartX-mini IoT-cloud playground, Internet-of-Things, cloud-centric ICT infrastructure, information and communication technology, continuous integration and delivery framework, CI CD framework, IoT-Cloud service enablement, Continuous integration and delivery, software-defined infrastructure, SDN NFV Cloud integration, IoT-Cloud software framework, Internet of Things",an experimental continuous delivery framework for smartx mini iot cloud playground internet of things iot one of the hottest research targets needs the systematic leverage from cloud centric ict infrastructure among several challenges for fast and economic iot cloud service enablement the continuous integration and delivery ci cd framework is one of commonly wanted capabilities in this paper we introduce early experience with deploying an experimental continuous delivery framework for smartx mini iot cloud playground cloud computing internet of things experimental continuous delivery framework smartx mini iot cloud playground internet of things cloud centric ict infrastructure information and communication technology continuous integration and delivery framework ci cd framework iot cloud service enablement continuous integration and delivery software defined infrastructure sdn nfv cloud integration iot cloud software framework internet of things,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
149,0,163,"Towards the Automated Fast Deployment and Clone of Private Cloud Service: The Ezilla Toolkit With the growth of Cloud based technologies, so have the cloud based services grown over the past few years. To meet the demands from the Cloud users, service providers have to support such a service environment that can be deployed and cloned automatically and easily. To help tackling such an issue, the Ezilla, which is considered as a private Cloud toolkit, has been developed. The new approach is automatic converting physical machines into virtual machines (P2V) in Cloud environment. It integrates the de facto Cloud middleware, and coordinated cloud infrastructure services such as storage, computing and networking services to form an integrated virtual computer. The merit of the Ezilla is simplifying the deployment complexity of the Cloud system for the users. The proposed Ezilla toolkit leverages genetic virtualization techniques and cluster management toolkits, such as Diskless Remote Boot in Linux - Single System Image (DRBL-SSI), and the clone OS toolkit - Clonezilla, to orchestrate distributed computing resources and convert physical machine to virtual one dynamically. cloud computing, middleware, software tools, virtual machines, distributed computing resources, Clonezilla, clone OS toolkit, DRBL-SSI, Linux-single system image, diskless remote boot, cluster management toolkits, genetic virtualization techniques, integrated virtual computer, coordinated cloud infrastructure services, de facto cloud middleware, cloud environment, automatic converting physical machines, P2V, virtual machines, cloud users, service providers, cloud based services, cloud based technology, Ezilla toolkit, private cloud service, automated fast deployment, Virtual machining, Servers, Cloud computing, Virtualization, Cloning, File systems, P2V, Cloud Middleware, Virtualization Techniques, DRBL, Clonezilla",towards the automated fast deployment and clone of private cloud service the ezilla toolkit with the growth of cloud based technologies so have the cloud based services grown over the past few years to meet the demands from the cloud users service providers have to support such a service environment that can be deployed and cloned automatically and easily to help tackling such an issue the ezilla which is considered as a private cloud toolkit has been developed the new approach is automatic converting physical machines into virtual machines p2v in cloud environment it integrates the de facto cloud middleware and coordinated cloud infrastructure services such as storage computing and networking services to form an integrated virtual computer the merit of the ezilla is simplifying the deployment complexity of the cloud system for the users the proposed ezilla toolkit leverages genetic virtualization techniques and cluster management toolkits such as diskless remote boot in linux single system image drbl ssi and the clone os toolkit clonezilla to orchestrate distributed computing resources and convert physical machine to virtual one dynamically cloud computing middleware software tools virtual machines distributed computing resources clonezilla clone os toolkit drbl ssi linux single system image diskless remote boot cluster management toolkits genetic virtualization techniques integrated virtual computer coordinated cloud infrastructure services de facto cloud middleware cloud environment automatic converting physical machines p2v virtual machines cloud users service providers cloud based services cloud based technology ezilla toolkit private cloud service automated fast deployment virtual machining servers cloud computing virtualization cloning file systems p2v cloud middleware virtualization techniques drbl clonezilla,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
150,0,189,"On-Demand Automated Fast Deployment and Coordinated Cloud Services To meet the demand from the Cloud users, service providers have to provide such a service environment that can be deployed and provisioned quickly and easily. To help tackle such an issue, the Ezilla which is considered as a private Cloud toolkit, has been developed by the Per Comp Team in the National Center for High-performance Computing. The Ezilla integrates the de facto Cloud middleware, Web-based interface, and coordinated cloud infrastructure services such as storage, computing and networking services to form an integrated virtual computer. With the Ezilla toolkit, a virtual computer, which is configured specifically to meet the needs of an individual Cloud user is just one click away. The merit of the Ezilla is that it can simplify the complexity of the Cloud system for users to deploy more easily. Scientist as well as genera users can deploy the Cloud for their work with no need to deal with the painful process of building the Cloud system. The proposed Ezilla toolkit leverages genetic virtualization techniques, the clone OS toolkit - Clonezilla, and distributed file system, to orchestrate distributed computing resources and convert physical machine to virtual one dynamically. cloud computing, distributed processing, middleware, storage management, virtual machines, ondemand automated fast deployment, coordinated cloud services, Ezilla toolkit, private Cloud toolkit, Cloud middleware, Web-based interface, coordinated cloud infrastructure services, integrated virtual computer, genetic virtualization technique, clone OS toolkit, Clonezilla, distributed file system, distributed computing resources, Virtual machining, File systems, Cloud computing, Writing, Computers, Monitoring, Virtualization, Virtualization Techniques, Virtual Cluster, Clonezilla, Distributed File system",on demand automated fast deployment and coordinated cloud services to meet the demand from the cloud users service providers have to provide such a service environment that can be deployed and provisioned quickly and easily to help tackle such an issue the ezilla which is considered as a private cloud toolkit has been developed by the per comp team in the national center for high performance computing the ezilla integrates the de facto cloud middleware web based interface and coordinated cloud infrastructure services such as storage computing and networking services to form an integrated virtual computer with the ezilla toolkit a virtual computer which is configured specifically to meet the needs of an individual cloud user is just one click away the merit of the ezilla is that it can simplify the complexity of the cloud system for users to deploy more easily scientist as well as genera users can deploy the cloud for their work with no need to deal with the painful process of building the cloud system the proposed ezilla toolkit leverages genetic virtualization techniques the clone os toolkit clonezilla and distributed file system to orchestrate distributed computing resources and convert physical machine to virtual one dynamically cloud computing distributed processing middleware storage management virtual machines ondemand automated fast deployment coordinated cloud services ezilla toolkit private cloud toolkit cloud middleware web based interface coordinated cloud infrastructure services integrated virtual computer genetic virtualization technique clone os toolkit clonezilla distributed file system distributed computing resources virtual machining file systems cloud computing writing computers monitoring virtualization virtualization techniques virtual cluster clonezilla distributed file system,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
151,0,126,"Towards Architecting for Continuous Delivery Continuous Delivery (CD) has emerged as an auspicious software development discipline, with the promise of providing organizations the capability to release valuable software continuously to customers. Our organization has been implementing CD for the last two years. Thus far, we have moved 22 software applications to CD. I observed that CD has created a new context for architecting these applications. In this paper, I will try to characterize such a context of CD, explain why we need to architect for CD, describe the implications of architecting for CD, and discuss the challenges this new context creates. This information can provide insights to other practitioners for architecting their software applications, and provide researchers with input for developing their research agendas to further study this increasingly important topic. software architecture, continuous delivery, CD, auspicious software development discipline, software architecture, Software, Context, Computer architecture, Monitoring, Pipelines, Companies, Software reliability, software architecture, continuous delivery, continuous deployment, continuous software engineering, quality attributes, architecturally significant requirements, non-functional requirements, DevOps",towards architecting for continuous delivery continuous delivery cd has emerged as an auspicious software development discipline with the promise of providing organizations the capability to release valuable software continuously to customers our organization has been implementing cd for the last two years thus far we have moved 22 software applications to cd i observed that cd has created a new context for architecting these applications in this paper i will try to characterize such a context of cd explain why we need to architect for cd describe the implications of architecting for cd and discuss the challenges this new context creates this information can provide insights to other practitioners for architecting their software applications and provide researchers with input for developing their research agendas to further study this increasingly important topic software architecture continuous delivery cd auspicious software development discipline software architecture software context computer architecture monitoring pipelines companies software reliability software architecture continuous delivery continuous deployment continuous software engineering quality attributes architecturally significant requirements non functional requirements devops,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
152,0,79,"Continuous Delivery: Overcoming Adoption Obstacles Continuous Delivery (CD) can bring huge benefits, but implementing CD is challenging. For some challenges, one can only see them when he/she travels on the journey far enough. Paddy Power has been implementing CD for more than three years. In this talk, I will present the major obstacles we encountered and how we addressed them. These obstacles cover various areas, including organizational, cultural, process, and technical. I will also discuss the areas where I see researchers can help. software engineering, adoption obstacles, continuous delivery, CD, Paddy Power, Software, Companies, Conferences, Cultural differences, Software engineering, Software reliability, Continuous Delivery, Continuous Deployment, Continuous Software Engineering, Agile Software Development",continuous delivery overcoming adoption obstacles continuous delivery cd can bring huge benefits but implementing cd is challenging for some challenges one can only see them when he she travels on the journey far enough paddy power has been implementing cd for more than three years in this talk i will present the major obstacles we encountered and how we addressed them these obstacles cover various areas including organizational cultural process and technical i will also discuss the areas where i see researchers can help software engineering adoption obstacles continuous delivery cd paddy power software companies conferences cultural differences software engineering software reliability continuous delivery continuous deployment continuous software engineering agile software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
153,0,40,"Research Opportunities in Continuous Delivery: Reflections from Two Years' Experiences in a Large Bookmaking Company We have been implementing continuous delivery in Paddy Power, a large organization in the bookmaking industry, for more than two years. In this talk, I will reflect on our journey to continuous delivery and discuss the research opportunities I see. organisational aspects, software prototyping, release engineering, continuous software engineering, DevOps, agile software development, bookmaking industry, Paddy Power, bookmaking company, continuous delivery, research opportunities, Companies, Software, Software engineering, Conferences, Industries, Reflection, Continuous delivery, continuous deployment, release engineering, continuous software engineering, DevOps, agile software development",research opportunities in continuous delivery reflections from two years experiences in a large bookmaking company we have been implementing continuous delivery in paddy power a large organization in the bookmaking industry for more than two years in this talk i will reflect on our journey to continuous delivery and discuss the research opportunities i see organisational aspects software prototyping release engineering continuous software engineering devops agile software development bookmaking industry paddy power bookmaking company continuous delivery research opportunities companies software software engineering conferences industries reflection continuous delivery continuous deployment release engineering continuous software engineering devops agile software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
154,0,85,"System Dynamics Modeling of Agile Continuous Delivery Process The popularization of agile development as well as the recent prevalence of virtualization and cloud computing has revolutionized the software delivery process- making it faster and affordable for businesses to release their software continuously. Hence, the need for a reliable and predictable delivery process for software applications. The aim of this paper is to develop a System Dynamics (SD) model to achieve a repetitive, risk-free and effortless Continuous Delivery process to reduce the perils of delayed delivery, delivery cost overrun and poor quality delivered software. cloud computing, software prototyping, software quality, virtualisation, agile development popularization, virtualization, cloud computing, software delivery process making, agile continuous delivery process, system dynamic modeling, SD model, repetitive risk-free effortless continuous delivery process, software delivery cost overrun, delayed delivery, Software, Production, Automation, Computational modeling, Pipelines, Educational institutions, Context, Agile software development, Continuous Delivery, Delivery Pipeline, System Dynamics",system dynamics modeling of agile continuous delivery process the popularization of agile development as well as the recent prevalence of virtualization and cloud computing has revolutionized the software delivery process making it faster and affordable for businesses to release their software continuously hence the need for a reliable and predictable delivery process for software applications the aim of this paper is to develop a system dynamics sd model to achieve a repetitive risk free and effortless continuous delivery process to reduce the perils of delayed delivery delivery cost overrun and poor quality delivered software cloud computing software prototyping software quality virtualisation agile development popularization virtualization cloud computing software delivery process making agile continuous delivery process system dynamic modeling sd model repetitive risk free effortless continuous delivery process software delivery cost overrun delayed delivery software production automation computational modeling pipelines educational institutions context agile software development continuous delivery delivery pipeline system dynamics,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
155,0,176,"Lessons Learned: Using a Static Analysis Tool within a Continuous Integration System Static analysis tools are used for improving software quality and reliability. Since these tools can be time consuming when used for analysis of big codebases, they are normally run during scheduled (e.g. nightly) builds. However, the sooner a defect is found, the easier it is to fix efficiently. In order to detect defects faster, some analysis tools offer an integration with the integrated development environment of the developers at the cost of not always detecting all the issues. To detect defects earlier and still provide a reliable solution, one could think of running an analysis tool at every build of a continuous integration system. In this paper, we share the lessons learned during the integration of the static analysis tool Klocwork (that we are developing) with our continuous integration system. We think that the lessons learned will be beneficial for most companies developing safety-critical software (or less critical systems) that wish to run their analysis tool more often in their build system. We report these lessons learned along with examples of our successes and failures. integrated software, program diagnostics, safety-critical software, software quality, static analysis tool, continuous integration system, software quality, software reliability, codebases, integrated development environment, Klocwork, safety-critical software, critical system, Engines, Software, Standards, Software reliability, Companies, Context, static code analysis, continuous integration, software quality, Klocwork",lessons learned using a static analysis tool within a continuous integration system static analysis tools are used for improving software quality and reliability since these tools can be time consuming when used for analysis of big codebases they are normally run during scheduled e g nightly builds however the sooner a defect is found the easier it is to fix efficiently in order to detect defects faster some analysis tools offer an integration with the integrated development environment of the developers at the cost of not always detecting all the issues to detect defects earlier and still provide a reliable solution one could think of running an analysis tool at every build of a continuous integration system in this paper we share the lessons learned during the integration of the static analysis tool klocwork that we are developing with our continuous integration system we think that the lessons learned will be beneficial for most companies developing safety critical software or less critical systems that wish to run their analysis tool more often in their build system we report these lessons learned along with examples of our successes and failures integrated software program diagnostics safety critical software software quality static analysis tool continuous integration system software quality software reliability codebases integrated development environment klocwork safety critical software critical system engines software standards software reliability companies context static code analysis continuous integration software quality klocwork,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
156,0,144,"PCCTE: A portable component conformance test environment based on container cloud for avionics software development Large-scale complex avionics software system developed by multiple teams may lead to inconsistency issues of different architectures and various standards. The generic open software architecture for avionics system similar to Future Airborne Capability Environment (FACE) is therefore proposed. In this architecture, the portability of components is allowed through standardized interfaces between components and the software computing environment, which consists of an operating system, transport services and I/O services. The conformance test (CT) for the portable components should be automatically executed to support continuous integration. This paper presents a portal component conformance test environment (PCCTE) in which CT tasks are scheduled using event-driven policies based on a continuous integration server (such as Jenkins), and are executed in the cloud environment based on container (suck as Docker & Kubernetes) and micro-services (such as ZooKeeper). As a result, the evaluation shows that PCCTE is workable and effective. aerospace computing, avionics, cloud computing, program testing, software engineering, PCCTE environment, container cloud, avionics software development, future airborne capability environment, FACE, avionics software system, conformance test, software computing environment, portal component conformance test environment, Aerospace electronics, Cloud computing, Containers, Computer architecture, Libraries, Testing, avionics software system, container cloud, conformance test environment",pccte a portable component conformance test environment based on container cloud for avionics software development large scale complex avionics software system developed by multiple teams may lead to inconsistency issues of different architectures and various standards the generic open software architecture for avionics system similar to future airborne capability environment face is therefore proposed in this architecture the portability of components is allowed through standardized interfaces between components and the software computing environment which consists of an operating system transport services and i o services the conformance test ct for the portable components should be automatically executed to support continuous integration this paper presents a portal component conformance test environment pccte in which ct tasks are scheduled using event driven policies based on a continuous integration server such as jenkins and are executed in the cloud environment based on container suck as docker kubernetes and micro services such as zookeeper as a result the evaluation shows that pccte is workable and effective aerospace computing avionics cloud computing program testing software engineering pccte environment container cloud avionics software development future airborne capability environment face avionics software system conformance test software computing environment portal component conformance test environment aerospace electronics cloud computing containers computer architecture libraries testing avionics software system container cloud conformance test environment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
157,0,222,"Test orchestration a framework for Continuous Integration and Continuous deployment The enterprises follow Agile Software Development methodology to because business requirements changes frequently. In Agile Software Development methodology, it is essential to continuously integrate the component into a main trunk of a project to test the new component of the system. Then test all the component of the project; this happens frequently so it needs to stream line processes to orchestrate the tests. So it is difficult to manage the software development life cycle for those changes and maintain the software code quality. To maintain the product quality it is essential to integrate the product component and need to deploy a product on pre-production environment and test the product. Hence the need for Continuous Integration and Continuous Delivery process for software product. The popularization of DevOps, and cloud computing has revolutionized the software delivery process- making it faster and affordable for business to release their software continuously. Hence Enterprises need for reliable and predictable delivery process of software. The objective of the paper is to design an effective framework for automated testing and deployment to help to automate the code analysis, test selection, test scheduling, environment provisioning, test execution, results analysis and deployment pipeline. Test orchestration framework typically very complicated to develop such pipeline to make software reliable, and bug free. For environment provisioning can be provided through virtualization and cloud computing. program testing, scheduling, software prototyping, software quality, test orchestration, continuous integration process, continuous deployment process, agile software development methodology, component test, software development life cycle, software code quality, product quality, software product, DevOps, cloud computing, code analysis, test selection, test scheduling, environment provisioning, test execution, results analysis, deployment pipeline, virtualization, Software, Automation, Pipelines, Software reliability, Software testing, ContinuousIntegration (CI), Continious Deployment (CD), DevOps, Agile Software Development, Test Oracle, Test Orchestration, Cloud Computing, Delivery Pipeline",test orchestration a framework for continuous integration and continuous deployment the enterprises follow agile software development methodology to because business requirements changes frequently in agile software development methodology it is essential to continuously integrate the component into a main trunk of a project to test the new component of the system then test all the component of the project this happens frequently so it needs to stream line processes to orchestrate the tests so it is difficult to manage the software development life cycle for those changes and maintain the software code quality to maintain the product quality it is essential to integrate the product component and need to deploy a product on pre production environment and test the product hence the need for continuous integration and continuous delivery process for software product the popularization of devops and cloud computing has revolutionized the software delivery process making it faster and affordable for business to release their software continuously hence enterprises need for reliable and predictable delivery process of software the objective of the paper is to design an effective framework for automated testing and deployment to help to automate the code analysis test selection test scheduling environment provisioning test execution results analysis and deployment pipeline test orchestration framework typically very complicated to develop such pipeline to make software reliable and bug free for environment provisioning can be provided through virtualization and cloud computing program testing scheduling software prototyping software quality test orchestration continuous integration process continuous deployment process agile software development methodology component test software development life cycle software code quality product quality software product devops cloud computing code analysis test selection test scheduling environment provisioning test execution results analysis deployment pipeline virtualization software automation pipelines software reliability software testing continuousintegration ci continious deployment cd devops agile software development test oracle test orchestration cloud computing delivery pipeline,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
158,0,262,"Practical realization of software integration process during the development of complex hardware-software systems Progress in the field of information technology and radio engineering along with customer requirements lead to development of more and more complex multicompanent radio-systems. Our research lab is specialized in creation of heterogeneous hardware-software systems which are made up of components based on various (including embedded) operating systems. Error-free operation of such systems depends on a lot of factors one of which is the software quality. The main conclusion to be drawn from previous experience of hardware-software systems production was to make changes in software development process. In particular, integration practice was set up as independent process with special build-engineer to maintain it. During the stage-by-stage evolution of integration process a number of key factors have been revealed: cross-platform development, component inter dependency, lack of human and time resources. To meet all requirements several integration schemes have been proposed and put into operation. On the basis of previous projects in-depth analysis the decision on introducing into practice continuous integration and automated build processes was made. The set of measures intended to integration process development which was presented in this paper allows us to reduce time cost on software releases from several days to several hours. Furthermore, suggested scheme of integration process is considered to be universal and, therefore, could be applied in any project regardless of operating systems which are used in development process. At the same time it should be noted that for the purpose of most successful implantation of continuous integration it is essential that all process stages should be well documented and understood by all developers before project starts. hardware-software codesign, integrated software, operating systems (computers), software quality, software integration process, complex hardware-software system, information technology, radio engineering, customer requirement, multicomponent radio system, operating system, error free operation, software quality, software development process, projects in depth analysis, Electronic mail, Programming, Operating systems, Time measurement, Software measurement, Information technology, hardware-software system, software development process, continuous integration",practical realization of software integration process during the development of complex hardware software systems progress in the field of information technology and radio engineering along with customer requirements lead to development of more and more complex multicompanent radio systems our research lab is specialized in creation of heterogeneous hardware software systems which are made up of components based on various including embedded operating systems error free operation of such systems depends on a lot of factors one of which is the software quality the main conclusion to be drawn from previous experience of hardware software systems production was to make changes in software development process in particular integration practice was set up as independent process with special build engineer to maintain it during the stage by stage evolution of integration process a number of key factors have been revealed cross platform development component inter dependency lack of human and time resources to meet all requirements several integration schemes have been proposed and put into operation on the basis of previous projects in depth analysis the decision on introducing into practice continuous integration and automated build processes was made the set of measures intended to integration process development which was presented in this paper allows us to reduce time cost on software releases from several days to several hours furthermore suggested scheme of integration process is considered to be universal and therefore could be applied in any project regardless of operating systems which are used in development process at the same time it should be noted that for the purpose of most successful implantation of continuous integration it is essential that all process stages should be well documented and understood by all developers before project starts hardware software codesign integrated software operating systems computers software quality software integration process complex hardware software system information technology radio engineering customer requirement multicomponent radio system operating system error free operation software quality software development process projects in depth analysis electronic mail programming operating systems time measurement software measurement information technology hardware software system software development process continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
159,0,104,"Continuous Delivery with Jenkins: Jenkins Solutions to Implement Continuous Delivery This paper illustrates how Jenkins evolved from being a pure Continuous Integration Platform to a Continuous Delivery one, embracing the new design tendency where not only the build but also the release and the delivery process of the product is automated. In this scenario Jenkins becomes the orchestrator tool for all the teams/roles involved in the software lifecycle, thanks to which Development, Quality&Assurance and Operations teams can work closely together. Goal of this paper is not only to position Jenkins as hub for CD, but also introduce the challenges that still need to be solved in order to strengthen Jenkins' tracking capabilities. public domain software, software quality, Jenkins, continuous integration platform, continuous delivery process, software lifecycle, orchestrator tool, open source CI platform, Pipelines, Software, Automation, Time to market, Fingerprint recognition, Collaboration, Companies",continuous delivery with jenkins jenkins solutions to implement continuous delivery this paper illustrates how jenkins evolved from being a pure continuous integration platform to a continuous delivery one embracing the new design tendency where not only the build but also the release and the delivery process of the product is automated in this scenario jenkins becomes the orchestrator tool for all the teams roles involved in the software lifecycle thanks to which development quality assurance and operations teams can work closely together goal of this paper is not only to position jenkins as hub for cd but also introduce the challenges that still need to be solved in order to strengthen jenkins tracking capabilities public domain software software quality jenkins continuous integration platform continuous delivery process software lifecycle orchestrator tool open source ci platform pipelines software automation time to market fingerprint recognition collaboration companies,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
160,0,137,"Metrics in Agile Project Courses We believe that software engineering should be taught in a hands-on way such as through a project-based capstone course where students apply the learned concepts in a real setting. However, such a teaching format can be challenging and time-consuming for instructors. In this paper we explain how we selected and introduced a set of metrics to improve the manageability of our large multi-project capstone course. We regularly run such a course with over 100 students developing applications in 10-12 parallel projects over the course of one semester. Our approach focuses on measuring the success of three key workflows, namely Merge Management, Continuous Integration and Continuous Delivery. We show how these metrics help the instructors to keep track of the progress of multiple projects running at the same time, enabling them to identify and react to problems early. computer science education, continuous improvement, educational courses, project management, software prototyping, teaching, agile project courses, software engineering, project-based capstone course, teaching format, large multiproject capstone course manageability improvement, application development, merge management, continuous integration, continuous delivery, Measurement, Education, Organizations, Industries, Software, Servers, agile software engineering, capstone course, project manage- ment, metrics, continuous integration, continuous delivery",metrics in agile project courses we believe that software engineering should be taught in a hands on way such as through a project based capstone course where students apply the learned concepts in a real setting however such a teaching format can be challenging and time consuming for instructors in this paper we explain how we selected and introduced a set of metrics to improve the manageability of our large multi project capstone course we regularly run such a course with over 100 students developing applications in 10 12 parallel projects over the course of one semester our approach focuses on measuring the success of three key workflows namely merge management continuous integration and continuous delivery we show how these metrics help the instructors to keep track of the progress of multiple projects running at the same time enabling them to identify and react to problems early computer science education continuous improvement educational courses project management software prototyping teaching agile project courses software engineering project based capstone course teaching format large multiproject capstone course manageability improvement application development merge management continuous integration continuous delivery measurement education organizations industries software servers agile software engineering capstone course project manage ment metrics continuous integration continuous delivery,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
161,0,111,"Soft-Union: An Overlay Based Efficient Software P2P Distribution Scheme Recent years, SaaS (Software as a Service) has become an innovative software delivery model. The traditional centralized software distribution model failed to meet the rapid deployment requirement of distribute software in large scale environment. In this paper, we present Soft-Union: a novel peer-to-peer based software distribution scheme. Soft-Union organizes the nodes sharing the same software into an overlay to reduce the query time. Furthermore, it incorporates a distributed mechanism for meta-information placement mechanism, and a search protocol to balance the query load and shorter the query time of the block meta-information. The experimental results validate that Soft-Union significantly reduces the block query time comparing with flooding and BT-like software distribution solutions. cloud computing, peer-to-peer computing, soft union, P2P distribution scheme, software as a service, SaaS, innovative software delivery model, scale environment, peer-to-peer based software distribution, metainformation placement mechanism, Software, Peer to peer computing, Computer architecture, Protocols, Servers, Computational modeling, Cloud computing, SaaS, Software Distribution, Overlay, Peer-to-Peer, Search",soft union an overlay based efficient software p2p distribution scheme recent years saas software as a service has become an innovative software delivery model the traditional centralized software distribution model failed to meet the rapid deployment requirement of distribute software in large scale environment in this paper we present soft union a novel peer to peer based software distribution scheme soft union organizes the nodes sharing the same software into an overlay to reduce the query time furthermore it incorporates a distributed mechanism for meta information placement mechanism and a search protocol to balance the query load and shorter the query time of the block meta information the experimental results validate that soft union significantly reduces the block query time comparing with flooding and bt like software distribution solutions cloud computing peer to peer computing soft union p2p distribution scheme software as a service saas innovative software delivery model scale environment peer to peer based software distribution metainformation placement mechanism software peer to peer computing computer architecture protocols servers computational modeling cloud computing saas software distribution overlay peer to peer search,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
162,0,191,"Continuous Delivery Practices in a Large Financial Organization Continuous Delivery is an agile software development practice in which developers frequently integrate changes into the main development line and produce releases of their software. An automated Continuous Integration infrastructure builds and tests these changes. Claimed advantages of CD include early discovery of (integration) errors, reduced cycle time, and better adoption of coding standards and guidelines. This paper reports on a study in which we surveyed 152 developers of a large financial organization (ING Nederland), and investigated how they adopt a Continuous Integration and delivery pipeline during their development activities. In our study, we focus on topics related to managing technical debt, as well as test automation practices. The survey results shed light on the adoption of some agile methods in practice, and sometimes confirm, while in other cases, confute common wisdom and results obtained in other studies. For example, we found that refactoring tends to be performed together with other development activities, technical debt is almost always ""self-admitted"", developers timely document source code, and assure the quality of their product through extensive automated testing, with a third of respondents dedicating more than 50% of their time to do testing activities. financial data processing, program testing, quality assurance, software maintenance, software prototyping, software quality, source code (software), continuous delivery, CD, financial organization, ING Nederland, agile software development, continuous integration infrastructure, CI infrastructure, software refactoring, document source code, product quality assurance, automated testing, Pipelines, Testing, Software, Monitoring, Organizations, Measurement, Continuous Delivery, Continuous Integration, DevOps, Agile Development, Technical Debt, Refactoring, Testing, Test-Driven Development",continuous delivery practices in a large financial organization continuous delivery is an agile software development practice in which developers frequently integrate changes into the main development line and produce releases of their software an automated continuous integration infrastructure builds and tests these changes claimed advantages of cd include early discovery of integration errors reduced cycle time and better adoption of coding standards and guidelines this paper reports on a study in which we surveyed 152 developers of a large financial organization ing nederland and investigated how they adopt a continuous integration and delivery pipeline during their development activities in our study we focus on topics related to managing technical debt as well as test automation practices the survey results shed light on the adoption of some agile methods in practice and sometimes confirm while in other cases confute common wisdom and results obtained in other studies for example we found that refactoring tends to be performed together with other development activities technical debt is almost always self admitted developers timely document source code and assure the quality of their product through extensive automated testing with a third of respondents dedicating more than 50 of their time to do testing activities financial data processing program testing quality assurance software maintenance software prototyping software quality source code software continuous delivery cd financial organization ing nederland agile software development continuous integration infrastructure ci infrastructure software refactoring document source code product quality assurance automated testing pipelines testing software monitoring organizations measurement continuous delivery continuous integration devops agile development technical debt refactoring testing test driven development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
163,0,136,"An Open Source-Based Approach to Software Development Infrastructures As software systems become larger and more complex, automated software engineering tools play a crucial role for effective software development management, which is a key factor to lead quality software systems. In this work, we present TRICA, an open source-based software development infrastructure. The name of TRICA represents its features such as Traceability, Relationship, Informativeness, Cost-effectiveness, and Automation. Essentially, in TRICA, a continuous integration tool is coupled with a software configuration management tool and an issue tracking tool. We provisioned a mechanism to connect the open source tools in TRICA so that project members use the collaborated information to solve various issues and implementation problems efficiently, and easily share forthcoming issues during the course of the project. We show that TRICA can help to decentralize risks throughout the software development cycle and achieve successful software development. configuration management, groupware, integrated software, program diagnostics, public domain software, software development management, software quality, software tools, software systems, automated software engineering tools, software development management, software quality, TRICA, continuous integration tool, software configuration management tool, tracking tool, collaborated information, software development cycle, open source tools, Programming, Open source software, Software engineering, Software systems, Software tools, Testing, Software development management, Automation, Collaborative tools, Collaborative software, software engineering tools, continuous integration, SCM, issue tracking, open source",an open source based approach to software development infrastructures as software systems become larger and more complex automated software engineering tools play a crucial role for effective software development management which is a key factor to lead quality software systems in this work we present trica an open source based software development infrastructure the name of trica represents its features such as traceability relationship informativeness cost effectiveness and automation essentially in trica a continuous integration tool is coupled with a software configuration management tool and an issue tracking tool we provisioned a mechanism to connect the open source tools in trica so that project members use the collaborated information to solve various issues and implementation problems efficiently and easily share forthcoming issues during the course of the project we show that trica can help to decentralize risks throughout the software development cycle and achieve successful software development configuration management groupware integrated software program diagnostics public domain software software development management software quality software tools software systems automated software engineering tools software development management software quality trica continuous integration tool software configuration management tool tracking tool collaborated information software development cycle open source tools programming open source software software engineering software systems software tools testing software development management automation collaborative tools collaborative software software engineering tools continuous integration scm issue tracking open source,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
164,0,180,"ACI (automated Continuous Integration) using Jenkins: Key for successful embedded Software development Continuous Integration (CI) is the most common practice among software developers where they integrate their work into a baseline frequently. The industry is facing huge challenges while developing Software (S/W)s at multiple sites and tested on multiple platforms. The best way to make CI faster and more efficient is to automate the build and testing process. Jenkins is a CI tool that helps in automating the complete process, reducing the work of a developer and check the development at each and every step of S/W evolution. In this paper, we discuss the implementation of Jenkins for software patch integration and release to client. We consider a real-life scenario, how the software development is carried out in corporate ventures and how Jenkins can save developers/integrators crucial work hours by automating the complete process. In this paper, Jenkins is implemented in a master/slave architecture where master node is the Jenkins server and slaves are the Jenkins clients. We discuss the usage of various plug-ins available that allow Jenkins to be used with any environment of software development. embedded systems, integrated software, program testing, software engineering, Jenkins clients, Jenkins server, master slave architecture, software patch integration, S W evolution, CI tool, embedded software development, automated continuous integration, ACI, Testing, Automation, Smart phones, Software, Servers, Computer architecture, Androids, Android, Build, Continuous Integration, Jenkins, Source code Management",aci automated continuous integration using jenkins key for successful embedded software development continuous integration ci is the most common practice among software developers where they integrate their work into a baseline frequently the industry is facing huge challenges while developing software s w s at multiple sites and tested on multiple platforms the best way to make ci faster and more efficient is to automate the build and testing process jenkins is a ci tool that helps in automating the complete process reducing the work of a developer and check the development at each and every step of s w evolution in this paper we discuss the implementation of jenkins for software patch integration and release to client we consider a real life scenario how the software development is carried out in corporate ventures and how jenkins can save developers integrators crucial work hours by automating the complete process in this paper jenkins is implemented in a master slave architecture where master node is the jenkins server and slaves are the jenkins clients we discuss the usage of various plug ins available that allow jenkins to be used with any environment of software development embedded systems integrated software program testing software engineering jenkins clients jenkins server master slave architecture software patch integration s w evolution ci tool embedded software development automated continuous integration aci testing automation smart phones software servers computer architecture androids android build continuous integration jenkins source code management,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
165,0,72,"Poster: Improving Cloud-Based Continuous Integration Environments We propose a novel technique for improving the efficiency of cloud-based continuous integration development environments. Our technique identifies repetitive, expensive and time-consuming setup activities that are required to run integration and system tests in the cloud, and consolidates them into preconfigured testing virtual machines such that the overall costs of test execution are minimized. We create such testing machines by reconfiguring and opportunistically snapshotting the virtual machines already registered in the cloud. cloud computing, virtual machines, of cloud-based continuous integration development environments, preconfigured testing virtual machines, test execution cost, testing machines, Virtual machining, Software, Servers, Standards, Load modeling, Software testing, integration test, test-driven development, snapshot, system tests, linear programming, cost flow, flow constraints",poster improving cloud based continuous integration environments we propose a novel technique for improving the efficiency of cloud based continuous integration development environments our technique identifies repetitive expensive and time consuming setup activities that are required to run integration and system tests in the cloud and consolidates them into preconfigured testing virtual machines such that the overall costs of test execution are minimized we create such testing machines by reconfiguring and opportunistically snapshotting the virtual machines already registered in the cloud cloud computing virtual machines of cloud based continuous integration development environments preconfigured testing virtual machines test execution cost testing machines virtual machining software servers standards load modeling software testing integration test test driven development snapshot system tests linear programming cost flow flow constraints,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
166,0,239,"Improving the delivery cycle: A multiple-case study of the toolchains in Finnish software intensive enterprises Context: Software companies seek to gain benefit from agile development approaches in order to meet evolving market needs without losing their innovative edge. Agile practices emphasize frequent releases with the help of an automated toolchain from code to delivery. Objective: We investigate, which tools are used in software delivery, what are the reasons omitting certain parts of the toolchain and what implications toolchains have on how rapidly software gets delivered to customers. Method: We present a multiple-case study of the toolchains currently in use in Finnish software-intensive organizations interested in improving their delivery frequency. We conducted qualitative semi-structured interviews in 18 case organizations from various software domains. The interviewees were key representatives of their organization, considering delivery activities. Results: Commodity tools, such as version control and continuous integration, were used in almost every organization. Modestly used tools, such as UI testing and performance testing, were more distinctly missing from some organizations. Uncommon tools, such as artifact repository and acceptance testing, were used only in a minority of the organizations. Tool usage is affected by the state of current workflows, manual work and relevancy of tools. Organizations whose toolchains were more automated and contained fewer manual steps were able to deploy software more rapidly. Conclusions: There is variety in the need for tool support in different development steps as there are domain-specific differences in the goals of the case organizations. Still, a well-founded toolchain supports speedy delivery of new software. Continuous deployment, Continuous delivery, Software development tools, Deployment pipeline, Agile software development",improving the delivery cycle a multiple case study of the toolchains in finnish software intensive enterprises context software companies seek to gain benefit from agile development approaches in order to meet evolving market needs without losing their innovative edge agile practices emphasize frequent releases with the help of an automated toolchain from code to delivery objective we investigate which tools are used in software delivery what are the reasons omitting certain parts of the toolchain and what implications toolchains have on how rapidly software gets delivered to customers method we present a multiple case study of the toolchains currently in use in finnish software intensive organizations interested in improving their delivery frequency we conducted qualitative semi structured interviews in 18 case organizations from various software domains the interviewees were key representatives of their organization considering delivery activities results commodity tools such as version control and continuous integration were used in almost every organization modestly used tools such as ui testing and performance testing were more distinctly missing from some organizations uncommon tools such as artifact repository and acceptance testing were used only in a minority of the organizations tool usage is affected by the state of current workflows manual work and relevancy of tools organizations whose toolchains were more automated and contained fewer manual steps were able to deploy software more rapidly conclusions there is variety in the need for tool support in different development steps as there are domain specific differences in the goals of the case organizations still a well founded toolchain supports speedy delivery of new software continuous deployment continuous delivery software development tools deployment pipeline agile software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
167,0,274,"Testing robot controllers using constraint programming and continuous integration Context Testing complex industrial robots (CIRs) requires testing several interacting control systems. This is challenging, especially for robots performing process-intensive tasks such as painting or gluing, since their dedicated process control systems can be loosely coupled with the robot's motion control. Objective Current practices for validating CIRs involve manual test case design and execution. To reduce testing costs and improve quality assurance, a trend is to automate the generation of test cases. Our work aims to define a cost-effective automated testing technique to validate CIR control systems in an industrial context. Method This paper reports on a methodology, developed at ABB Robotics in collaboration with SIMULA, for the fully automated testing of CIRs control systems. Our approach draws on continuous integration principles and well-established constraint-based testing techniques. It is based on a novel constraint-based model for automatically generating test sequences where test sequences are both generated and executed as part of a continuous integration process. Results By performing a detailed analysis of experimental results over a simplified version of our constraint model, we determine the most appropriate parameterization of the operational version of the constraint model. This version is now being deployed at ABB Robotics's CIR testing facilities and used on a permanent basis. This paper presents the empirical results obtained when automatically generating test sequences for CIRs at ABB Robotics. In a real industrial setting, the results show that our methodology is not only able to detect reintroduced known faults, but also to spot completely new faults. Conclusion Our empirical evaluation shows that constraint-based testing is appropriate for automatically generating test sequences for CIRs and can be faithfully deployed in an industrial context. Constraint programming, Continuous integration, Robotized painting, Software testing, Distributed real time systems, Agile development",testing robot controllers using constraint programming and continuous integration context testing complex industrial robots cirs requires testing several interacting control systems this is challenging especially for robots performing process intensive tasks such as painting or gluing since their dedicated process control systems can be loosely coupled with the robot s motion control objective current practices for validating cirs involve manual test case design and execution to reduce testing costs and improve quality assurance a trend is to automate the generation of test cases our work aims to define a cost effective automated testing technique to validate cir control systems in an industrial context method this paper reports on a methodology developed at abb robotics in collaboration with simula for the fully automated testing of cirs control systems our approach draws on continuous integration principles and well established constraint based testing techniques it is based on a novel constraint based model for automatically generating test sequences where test sequences are both generated and executed as part of a continuous integration process results by performing a detailed analysis of experimental results over a simplified version of our constraint model we determine the most appropriate parameterization of the operational version of the constraint model this version is now being deployed at abb robotics s cir testing facilities and used on a permanent basis this paper presents the empirical results obtained when automatically generating test sequences for cirs at abb robotics in a real industrial setting the results show that our methodology is not only able to detect reintroduced known faults but also to spot completely new faults conclusion our empirical evaluation shows that constraint based testing is appropriate for automatically generating test sequences for cirs and can be faithfully deployed in an industrial context constraint programming continuous integration robotized painting software testing distributed real time systems agile development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
168,0,97,"Improved Software Production for the LHC Tunnel Cryogenics Control System The software development for the control system of the cryogenics in the LHC is partially automatized. However, every single modification requires a sequence of consecutive and interdependent tasks to be executed manually by software developers. A large number of control system consolidations and the evolution of the used IT technologies lead to reviewing the software production methodology. As a result, an open-source continuous integration server has been employed integrating all development tasks, tools and technologies. This paper describes the main improvements that have been made to fully automate the process of software production and the achieved results. CERN, LHC, cryogenics, industrial, control, software, development, continuous integration",improved software production for the lhc tunnel cryogenics control system the software development for the control system of the cryogenics in the lhc is partially automatized however every single modification requires a sequence of consecutive and interdependent tasks to be executed manually by software developers a large number of control system consolidations and the evolution of the used it technologies lead to reviewing the software production methodology as a result an open source continuous integration server has been employed integrating all development tasks tools and technologies this paper describes the main improvements that have been made to fully automate the process of software production and the achieved results cern lhc cryogenics industrial control software development continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
169,0,85,"Improved framework for the maintenance of the JET intershot analysis chain At the JET experiment data from routine diagnostics is analysed automatically by a suite of codes within minutes after operation. The maintenance of these interdependent codes and the provision of a consistent state of the physics database over many experimental campaigns against a backdrop of continuous hardware and software updates, requires well defined maintenance and validation procedures. In this paper, the development of a new generation of maintenance tools using distributed version control and a work-flow following the principle of continuous integration [1] is described. Intershot analysis of plasma diagnostics, Continuous integration, Distributed version control, Data traceability",improved framework for the maintenance of the jet intershot analysis chain at the jet experiment data from routine diagnostics is analysed automatically by a suite of codes within minutes after operation the maintenance of these interdependent codes and the provision of a consistent state of the physics database over many experimental campaigns against a backdrop of continuous hardware and software updates requires well defined maintenance and validation procedures in this paper the development of a new generation of maintenance tools using distributed version control and a work flow following the principle of continuous integration 1 is described intershot analysis of plasma diagnostics continuous integration distributed version control data traceability,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
170,0,128,"A Modeling and Verification Approach to the Design of Distributed IMA Architectures Using TTEthernet Integrated Modular Avionics (IMA) architectures complemented with Time-Triggered Ethernet (TTEthernet) provides a strong platform to support the design and deployment of distributed avionic software systems. The complexity of the design and continuous integration of such systems can be managed using a model-based methodology. In this paper, we build on top of our extension of the AADL modeling language to model TTEthernet-based distributed systems and leverage model transformations to enable undertaking the verification of the system models produced with this methodology. In particular, we propose to transform the system models to a model suitable for a simulation with DEVS. We illustrate the proposed approach using an example of a navigation and guidance system and we use this example to show the verification of the contention-freedom property of TTEthernet schedule. Integrated Modular Avionics, TTEthernet, Model transformation, Verification, DEVS, Simulation",a modeling and verification approach to the design of distributed ima architectures using ttethernet integrated modular avionics ima architectures complemented with time triggered ethernet ttethernet provides a strong platform to support the design and deployment of distributed avionic software systems the complexity of the design and continuous integration of such systems can be managed using a model based methodology in this paper we build on top of our extension of the aadl modeling language to model ttethernet based distributed systems and leverage model transformations to enable undertaking the verification of the system models produced with this methodology in particular we propose to transform the system models to a model suitable for a simulation with devs we illustrate the proposed approach using an example of a navigation and guidance system and we use this example to show the verification of the contention freedom property of ttethernet schedule integrated modular avionics ttethernet model transformation verification devs simulation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
171,0,197,"MDSplus automated build and distribution system Support of the MDSplus data handling system has been enhanced by the addition of an automated build system which does nightly builds of MDSplus for many computer platforms producing software packages which can now be downloaded using a web browser or via package repositories suitable for automatic updating. The build system was implemented using an extensible continuous integration server product called Hudson which schedules software builds on a collection of VMware based virtual machines. New releases are created based on updates via the MDSplus cvs code repository and versioning are managed using cvs tags and branches. Currently stable, beta and alpha releases of MDSplus are maintained for eleven different platforms including Windows, MacOSX, RedHat Enterprise Linux, Fedora, Ubuntu and Solaris. For some of these platforms, MDSplus packaging has been broken into functional modules so users can pick and choose which MDSplus features they want to install. An added feature to the latest Linux based platforms is the use of package dependencies. When installing MDSplus from the package repositories, any additional required packages used by MDSplus will be installed automatically greatly simplifying the installation of MDSplus. This paper will describe the MDSplus package automated build and distribution system. Data acquisition systems, Data management, Data formats, MDSplus",mdsplus automated build and distribution system support of the mdsplus data handling system has been enhanced by the addition of an automated build system which does nightly builds of mdsplus for many computer platforms producing software packages which can now be downloaded using a web browser or via package repositories suitable for automatic updating the build system was implemented using an extensible continuous integration server product called hudson which schedules software builds on a collection of vmware based virtual machines new releases are created based on updates via the mdsplus cvs code repository and versioning are managed using cvs tags and branches currently stable beta and alpha releases of mdsplus are maintained for eleven different platforms including windows macosx redhat enterprise linux fedora ubuntu and solaris for some of these platforms mdsplus packaging has been broken into functional modules so users can pick and choose which mdsplus features they want to install an added feature to the latest linux based platforms is the use of package dependencies when installing mdsplus from the package repositories any additional required packages used by mdsplus will be installed automatically greatly simplifying the installation of mdsplus this paper will describe the mdsplus package automated build and distribution system data acquisition systems data management data formats mdsplus,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
172,0,328,"Systematic scalability assessment for feature oriented multi-tenant services Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand. In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services. Distributed systems, Scalability, Tool support",systematic scalability assessment for feature oriented multi tenant services recent software engineering paradigms such as software product lines supporting development techniques like feature modeling and cloud provisioning models such as platform and infrastructure as a service allow for great flexibility during both software design and deployment resulting in potentially large cost savings however all this flexibility comes with a catch as the combinatorial complexity of optional design features and deployment variability increases the difficulty of assessing system qualities such as scalability and quality of service increases too and if the software itself is not scalable for instance because of a specific set of selected features deploying additional service instances is a futile endeavor clearly there is a need to systematically measure the impact of feature selection on scalability as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand in this work we document our results on systematic load testing for automated quality of service and scalability analysis the major contribution of our work is tool support and a methodology to analyze the scalability of these distributed feature oriented multi tenant software systems in a continuous integration process we discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions additionally we highlight how our methodology and framework for performance and scalability prediction differs from state of practice solutions we take the viewpoint of both the tenant of the service and the service provider and report on our experiences applying the approach to an industrial use case in the domain of electronic payments we conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers such as the ability to quantify the impact of new features in existing service compositions and the early detection of hidden feature interactions that may negatively affect the overall performance of multi tenant services distributed systems scalability tool support,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
173,0,99,"A Current Study on the Limitations of Agile Methods in Industry Using Secure Google Forms The Agile methods favours more communication, continuous integration, rapid delivery of software modules, iterative and incremental approach, but at the same time Agile software development has limitations like lack of upfront planning, lack of sufficient documentation, lack of predictability, etc.. Sometimes these limitations and so many methods make Agile software development more stressful. This work is about finding the current limitations and advantages of Agile software development. For finding the actual limitations beyond the literature, an online survey was conducted with the specified sample size of Agile experienced professionals, then the ANOVA test is applied to satisfy the hypothesis. Agile, Scrum, Survey, SDLC, Software Development",a current study on the limitations of agile methods in industry using secure google forms the agile methods favours more communication continuous integration rapid delivery of software modules iterative and incremental approach but at the same time agile software development has limitations like lack of upfront planning lack of sufficient documentation lack of predictability etc sometimes these limitations and so many methods make agile software development more stressful this work is about finding the current limitations and advantages of agile software development for finding the actual limitations beyond the literature an online survey was conducted with the specified sample size of agile experienced professionals then the anova test is applied to satisfy the hypothesis agile scrum survey sdlc software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
174,0,307,"Personalized Interactive Storyboarding utilizing Content Based Multimedia Retrieval An explosion of digital multimedia technologies that permit quick and easy uploading of any multimedia file to the web, coupled with the rapid advancement of hardware and software have resulted in petabytes of multimedia data being available on the internet. In this paper, we provide a framework that rapidly integrates these data for personalized storyboarding utilizing Content Based Multimedia Retrieval (CBMR) techniques. A storyboard is a narrative of audio, text, video and images of a particular topic that are linked systematically. People have different abilities to understand a story/topic according to their learning styles coupled with experience, age, knowledge, gender etc. Personalized storyboard multimedia learning help the user dictate the learning process in a creative way through multiple means of representation, expression and engagement. This process involves active learning, both behavioral as well as cognitive. Learning is constructive, and information learned is remembered at a deeper level. The use of multimedia promotes meaningful learning that can be transferred or generalized to other situations. In this paper, various learning styles are discussed. The goal of the research was to design and implement a novel approach that integrates human reasoning with computerized algorithms for multimedia storyboarding for learning. By systematically coupling human reasoning and computerized process, we attempt to minimize the barrier between the human's cognitive model of what they are trying to storyboard and the computers understanding of the user's task. We present our model which utilizes a fusion of content based image retrieval, content based video retrieval, content based audio retrieval, and text based retrieval techniques with human computer interaction based relevance feedback to enhance the learning process through personalized multimedia storyboarding. To illustrate the advantages of this model in a greater detail, we showcased the concepts of this model utilizing the game of cricket. We summarize the paper by discussing the future in this area. Human Computer Interaction, Content Based Multimedia Retrieval, Learning, Storyboarding",personalized interactive storyboarding utilizing content based multimedia retrieval an explosion of digital multimedia technologies that permit quick and easy uploading of any multimedia file to the web coupled with the rapid advancement of hardware and software have resulted in petabytes of multimedia data being available on the internet in this paper we provide a framework that rapidly integrates these data for personalized storyboarding utilizing content based multimedia retrieval cbmr techniques a storyboard is a narrative of audio text video and images of a particular topic that are linked systematically people have different abilities to understand a story topic according to their learning styles coupled with experience age knowledge gender etc personalized storyboard multimedia learning help the user dictate the learning process in a creative way through multiple means of representation expression and engagement this process involves active learning both behavioral as well as cognitive learning is constructive and information learned is remembered at a deeper level the use of multimedia promotes meaningful learning that can be transferred or generalized to other situations in this paper various learning styles are discussed the goal of the research was to design and implement a novel approach that integrates human reasoning with computerized algorithms for multimedia storyboarding for learning by systematically coupling human reasoning and computerized process we attempt to minimize the barrier between the human s cognitive model of what they are trying to storyboard and the computers understanding of the user s task we present our model which utilizes a fusion of content based image retrieval content based video retrieval content based audio retrieval and text based retrieval techniques with human computer interaction based relevance feedback to enhance the learning process through personalized multimedia storyboarding to illustrate the advantages of this model in a greater detail we showcased the concepts of this model utilizing the game of cricket we summarize the paper by discussing the future in this area human computer interaction content based multimedia retrieval learning storyboarding,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
175,0,149,"Modeling continuous integration practice differences in industry software development Continuous integration is a software practice where developers integrate frequently, at least daily. While this is an ostensibly simple concept, it does leave ample room for interpretation: what is it the developers integrate with, what happens when they do, and what happens before they do? These are all open questions with regards to the details of how one implements the practice of continuous integration, and it is conceivable that not all such implementations in the industry are alike. In this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case. Based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences. The application of the model to an industry software development project is then described in an illustrative case study. Continuous integration, Agile software development",modeling continuous integration practice differences in industry software development continuous integration is a software practice where developers integrate frequently at least daily while this is an ostensibly simple concept it does leave ample room for interpretation what is it the developers integrate with what happens when they do and what happens before they do these are all open questions with regards to the details of how one implements the practice of continuous integration and it is conceivable that not all such implementations in the industry are alike in this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences the application of the model to an industry software development project is then described in an illustrative case study continuous integration agile software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
176,0,213,"A Quality Framework for Software Continuous Integration The research in this paper combines two main areas, the first one is software quality and the second is the agile practices of continuous integration. Software quality has been an important topic since the beginning of the software development and production. Many researches have been conducted to discuss how the quality of software is a critical factor to its success [1--5]. Because software became an important part of almost every task in our daily life, having high quality software that meets the users' expectations is important [6]. Software integration is a stage in every software development lifecycle, it is defined as the process to assemble the software components and produce a single product. It has been shown that software integration and integration testing can make more than 40% of the overall project cost, so it is important that they are done efficiently and easily to be able to manage the involved risks [7]. A software engineering practice called continuous integration (CI) was introduced by Kent Beck and Ron Jeffries to mitigate the risks of software integration, enhance its process and improve its quality [8]. In this research, the principles of CI are identified and applied to a case study in order to analyze their impact on the software development process quality factors. Continuous integration, Software quality framework, ISO, Agile, Extreme programming, Software development",a quality framework for software continuous integration the research in this paper combines two main areas the first one is software quality and the second is the agile practices of continuous integration software quality has been an important topic since the beginning of the software development and production many researches have been conducted to discuss how the quality of software is a critical factor to its success 1 5 because software became an important part of almost every task in our daily life having high quality software that meets the users expectations is important 6 software integration is a stage in every software development lifecycle it is defined as the process to assemble the software components and produce a single product it has been shown that software integration and integration testing can make more than 40 of the overall project cost so it is important that they are done efficiently and easily to be able to manage the involved risks 7 a software engineering practice called continuous integration ci was introduced by kent beck and ron jeffries to mitigate the risks of software integration enhance its process and improve its quality 8 in this research the principles of ci are identified and applied to a case study in order to analyze their impact on the software development process quality factors continuous integration software quality framework iso agile extreme programming software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
177,0,154,"Integrative 3D modelling of complex carving surface Modelling of a complex carving surface is the most important process for digitization of art carving such as Chinese classical furniture carving, and it is difficult to be fulfilled. However, a complex 2D curve flower pattern can be easily acquired or drawn by handcraft or a drawing software. This paper presents a quick integrative 3D modeling method of complex carving surface based on a 2D curve flower pattern. The proposed method uses a scanning analysis algorithm, a normal distribution function and a distance function to model and create carving tracks. In this paper, the delamination, combination and interpolation of modelling process are described as well. The provided research method will make the modelling of complex carving surface more intelligent, agile, and will meet the requirement of integrative 3D modelling of digital art carving. Experimental results show that this method is of quick modelling and multi-model effective characteristics with realizable interactive designing and excellent practicability. 3D modelling, Complex carving surface, Multilayer model, Combinating model, Model interpolation",integrative 3d modelling of complex carving surface modelling of a complex carving surface is the most important process for digitization of art carving such as chinese classical furniture carving and it is difficult to be fulfilled however a complex 2d curve flower pattern can be easily acquired or drawn by handcraft or a drawing software this paper presents a quick integrative 3d modeling method of complex carving surface based on a 2d curve flower pattern the proposed method uses a scanning analysis algorithm a normal distribution function and a distance function to model and create carving tracks in this paper the delamination combination and interpolation of modelling process are described as well the provided research method will make the modelling of complex carving surface more intelligent agile and will meet the requirement of integrative 3d modelling of digital art carving experimental results show that this method is of quick modelling and multi model effective characteristics with realizable interactive designing and excellent practicability 3d modelling complex carving surface multilayer model combinating model model interpolation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
178,0,234,"Temporal dynamics of neural activity in an integration of visual and contextual information in an esthetic preference task While viewing works of art in galleries, we evaluate them by integrating at least two types of information: their visual properties (e.g., colors, symmetry, and proportion) and contextual information accompanying them (e.g., titles and names of artists). How rapidly the brain integrates visual and contextual information of artworks remains to be investigated. Using electroencephalography (EEG), we investigated neural activity when subjects with no professional experience in art viewed images of sculptures (masterpieces from the Classical and Renaissance periods, characterized by a canonical proportion of the golden ratio) and performed a five-scale rating of how appealing they were. At the beginning of each trial, we manipulated the expectations of the subjects for an upcoming sculpture by presenting information about its authenticity (either ``genuine'' or ``fake''), although all images were actually taken from genuine artworks. The image of the sculpture was then presented, either in its original proportion or after being deformed by a photo-editing software. This 2 × 2 factorial design enabled us to identify whether each component of the EEG response was sensitive to contextual information (genuine or fake), visual information (original or deformed), or both. Results revealed that amplitudes of a positive EEG component emerging at 200--300ms after the presentation of the artworks (mainly distributed over the parietal cortex) were significantly modulated by both visual and contextual factors, indicating a rapid integration of these two types of information in the brain. Neuroesthetics, Electroencephalography, Contextual information, Parietal cortex",temporal dynamics of neural activity in an integration of visual and contextual information in an esthetic preference task while viewing works of art in galleries we evaluate them by integrating at least two types of information their visual properties e g colors symmetry and proportion and contextual information accompanying them e g titles and names of artists how rapidly the brain integrates visual and contextual information of artworks remains to be investigated using electroencephalography eeg we investigated neural activity when subjects with no professional experience in art viewed images of sculptures masterpieces from the classical and renaissance periods characterized by a canonical proportion of the golden ratio and performed a five scale rating of how appealing they were at the beginning of each trial we manipulated the expectations of the subjects for an upcoming sculpture by presenting information about its authenticity either genuine or fake although all images were actually taken from genuine artworks the image of the sculpture was then presented either in its original proportion or after being deformed by a photo editing software this 2 2 factorial design enabled us to identify whether each component of the eeg response was sensitive to contextual information genuine or fake visual information original or deformed or both results revealed that amplitudes of a positive eeg component emerging at 200 300ms after the presentation of the artworks mainly distributed over the parietal cortex were significantly modulated by both visual and contextual factors indicating a rapid integration of these two types of information in the brain neuroesthetics electroencephalography contextual information parietal cortex,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
179,0,235,"A flexible environment for rapid prototyping and analysis of distributed real-time safety-critical systems Currently, there is a plethora of low-cost commercial off-the-shelf (COTS) hardware available for implementing control systems. These range from devices with fairly low intelligence, e.g. smart sensors and actuators, to dedicated controllers such as PowerPC, programmable logic controllers (PLCs) and PC-based boards to dedicated systems-on-a-chip (SoC) ASICS and FPGAs. When considering the construction of complex distributed systems, e.g. for a ship, aircraft, car, train, process plant, the ability to rapidly integrate a variety of devices from different manufacturers is essential. A problem, however, is that manufacturers prefer to supply proprietary tools for programming their products. As a consequence of this lack of `openness', rapid prototyping and development of distributed systems is extremely difficult and costly for a systems integrator. Great opportunities thus exist to produce high-performance, dependable distributed systems. However, the key element that is missing is software tool support for systems integration. The objective of the Flexible Control Systems Development and Integration Environment for Control Systems (FLEXICON) project IST-2001-37269 is to solve these problems for industry and reduce development and implementation costs for distributed control systems by providing an integrated suite of tools to support all the development life-cycle of the system. Work within the Rolls-Royce supported University Technology Centre (UTC) is investigating rapid prototyping of controllers for aero-engines, unmanned aerial vehicles and ships. This paper describes the use of the developed co-simulation environment for a high-speed merchant vessel propulsion system application. CORBA, Co-simulation, Distributed systems, Tool support, Systems integration, Obsolescence management",a flexible environment for rapid prototyping and analysis of distributed real time safety critical systems currently there is a plethora of low cost commercial off the shelf cots hardware available for implementing control systems these range from devices with fairly low intelligence e g smart sensors and actuators to dedicated controllers such as powerpc programmable logic controllers plcs and pc based boards to dedicated systems on a chip soc asics and fpgas when considering the construction of complex distributed systems e g for a ship aircraft car train process plant the ability to rapidly integrate a variety of devices from different manufacturers is essential a problem however is that manufacturers prefer to supply proprietary tools for programming their products as a consequence of this lack of openness rapid prototyping and development of distributed systems is extremely difficult and costly for a systems integrator great opportunities thus exist to produce high performance dependable distributed systems however the key element that is missing is software tool support for systems integration the objective of the flexible control systems development and integration environment for control systems flexicon project ist 2001 37269 is to solve these problems for industry and reduce development and implementation costs for distributed control systems by providing an integrated suite of tools to support all the development life cycle of the system work within the rolls royce supported university technology centre utc is investigating rapid prototyping of controllers for aero engines unmanned aerial vehicles and ships this paper describes the use of the developed co simulation environment for a high speed merchant vessel propulsion system application corba co simulation distributed systems tool support systems integration obsolescence management,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
180,0,196,"Ambient intelligence platform using multi-agent system and mobile ubiquitous hardware In this paper, a novel ambient intelligence (AmI) platform is proposed to facilitate fast integration of different control algorithms, device networks and user interfaces. This platform defines the overall hardware/software architecture and communication standards. It consists of four layers, namely the ubiquitous environment, middleware, multi-agent system and application layer. The multi-agent system is implemented using Java Agent DEvelopment (JADE) framework and allows users to incorporate multiple control algorithms as agents for managing different tasks. The Universal Plug and Play (UPnP) device discovery protocol is used as a middleware, which isolates the multi-agent system and physical ubiquitous environment while providing a standard communication channel between the two. An XML content language has been designed to provide standard communication between various user interfaces and the multi-agent system. A mobile ubiquitous setup box is designed to allow fast construction of ubiquitous environments in any physical space. The real time performance analysis shows the potential of the proposed AmI platform to be used in real-life AmI applications. A case study has also been carried out to demonstrate the possibility of integrating multiple control algorithms in the multi-agent system and achieving a significant improvement on the overall offline learning performance. Ambient intelligence, Multi-agent system, Ubiquitous environment",ambient intelligence platform using multi agent system and mobile ubiquitous hardware in this paper a novel ambient intelligence ami platform is proposed to facilitate fast integration of different control algorithms device networks and user interfaces this platform defines the overall hardware software architecture and communication standards it consists of four layers namely the ubiquitous environment middleware multi agent system and application layer the multi agent system is implemented using java agent development jade framework and allows users to incorporate multiple control algorithms as agents for managing different tasks the universal plug and play upnp device discovery protocol is used as a middleware which isolates the multi agent system and physical ubiquitous environment while providing a standard communication channel between the two an xml content language has been designed to provide standard communication between various user interfaces and the multi agent system a mobile ubiquitous setup box is designed to allow fast construction of ubiquitous environments in any physical space the real time performance analysis shows the potential of the proposed ami platform to be used in real life ami applications a case study has also been carried out to demonstrate the possibility of integrating multiple control algorithms in the multi agent system and achieving a significant improvement on the overall offline learning performance ambient intelligence multi agent system ubiquitous environment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
181,0,238,"A geomorphologic GIS-multivariate analysis approach to delineate environmental units, a case study of La Malinche volcano (central México) The Environmental Units Map (EUM) is a strategic document that provides valuable information about landscape attributes for studies focused on environmental research, urban and land planning and environmental management. Traditionally, the systematic mapping of landforms has been used to integrate landforms and environmental data. Widespread availability of remote sensing data along with thematic cartography and implementation of Geographic Information Systems (GIS), allows a fast integration of landscape environmental attributes, effectively reducing time and costs. In this study we propose an approach to delineate an environmental units map using a geomorphologic map and a multivariate analysis processed in a GIS on a regional cartographic scale (1:75,000). Our study area is La Malinche volcano (located in central México) where there are highly contrasting biophysical conditions and land use over relatively short distances. By means of a Hierarchical Cluster Analysis (HCA) a total of 29 environmental units were obtained for La Malinche. The environmental units range from alpine environments to semi arid lowlands (over a wide volcanic piedmont) where crops and urban development predominate. Our results suggest that integrating environmental units using a multivariate statistical approach not only produces results in agreement with what we observe empirically, but it also allows us to identify those factors which control the grouping of environmental attributes. The method proposed here can be used to integrate environmental data in a single map, and this could prove useful for environmental management in the future. Environmental units map, Land system mapping, Hierarchical cluster analysis, GIS, La Malinche volcano",a geomorphologic gis multivariate analysis approach to delineate environmental units a case study of la malinche volcano central m xico the environmental units map eum is a strategic document that provides valuable information about landscape attributes for studies focused on environmental research urban and land planning and environmental management traditionally the systematic mapping of landforms has been used to integrate landforms and environmental data widespread availability of remote sensing data along with thematic cartography and implementation of geographic information systems gis allows a fast integration of landscape environmental attributes effectively reducing time and costs in this study we propose an approach to delineate an environmental units map using a geomorphologic map and a multivariate analysis processed in a gis on a regional cartographic scale 1 75 000 our study area is la malinche volcano located in central m xico where there are highly contrasting biophysical conditions and land use over relatively short distances by means of a hierarchical cluster analysis hca a total of 29 environmental units were obtained for la malinche the environmental units range from alpine environments to semi arid lowlands over a wide volcanic piedmont where crops and urban development predominate our results suggest that integrating environmental units using a multivariate statistical approach not only produces results in agreement with what we observe empirically but it also allows us to identify those factors which control the grouping of environmental attributes the method proposed here can be used to integrate environmental data in a single map and this could prove useful for environmental management in the future environmental units map land system mapping hierarchical cluster analysis gis la malinche volcano,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
182,0,253,"A framework to explore innovation at SAP through bibliometric analysis of patent applications Easily accessible patent databases and advances in technology have enabled the exploration of organizational innovation through the analysis of patent records. However, the textual content of patents presents obstacles to gleaning useful information. In this study, we develop an expert system framework that utilizes text and data mining procedures for analyzing innovation through textual patent data. Specifically, we use patent titles representing the innovation activity at one company (SAP) and perform a bibliometric analysis using our proposed framework. Enterprise software, of which SAP is a pioneering developer, must serve a wide assortment of functions for companies in many different industries. In addition, SAP's sole focus is on enterprise software and it is a market leader in the category with substantial patent activity over the last decade. Using our framework to analyze SAP's patent activity provides a demonstration of how our bibliometric analysis can summarize and identify trends in innovation in a large software company. Our results illustrate that SAP has a breadth of innovative activity spread over the three-tier software engineering architecture and a lack of topical repetition indicative of limited depth. SAP's innovation is also seen to emphasize data management and quickly integrate emerging technologies. Results of an analysis on any company following our framework could be used for a variety of purposes, including: to examine the scope and scale of innovation of an organization, to examine the influence of technological trends on businesses, or to gain insight into corporate strategy that could be used to aid planning, investment, and purchasing decisions. Software, Innovation, Enterprise data management, Business intelligence and social media data analytics, Text analysis, Cluster analysis",a framework to explore innovation at sap through bibliometric analysis of patent applications easily accessible patent databases and advances in technology have enabled the exploration of organizational innovation through the analysis of patent records however the textual content of patents presents obstacles to gleaning useful information in this study we develop an expert system framework that utilizes text and data mining procedures for analyzing innovation through textual patent data specifically we use patent titles representing the innovation activity at one company sap and perform a bibliometric analysis using our proposed framework enterprise software of which sap is a pioneering developer must serve a wide assortment of functions for companies in many different industries in addition sap s sole focus is on enterprise software and it is a market leader in the category with substantial patent activity over the last decade using our framework to analyze sap s patent activity provides a demonstration of how our bibliometric analysis can summarize and identify trends in innovation in a large software company our results illustrate that sap has a breadth of innovative activity spread over the three tier software engineering architecture and a lack of topical repetition indicative of limited depth sap s innovation is also seen to emphasize data management and quickly integrate emerging technologies results of an analysis on any company following our framework could be used for a variety of purposes including to examine the scope and scale of innovation of an organization to examine the influence of technological trends on businesses or to gain insight into corporate strategy that could be used to aid planning investment and purchasing decisions software innovation enterprise data management business intelligence and social media data analytics text analysis cluster analysis,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
183,0,299,"Continuous Integration of Field Level Production Data into Top-level Information Systems Using the OPC Interface Standard On the way to the fourth industrial revolution, one major requirement lies in reaching interoperability between hardware and software systems. Especially real-time propagation of shop floor information in top-level production planning and control systems as well as the consolidation of distributed information into a consistent data basis for comprehensive data analysis are still missing in most production environments. Existing approaches to serve interoperability through standardized interfaces are limited by proprietary data exchange protocols and information models. Within industrial manufacturing and automation, standardization attempts between these systems are primarily focused on industrial interfaces like OPC/OPC-UA. However, the aggregation of data created by devices like sensors or machinery control units into useful information has not been satisfactorily solved yet as their underlying models are carried out using different modeling paradigms and programming languages, thus intercommunication is difficult to implement and to maintain. In this work, an integration chain for data from field level to top-level information systems is presented. As Manufacturing Execution Systems or Enterprise Resource Planning tools are implemented in higher programming languages, the modeling of field level information has to be adapted in terms of a semantic interpretation. The approach provides integration capabilities for OPC-conform data generated on the field level. The information is extracted from low level information systems, transformed according to object-oriented programming paradigms and object-relational standards and finally integrated into databases that allow full semantic annotation and interpretation compatible to a common information model. Hence, users on management levels of the enterprise are able to perform holistic data treatment and data exploration along with personalized information views based on this central data storage by means of a reliable and comfortable data acquisition. This increases the quality of data and of the decision support itself, as more time remains for the actual task of data evaluation. Interoperability, Information integration, Ontology, Semantic data, OPC, OPC UA",continuous integration of field level production data into top level information systems using the opc interface standard on the way to the fourth industrial revolution one major requirement lies in reaching interoperability between hardware and software systems especially real time propagation of shop floor information in top level production planning and control systems as well as the consolidation of distributed information into a consistent data basis for comprehensive data analysis are still missing in most production environments existing approaches to serve interoperability through standardized interfaces are limited by proprietary data exchange protocols and information models within industrial manufacturing and automation standardization attempts between these systems are primarily focused on industrial interfaces like opc opc ua however the aggregation of data created by devices like sensors or machinery control units into useful information has not been satisfactorily solved yet as their underlying models are carried out using different modeling paradigms and programming languages thus intercommunication is difficult to implement and to maintain in this work an integration chain for data from field level to top level information systems is presented as manufacturing execution systems or enterprise resource planning tools are implemented in higher programming languages the modeling of field level information has to be adapted in terms of a semantic interpretation the approach provides integration capabilities for opc conform data generated on the field level the information is extracted from low level information systems transformed according to object oriented programming paradigms and object relational standards and finally integrated into databases that allow full semantic annotation and interpretation compatible to a common information model hence users on management levels of the enterprise are able to perform holistic data treatment and data exploration along with personalized information views based on this central data storage by means of a reliable and comfortable data acquisition this increases the quality of data and of the decision support itself as more time remains for the actual task of data evaluation interoperability information integration ontology semantic data opc opc ua,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
184,0,210,"Enabling Radical Innovation through Highly Iterative Product Expedition in Ramp up and Demonstration Factories Short development times that are characterized by a high return on engineering (RoE) become increasingly important as a critical success factor for the realization of radical innovation. However, the high degree of complexity of modern products results in a long and costly development time when traditional sequential development processes are employed. The paper at hand describes the restraints of typical gate-oriented product development processes and builds up on recent studies recommending highly iterative innovation processes for the fast realization of physical product ideas. The suggested methodology represents an approach based on the Scrum process model from the software industry that includes the continuous integration of costumers and production engineers based on the execution of feasibility studies by the early and stepwise development of prototypes. In this context, modern ramp up and demonstration factories possessing a product lifecycle management (PLM) system, an integrated ICT infrastructure, interdisciplinary engineering teams and scalable manufacturing technologies are suggested as key enablers. The authors illustrate that these facilities, together with a sensor-based product expedition are particularly suitable for implementing an adapted Scrum process for the development of physical product ideas. A critical reflection on the basis of the development of an electric car aims to underline the suitability of the presented methodology in enabling radical innovation Radical innovation, Highly iterative product development, Scrum, Complexity management, Ramp up",enabling radical innovation through highly iterative product expedition in ramp up and demonstration factories short development times that are characterized by a high return on engineering roe become increasingly important as a critical success factor for the realization of radical innovation however the high degree of complexity of modern products results in a long and costly development time when traditional sequential development processes are employed the paper at hand describes the restraints of typical gate oriented product development processes and builds up on recent studies recommending highly iterative innovation processes for the fast realization of physical product ideas the suggested methodology represents an approach based on the scrum process model from the software industry that includes the continuous integration of costumers and production engineers based on the execution of feasibility studies by the early and stepwise development of prototypes in this context modern ramp up and demonstration factories possessing a product lifecycle management plm system an integrated ict infrastructure interdisciplinary engineering teams and scalable manufacturing technologies are suggested as key enablers the authors illustrate that these facilities together with a sensor based product expedition are particularly suitable for implementing an adapted scrum process for the development of physical product ideas a critical reflection on the basis of the development of an electric car aims to underline the suitability of the presented methodology in enabling radical innovation radical innovation highly iterative product development scrum complexity management ramp up,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
185,0,196,"Vector space architecture for emergent interoperability of systems by learning from demonstration The rapid integration of physical systems with cyberspace infrastructure, the so-called Internet of Things, is likely to have a significant effect on how people interact with the physical environment and design information and communication systems. Internet-connected systems are expected to vastly outnumber people on the planet in the near future, leading to grand challenges in software engineering and automation in application domains involving complex and evolving systems. Several decades of artificial intelligence research suggests that conventional approaches to making such systems automatically interoperable using handcrafted ``semantic'' descriptions of services and information are difficult to apply. In this paper we outline a bioinspired learning approach to creating interoperable systems, which does not require handcrafted semantic descriptions and rules. Instead, the idea is that a functioning system (of systems) can emerge from an initial pseudorandom state through learning from examples, provided that each component conforms to a set of information coding rules. We combine a binary vector symbolic architecture (VSA) with an associative memory known as sparse distributed memory (SDM) to model context-dependent prediction by learning from examples. We present simulation results demonstrating that the proposed architecture can enable system interoperability by learning, for example by human demonstration. Communications, Interoperability, Learning, Vector symbolic architecture, Sparse distributed memory, System of systems",vector space architecture for emergent interoperability of systems by learning from demonstration the rapid integration of physical systems with cyberspace infrastructure the so called internet of things is likely to have a significant effect on how people interact with the physical environment and design information and communication systems internet connected systems are expected to vastly outnumber people on the planet in the near future leading to grand challenges in software engineering and automation in application domains involving complex and evolving systems several decades of artificial intelligence research suggests that conventional approaches to making such systems automatically interoperable using handcrafted semantic descriptions of services and information are difficult to apply in this paper we outline a bioinspired learning approach to creating interoperable systems which does not require handcrafted semantic descriptions and rules instead the idea is that a functioning system of systems can emerge from an initial pseudorandom state through learning from examples provided that each component conforms to a set of information coding rules we combine a binary vector symbolic architecture vsa with an associative memory known as sparse distributed memory sdm to model context dependent prediction by learning from examples we present simulation results demonstrating that the proposed architecture can enable system interoperability by learning for example by human demonstration communications interoperability learning vector symbolic architecture sparse distributed memory system of systems,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
186,0,171,"The Tarpit -- A general theory of software engineering Context Recent years have seen an increasing interest in general theories of software engineering. As in other academic fields, these theories aim to explain and predict the key phenomena of the discipline. Objective The present article proposes a general theory of software engineering that we have labeled the Tarpit theory, in reference to the 1982 epigram by Alan Perlis. Method An integrative theory development approach was employed to develop the Tarpit theory from four underlying theoretical fields: (i) languages and automata, (ii) cognitive architecture, (iii) problem solving, and (iv) organization structure. Its applicability was explored in three test cases. Results The theory demonstrates an explanatory and predictive potential for a diverse set of software engineering phenomena. It demonstrates a capability of explaining Brooks's law, of making predictions about domain-specific languages, and of evaluating the pros and cons of the practice of continuous integration. Conclusion The presented theory appears capable of explaining and predicting a wide range of software engineering phenomena. Further refinement and application of the theory remains as future work. Software engineering, Theory, Languages and automata, Cognitive architecture, Problem solving, Organization structure",the tarpit a general theory of software engineering context recent years have seen an increasing interest in general theories of software engineering as in other academic fields these theories aim to explain and predict the key phenomena of the discipline objective the present article proposes a general theory of software engineering that we have labeled the tarpit theory in reference to the 1982 epigram by alan perlis method an integrative theory development approach was employed to develop the tarpit theory from four underlying theoretical fields i languages and automata ii cognitive architecture iii problem solving and iv organization structure its applicability was explored in three test cases results the theory demonstrates an explanatory and predictive potential for a diverse set of software engineering phenomena it demonstrates a capability of explaining brooks s law of making predictions about domain specific languages and of evaluating the pros and cons of the practice of continuous integration conclusion the presented theory appears capable of explaining and predicting a wide range of software engineering phenomena further refinement and application of the theory remains as future work software engineering theory languages and automata cognitive architecture problem solving organization structure,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
187,0,236,"The Capabilities of Automated Functional Testing of Programming Assignments In the emerging world of information technologies, a growing number of students is choosing this specialization for their education. Therefore, the number of homework and laboratory research assignments that should be tested is also growing. The majority of these tasks is based on the necessity to implement some algorithm as a small program. This article discusses the possible solutions to the problem of automated testing of programming laboratory research assignments. The course ``Algorithmization and Programming of Solutions'' is offered to all the first-year students of The Faculty of Computer Science and Information Technology (∼500 students) in Riga Technical University and it provides the students the basics of the algorithmization of computing processes and the technology of program design using Java programming language (the given course and the University will be considered as an example of the implementation of the automated testing). During the course eight laboratory research assignments are planned, where the student has to develop an algorithm, create a program and submit it to the education portal of the University. The VBA test program was designed as one of the solutions, the requirements for each laboratory assignment were determined and the special tests have been created. At some point, however, the VBA offered options were no longer able to meet the requirements, therefore the activities on identifying the requirements for the automation of the whole cycle of programming work reception, testing and evaluation have begun. Automation, Assignment, Testing, Continuous Integration , ",the capabilities of automated functional testing of programming assignments in the emerging world of information technologies a growing number of students is choosing this specialization for their education therefore the number of homework and laboratory research assignments that should be tested is also growing the majority of these tasks is based on the necessity to implement some algorithm as a small program this article discusses the possible solutions to the problem of automated testing of programming laboratory research assignments the course algorithmization and programming of solutions is offered to all the first year students of the faculty of computer science and information technology 500 students in riga technical university and it provides the students the basics of the algorithmization of computing processes and the technology of program design using java programming language the given course and the university will be considered as an example of the implementation of the automated testing during the course eight laboratory research assignments are planned where the student has to develop an algorithm create a program and submit it to the education portal of the university the vba test program was designed as one of the solutions the requirements for each laboratory assignment were determined and the special tests have been created at some point however the vba offered options were no longer able to meet the requirements therefore the activities on identifying the requirements for the automation of the whole cycle of programming work reception testing and evaluation have begun automation assignment testing continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
188,0,235,"Making the leap to a software platform strategy: Issues and challenges Context While there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy. Objective This case study provides a comprehensive taxonomy of the challenges faced when a medium-scale organization decided to adopt software platforms. The study also reveals how new trends in software engineering (i.e. agile methods, distributed development, and flat management structures) interplayed with the chosen platform strategy. Method We used an ethnographic approach to collect data by spending time at a medium-scale company in Scandinavia. We conducted 16in-depth interviews with representatives of eight different teams, three of which were working on three separate platforms. The collected data was analyzed using Grounded Theory. Results The findings identify four classes of challenges, namely: business challenges, organizational challenges, technical challenges, and people challenges. The article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support. Conclusion The organization's decision to adopt a software platform strategy introduced a number of challenges. These challenges need to be understood and addressed in order to reap the benefits of reuse. Researchers need to further investigate issues such as supportive organizational structures for platform development, the role of agile methods in software platforms, tool support for testing and continuous integration in the platform context, and reuse recommendation systems. Software platform, Software reuse, Platform challenges, Ethnographic study, Grounded Theory",making the leap to a software platform strategy issues and challenges context while there are many success stories of achieving high reuse and improved quality using software platforms there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy objective this case study provides a comprehensive taxonomy of the challenges faced when a medium scale organization decided to adopt software platforms the study also reveals how new trends in software engineering i e agile methods distributed development and flat management structures interplayed with the chosen platform strategy method we used an ethnographic approach to collect data by spending time at a medium scale company in scandinavia we conducted 16in depth interviews with representatives of eight different teams three of which were working on three separate platforms the collected data was analyzed using grounded theory results the findings identify four classes of challenges namely business challenges organizational challenges technical challenges and people challenges the article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support conclusion the organization s decision to adopt a software platform strategy introduced a number of challenges these challenges need to be understood and addressed in order to reap the benefits of reuse researchers need to further investigate issues such as supportive organizational structures for platform development the role of agile methods in software platforms tool support for testing and continuous integration in the platform context and reuse recommendation systems software platform software reuse platform challenges ethnographic study grounded theory,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
189,0,124,"Advancing analytical algorithms and pipelines for billions of microbial sequences The vast number of microbial sequences resulting from sequencing efforts using new technologies require us to re-assess currently available analysis methodologies and tools. Here we describe trends in the development and distribution of software for analyzing microbial sequence data. We then focus on one widely used set of methods, dimensionality reduction techniques, which allow users to summarize and compare these vast datasets. We conclude by emphasizing the utility of formal software engineering methods for the development of computational biology tools, and the need for new algorithms for comparing microbial communities. Such large-scale comparisons will allow us to fulfill the dream of rapid integration and comparison of microbial sequence data sets, in a replicable analytical environment, in order to describe the microbial world we inhabit. []",advancing analytical algorithms and pipelines for billions of microbial sequences the vast number of microbial sequences resulting from sequencing efforts using new technologies require us to re assess currently available analysis methodologies and tools here we describe trends in the development and distribution of software for analyzing microbial sequence data we then focus on one widely used set of methods dimensionality reduction techniques which allow users to summarize and compare these vast datasets we conclude by emphasizing the utility of formal software engineering methods for the development of computational biology tools and the need for new algorithms for comparing microbial communities such large scale comparisons will allow us to fulfill the dream of rapid integration and comparison of microbial sequence data sets in a replicable analytical environment in order to describe the microbial world we inhabit,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
190,0,275,"Development of combined models to describe the residence time distribution in fluidized-bed bioreactor with light beads The modeling of bioreactors with immobilized cells is a complex process, incorporating in itself different complex operations. Depending on the design of the bioreactor and the biotechnological process are possible different combinations of mathematical models. Knowledge of the kinetics of biological processes and mass transfer are necessary to understand the basic principle of operation of the bioreactor. An important step in developing a complete model of the bioreactor is to explore and describe the flows' structure in the working volume of the apparatus. For reactors with different flow and mixing characteristics are needed different methods for design, modeling and scaling-up. In this work an analysis of the experimental curves to determine the liquid residence time distribution (RTD) of liquid in the bioreactor was made. Based on this analysis, two different combined models of the flow structures were built. The study was done using specialized software RTD 3.14. The estimation of the models parameter was made using least square method. The algorithm permits parallel, serial and other kinds of connections of building blocks, including recycles. Directions of flows between blocks and flow fractions, when necessary, are set by user. The models are characterized with simple structure and with good ability to reproduce experimental results with high accuracy. They were built by plug flow and ideal mixing units, which is a prerequisite for easy and quick integration in the synthesis model of the fermentation process in the bioreactor. The developed combined models for RTD description will be used to establish a system for control of the ethanol fermentation with immobilized cells. Thus, the impact of flow hydrodynamics will be render in account on the fermentation process. immobilized cells, fluidized bed, residence time distribution, combined models, RTD 3.14 , ",development of combined models to describe the residence time distribution in fluidized bed bioreactor with light beads the modeling of bioreactors with immobilized cells is a complex process incorporating in itself different complex operations depending on the design of the bioreactor and the biotechnological process are possible different combinations of mathematical models knowledge of the kinetics of biological processes and mass transfer are necessary to understand the basic principle of operation of the bioreactor an important step in developing a complete model of the bioreactor is to explore and describe the flows structure in the working volume of the apparatus for reactors with different flow and mixing characteristics are needed different methods for design modeling and scaling up in this work an analysis of the experimental curves to determine the liquid residence time distribution rtd of liquid in the bioreactor was made based on this analysis two different combined models of the flow structures were built the study was done using specialized software rtd 3 14 the estimation of the models parameter was made using least square method the algorithm permits parallel serial and other kinds of connections of building blocks including recycles directions of flows between blocks and flow fractions when necessary are set by user the models are characterized with simple structure and with good ability to reproduce experimental results with high accuracy they were built by plug flow and ideal mixing units which is a prerequisite for easy and quick integration in the synthesis model of the fermentation process in the bioreactor the developed combined models for rtd description will be used to establish a system for control of the ethanol fermentation with immobilized cells thus the impact of flow hydrodynamics will be render in account on the fermentation process immobilized cells fluidized bed residence time distribution combined models rtd 3 14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
191,0,99,"Using Web services to integrate heterogeneous simulations in a grid environment The distributed information technologies collectively known as Web services recently have demonstrated powerful capabilities for scalable interoperation of heterogeneous software across a wide variety of networked platforms. This approach supports a rapid integration cycle and shows promise for ultimately supporting automatic composability of services using discovery via registries. This paper presents a rationale for extending Web services to distributed simulation environments, together with a description and examples of the integration methodology used to develop significant prototype implementations, and argues for combining the power of Grid computing with Web services to further expand this demanding computation and database access environment. Web services, Grid computing, Distributed simulation, Composability",using web services to integrate heterogeneous simulations in a grid environment the distributed information technologies collectively known as web services recently have demonstrated powerful capabilities for scalable interoperation of heterogeneous software across a wide variety of networked platforms this approach supports a rapid integration cycle and shows promise for ultimately supporting automatic composability of services using discovery via registries this paper presents a rationale for extending web services to distributed simulation environments together with a description and examples of the integration methodology used to develop significant prototype implementations and argues for combining the power of grid computing with web services to further expand this demanding computation and database access environment web services grid computing distributed simulation composability,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
192,0,383,"Advanced data acquisition system for SEVAN Huge magnetic clouds of plasma emitted by the Sun dominate intense geomagnetic storm occurrences and simultaneously they are correlated with variations of spectra of particles and nuclei in the interplanetary space, ranging from subtermal solar wind ions till GeV energy galactic cosmic rays. For a reliable and fast forecast of Space Weather world-wide networks of particle detectors are operated at different latitudes, longitudes, and altitudes. Based on a new type of hybrid particle detector developed in the context of the International Heliophysical Year (IHY 2007) at Aragats Space Environmental Center (ASEC) we start to prepare hardware and software for the first sites of Space Environmental Viewing and Analysis Network (SEVAN). In the paper the architecture of the newly developed data acquisition system for SEVAN is presented. We plan to run the SEVAN network under one-and-the-same data acquisition system, enabling fast integration of data for on-line analysis of Solar Flare Events. An Advanced Data Acquisition System (ADAS) is designed as a distributed network of uniform components connected by Web Services. Its main component is Unified Readout and Control Server (URCS) which controls the underlying electronics by means of detector specific drivers and makes a preliminary analysis of the on-line data. The lower level components of URCS are implemented in C and a fast binary representation is used for the data exchange with electronics. However, after preprocessing, the data are converted to a self-describing hybrid XML/Binary format. To achieve better reliability all URCS are running on embedded computers without disk and fans to avoid the limited lifetime of moving mechanical parts. The data storage is carried out by means of high performance servers working in parallel to provide data security. These servers are periodically inquiring the data from all URCS and storing it in a MySQL database. The implementation of the control interface is based on high level web standards and, therefore, all properties of the system can be remotely managed and monitored by the operators using web browsers. The advanced data acquisition system at ASEC in Armenia was started in November, 2006. The reliability of the multi-client service was proven by continuously monitoring neutral and charged cosmic ray particles. Seven particle monitors are located at 2000 and 3200m above sea level at a distance of 40 and 60km from the main data server. Data acquisition, Particle monitor, Detector networks, IHY",advanced data acquisition system for sevan huge magnetic clouds of plasma emitted by the sun dominate intense geomagnetic storm occurrences and simultaneously they are correlated with variations of spectra of particles and nuclei in the interplanetary space ranging from subtermal solar wind ions till gev energy galactic cosmic rays for a reliable and fast forecast of space weather world wide networks of particle detectors are operated at different latitudes longitudes and altitudes based on a new type of hybrid particle detector developed in the context of the international heliophysical year ihy 2007 at aragats space environmental center asec we start to prepare hardware and software for the first sites of space environmental viewing and analysis network sevan in the paper the architecture of the newly developed data acquisition system for sevan is presented we plan to run the sevan network under one and the same data acquisition system enabling fast integration of data for on line analysis of solar flare events an advanced data acquisition system adas is designed as a distributed network of uniform components connected by web services its main component is unified readout and control server urcs which controls the underlying electronics by means of detector specific drivers and makes a preliminary analysis of the on line data the lower level components of urcs are implemented in c and a fast binary representation is used for the data exchange with electronics however after preprocessing the data are converted to a self describing hybrid xml binary format to achieve better reliability all urcs are running on embedded computers without disk and fans to avoid the limited lifetime of moving mechanical parts the data storage is carried out by means of high performance servers working in parallel to provide data security these servers are periodically inquiring the data from all urcs and storing it in a mysql database the implementation of the control interface is based on high level web standards and therefore all properties of the system can be remotely managed and monitored by the operators using web browsers the advanced data acquisition system at asec in armenia was started in november 2006 the reliability of the multi client service was proven by continuously monitoring neutral and charged cosmic ray particles seven particle monitors are located at 2000 and 3200m above sea level at a distance of 40 and 60km from the main data server data acquisition particle monitor detector networks ihy,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
193,0,283,"Successful extreme programming: Fidelity to the methodology or good teamworking? Context Developing a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team's success? Perhaps the more specific software engineering team factors, for example the agile development method's collective ownership and code management, are decisive. Objective To assess the contribution of agile methodology, agile-specific team methods, and general team factors in the performance of software teams. Method We studied 40 small-scale software development teams which used Extreme Programming (XP). We measured (1) the teams' adherence to XP methods, (2) their use of XP-specific team practices, and (3) standard team attributes, as well as the quality of the project's outcomes. We used Williams et al.'s (2004a) [33] Shodan measures of XP methods, and regression analysis. Results All three types of variables are associated with the project's performance. Teamworking is important but it is the XP-specific team factor (continuous integration, coding standards, and collective code ownership) that is significant. Only customer planning (release planning/planning game, customer access, short releases, and stand-up meeting) is positively related to performance. A negative relationship between foundations (automated unit tests, customer acceptance tests, test-first design, pair programming, and refactoring) is found and is moderated by craftsmanship (sustainable pace, simple design, and metaphor/system of names). Of the general team factors only cooperation is related to performance. Cooperation mediates the relationship between the XP-specific team factor and performance. Conclusion Client and team foci of the XP method are its critical active ingredients. Software development, Extreme programming, Agile methods, Teamwork, Cooperation, Performance",successful extreme programming fidelity to the methodology or good teamworking context developing a theory of agile technology in combination with empirical work must include assessing its performance effects and whether all or some of its key ingredients account for any performance advantage over traditional methods given the focus on teamwork is the agile technology what really matters or do general team factors such as cohesion primarily account for a team s success perhaps the more specific software engineering team factors for example the agile development method s collective ownership and code management are decisive objective to assess the contribution of agile methodology agile specific team methods and general team factors in the performance of software teams method we studied 40 small scale software development teams which used extreme programming xp we measured 1 the teams adherence to xp methods 2 their use of xp specific team practices and 3 standard team attributes as well as the quality of the project s outcomes we used williams et al s 2004a 33 shodan measures of xp methods and regression analysis results all three types of variables are associated with the project s performance teamworking is important but it is the xp specific team factor continuous integration coding standards and collective code ownership that is significant only customer planning release planning planning game customer access short releases and stand up meeting is positively related to performance a negative relationship between foundations automated unit tests customer acceptance tests test first design pair programming and refactoring is found and is moderated by craftsmanship sustainable pace simple design and metaphor system of names of the general team factors only cooperation is related to performance cooperation mediates the relationship between the xp specific team factor and performance conclusion client and team foci of the xp method are its critical active ingredients software development extreme programming agile methods teamwork cooperation performance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
194,0,129,"Adaptive Robot Design and Applications in Flexible Manufacturing Environments Robots have played a very important role in the growing popular flexible manufacturing environments. However, state-of-art industrial robots with high accuracy are rather costly and static. Our works aims at providing a low-cost fast integrating platform with advanced middleware support to seamlessly integrate off-the-shelf or future robot sensors, robots, and actuators as well as industrial IT system. To support such approach, a component-based reconfigurable middleware system is designed. A system runtime service is employed to manage the dependence and whole lifecycle of realtime components by reasoning from component's contract-based service description. A continues deployment mechanism is also designed The software architecture was implemented by so called -- Hybrid component model. The evaluation shows that the ARFLEX system achieve the goal of enhance in accuracy, flexibility while provide good real-time characteristics. []",adaptive robot design and applications in flexible manufacturing environments robots have played a very important role in the growing popular flexible manufacturing environments however state of art industrial robots with high accuracy are rather costly and static our works aims at providing a low cost fast integrating platform with advanced middleware support to seamlessly integrate off the shelf or future robot sensors robots and actuators as well as industrial it system to support such approach a component based reconfigurable middleware system is designed a system runtime service is employed to manage the dependence and whole lifecycle of realtime components by reasoning from component s contract based service description a continues deployment mechanism is also designed the software architecture was implemented by so called hybrid component model the evaluation shows that the arflex system achieve the goal of enhance in accuracy flexibility while provide good real time characteristics,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
195,0,210,"Live 3-Dimensional Transesophageal Echocardiography: Initial Experience Using the Fully-Sampled Matrix Array Probe Objectives Our study goals were to evaluate the 3-dimensional matrix array transesophageal echocardiographic (3D-MTEE) probe by assessing the image quality of native valves and other intracardiac structures. Background Because 3-dimensional transesophageal echocardiography with gated rotational acquisition is not used routinely as the result of artifacts, lengthy acquisition, and processing, a 3D-MTEE probe was developed (Philips Medical Systems, Andover, Massachusetts). Methods In 211 patients, 3D-MTEE zoom images of the mitral valve (MV), aortic valve, tricuspid valve, interatrial septum, and left atrial appendage were obtained, followed by a left ventricular wide-angled acquisition. Images were reviewed and graded off-line (Xcelera with QLAB software, Philips Medical Systems). Results Excellent visualization of the MV (85% to 91% for all scallops of both MV leaflets), interatrial septum (84%), left atrial appendage (86%), and left ventricle (77%) was observed. Native aortic and tricuspid valves were optimally visualized only in 18% and 11% of patients, respectively. Conclusions The use of 3D-MTEE imaging, which is feasible in most patients, provides superb imaging of native MVs, which makes this modality an excellent choice for MV surgical planning and guidance of percutaneous interventions. Optimal aortic and tricuspid valve imaging will depend on further technological developments. Fast acquisition and immediate online display will facilitate wider acceptance and routine use in clinical practice. transesophageal echocardiography, 3-dimensional echocardiography, intraoperative echocardiography, mitral valve, valvular disease",live 3 dimensional transesophageal echocardiography initial experience using the fully sampled matrix array probe objectives our study goals were to evaluate the 3 dimensional matrix array transesophageal echocardiographic 3d mtee probe by assessing the image quality of native valves and other intracardiac structures background because 3 dimensional transesophageal echocardiography with gated rotational acquisition is not used routinely as the result of artifacts lengthy acquisition and processing a 3d mtee probe was developed philips medical systems andover massachusetts methods in 211 patients 3d mtee zoom images of the mitral valve mv aortic valve tricuspid valve interatrial septum and left atrial appendage were obtained followed by a left ventricular wide angled acquisition images were reviewed and graded off line xcelera with qlab software philips medical systems results excellent visualization of the mv 85 to 91 for all scallops of both mv leaflets interatrial septum 84 left atrial appendage 86 and left ventricle 77 was observed native aortic and tricuspid valves were optimally visualized only in 18 and 11 of patients respectively conclusions the use of 3d mtee imaging which is feasible in most patients provides superb imaging of native mvs which makes this modality an excellent choice for mv surgical planning and guidance of percutaneous interventions optimal aortic and tricuspid valve imaging will depend on further technological developments fast acquisition and immediate online display will facilitate wider acceptance and routine use in clinical practice transesophageal echocardiography 3 dimensional echocardiography intraoperative echocardiography mitral valve valvular disease,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
196,0,307,"An IT perspective on integrated environmental modelling: The SIAT case Policy makers have a growing interest in integrated assessments of policies. The Integrated Assessment Modelling (IAM) community is reacting to this interest by extending the application of model development from pure scientific analysis towards application in decision making or policy context by giving tools a higher capability for analysis targeted at non-experts, but intelligent users. Many parties are involved in the construction of such tools including modellers, domain experts and tool users, resulting in as many views on the proposed tool. During tool development research continues which leads to advanced understanding of the system and may alter early specifications. Accumulation of changes to the initial design obscures the design, usually vastly increasing the number of defects in the software. The software engineering community uses concepts, methods and practices to deal with ambiguous specifications, changing requirements and incompletely conceived visions, and to design and develop maintainable/extensible quality software. The aim of this paper is to introduce modellers to software engineering concepts and methods which have the potential to improve model and tool development using experiences from the development of the Sustainability Impact Assessment Tool. These range from choosing a software development methodology for planning activities and coordinating people, technical design principles impacting maintainability, quality and reusability of the software to prototyping and user involvement. It is argued that adaptive development methods seem to best fit research projects, that typically have unclear upfront and changing requirements. The break-down of a system into elements that overlap as little as possible in features and behaviour helps to divide the work across teams and to achieve a modular and flexible system. However, this must be accompanied by proper automated testing methods and automated continuous integration of the elements. Prototypes, screen sketches and mock-ups are useful to align the different views, build a shared vision of required functionality and to match expectations. Software development process, Software architecture, Modelling, Integrated assessment, Assessment tool",an it perspective on integrated environmental modelling the siat case policy makers have a growing interest in integrated assessments of policies the integrated assessment modelling iam community is reacting to this interest by extending the application of model development from pure scientific analysis towards application in decision making or policy context by giving tools a higher capability for analysis targeted at non experts but intelligent users many parties are involved in the construction of such tools including modellers domain experts and tool users resulting in as many views on the proposed tool during tool development research continues which leads to advanced understanding of the system and may alter early specifications accumulation of changes to the initial design obscures the design usually vastly increasing the number of defects in the software the software engineering community uses concepts methods and practices to deal with ambiguous specifications changing requirements and incompletely conceived visions and to design and develop maintainable extensible quality software the aim of this paper is to introduce modellers to software engineering concepts and methods which have the potential to improve model and tool development using experiences from the development of the sustainability impact assessment tool these range from choosing a software development methodology for planning activities and coordinating people technical design principles impacting maintainability quality and reusability of the software to prototyping and user involvement it is argued that adaptive development methods seem to best fit research projects that typically have unclear upfront and changing requirements the break down of a system into elements that overlap as little as possible in features and behaviour helps to divide the work across teams and to achieve a modular and flexible system however this must be accompanied by proper automated testing methods and automated continuous integration of the elements prototypes screen sketches and mock ups are useful to align the different views build a shared vision of required functionality and to match expectations software development process software architecture modelling integrated assessment assessment tool,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
197,0,81,"Research and Practice on ``Triple-driven'' Based Software Development Practical Teaching System According to the situation that the IT students can not meet the software industry demand for qualified personnel, a ``triple-driven'' three-dimensional software development practical teaching system was proposed, aiming to improve the software development capabilities and innovation sense of students. This system can effectively improve students the interest of software development and the practical skills and sense of innovation, laying a solid foundation for student after graduation to rapidly integrate into the software development process, meeting the needs of software industry. triple-driven, practical teaching, software development",research and practice on triple driven based software development practical teaching system according to the situation that the it students can not meet the software industry demand for qualified personnel a triple driven three dimensional software development practical teaching system was proposed aiming to improve the software development capabilities and innovation sense of students this system can effectively improve students the interest of software development and the practical skills and sense of innovation laying a solid foundation for student after graduation to rapidly integrate into the software development process meeting the needs of software industry triple driven practical teaching software development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
198,1,141,"Visualizing Testing Activities to Support Continuous Integration: A Multiple Case Study While efficient testing arrangements are the key for software companies that are striving for continuous integration, most companies struggle with arranging these highly complex and interconnected testing activities. There is often a lack of an adequate overview of companies' end-to-end testing activities, which tend to lead to problems such as double work, slow feedback loops, too many issues found during post-development, disconnected organizations, and unpredictable release schedules. We report from a multiple-case study in which we explore current testing arrangements at five different software development sites. The outcome of the study is a visualization technique of the testing activities involved from unit and component level to product and release level that support the identification of improvement areas. This model for visualizing the end-to-end testing activities for a system has been used to visualize these five cases and has been validated empirically. continuous integration,  software testing,  visualization",visualizing testing activities to support continuous integration a multiple case study while efficient testing arrangements are the key for software companies that are striving for continuous integration most companies struggle with arranging these highly complex and interconnected testing activities there is often a lack of an adequate overview of companies end to end testing activities which tend to lead to problems such as double work slow feedback loops too many issues found during post development disconnected organizations and unpredictable release schedules we report from a multiple case study in which we explore current testing arrangements at five different software development sites the outcome of the study is a visualization technique of the testing activities involved from unit and component level to product and release level that support the identification of improvement areas this model for visualizing the end to end testing activities for a system has been used to visualize these five cases and has been validated empirically continuous integration software testing visualization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
199,1,221,"Feature toggles: practitioner practices and a case study Continuous delivery and rapid releases have led to innovative techniques for integrating new features and bug fixes into a new release faster. To reduce the probability of integration conflicts, major software companies, including Google, Facebook and Netflix, use feature toggles to incrementally integrate and test new features instead of integrating the feature only when it's ready. Even after release, feature toggles allow operations managers to quickly disable a new feature that is behaving erratically or to enable certain features only for certain groups of customers. Since literature on feature toggles is surprisingly slim, this paper tries to understand the prevalence and impact of feature toggles. First, we conducted a quantitative analysis of feature toggle usage across 39 releases of Google Chrome (spanning five years of release history). Then, we studied the technical debt involved with feature toggles by mining a spreadsheet used by Google developers for feature toggle maintenance. Finally, we performed thematic analysis of videos and blog posts of release engineers at major software companies in order to further understand the strengths and drawbacks of feature toggles in practice. We also validated our findings with four Google developers. We find that toggles can reconcile rapid releases with long-term feature development and allow flexible control over which features to deploy. However they also introduce technical debt and additional maintenance for developers. []",feature toggles practitioner practices and a case study continuous delivery and rapid releases have led to innovative techniques for integrating new features and bug fixes into a new release faster to reduce the probability of integration conflicts major software companies including google facebook and netflix use feature toggles to incrementally integrate and test new features instead of integrating the feature only when it s ready even after release feature toggles allow operations managers to quickly disable a new feature that is behaving erratically or to enable certain features only for certain groups of customers since literature on feature toggles is surprisingly slim this paper tries to understand the prevalence and impact of feature toggles first we conducted a quantitative analysis of feature toggle usage across 39 releases of google chrome spanning five years of release history then we studied the technical debt involved with feature toggles by mining a spreadsheet used by google developers for feature toggle maintenance finally we performed thematic analysis of videos and blog posts of release engineers at major software companies in order to further understand the strengths and drawbacks of feature toggles in practice we also validated our findings with four google developers we find that toggles can reconcile rapid releases with long term feature development and allow flexible control over which features to deploy however they also introduce technical debt and additional maintenance for developers,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
200,1,234,"POD-Diagnosis: Error Diagnosis of Sporadic Operations on Cloud Applications Applications in the cloud are subject to sporadic changes due to operational activities such as upgrade, redeployment, and on-demand scaling. These operations are also subject to interferences from other simultaneous operations. Increasing the dependability of these sporadic operations is non-trivial, particularly since traditional anomaly-detection-based diagnosis techniques are less effective during sporadic operation periods. A wide range of legitimate changes confound anomaly diagnosis and make baseline establishment for ""normal"" operation difficult. The increasing frequency of these sporadic operations (e.g. due to continuous deployment) is exacerbating the problem. Diagnosing failures during sporadic operations relies heavily on logs, while log analysis challenges stemming from noisy, inconsistent and voluminous logs from multiple sources remain largely unsolved. In this paper, we propose Process Oriented Dependability (POD)-Diagnosis, an approach that explicitly models these sporadic operations as processes. These models allow us to (i) determine orderly execution of the process, and (ii) use the process context to filter logs, trigger assertion evaluations, visit fault trees and perform on-demand assertion evaluation for online error diagnosis and root cause analysis. We evaluated the approach on rolling upgrade operations in Amazon Web Services (A WS) while performing other simultaneous operations. During our evaluation, we correctly detected all of the 160 injected faults, as well as 46 interferences caused by concurrent operations. We did this with 91.95% precision. Of the correctly detected faults, the accuracy rate of error diagnosis is 96.55%. system administration, cloud, deployment, process mining, error detection, error diagnosis, DevOps",pod diagnosis error diagnosis of sporadic operations on cloud applications applications in the cloud are subject to sporadic changes due to operational activities such as upgrade redeployment and on demand scaling these operations are also subject to interferences from other simultaneous operations increasing the dependability of these sporadic operations is non trivial particularly since traditional anomaly detection based diagnosis techniques are less effective during sporadic operation periods a wide range of legitimate changes confound anomaly diagnosis and make baseline establishment for normal operation difficult the increasing frequency of these sporadic operations e g due to continuous deployment is exacerbating the problem diagnosing failures during sporadic operations relies heavily on logs while log analysis challenges stemming from noisy inconsistent and voluminous logs from multiple sources remain largely unsolved in this paper we propose process oriented dependability pod diagnosis an approach that explicitly models these sporadic operations as processes these models allow us to i determine orderly execution of the process and ii use the process context to filter logs trigger assertion evaluations visit fault trees and perform on demand assertion evaluation for online error diagnosis and root cause analysis we evaluated the approach on rolling upgrade operations in amazon web services a ws while performing other simultaneous operations during our evaluation we correctly detected all of the 160 injected faults as well as 46 interferences caused by concurrent operations we did this with 91 95 precision of the correctly detected faults the accuracy rate of error diagnosis is 96 55 system administration cloud deployment process mining error detection error diagnosis devops,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
201,1,115,"Fast Feedback from Automated Tests Executed with the Product Build Nowadays Continuous Integration (CI) is a very common practice with many advantages and it is used in many software projects. For large software projects checking out the source code, building the product and testing the product build via automated tests during CI can take a long time (e.g. many hours). So the software developers do not get fast feedback about their changes. Often the test report contains the results of many changes from several software developers or the feedback is not accurate enough according to the developer's source code changes. This paper describes a novel approach to reduce the feedback time and to provide test results for only these changes the developer has committed. Continuous integration,  Automated testing,  Test case prioritization",fast feedback from automated tests executed with the product build nowadays continuous integration ci is a very common practice with many advantages and it is used in many software projects for large software projects checking out the source code building the product and testing the product build via automated tests during ci can take a long time e g many hours so the software developers do not get fast feedback about their changes often the test report contains the results of many changes from several software developers or the feedback is not accurate enough according to the developer s source code changes this paper describes a novel approach to reduce the feedback time and to provide test results for only these changes the developer has committed continuous integration automated testing test case prioritization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
202,1,139,"Composing Patterns to Construct Secure Systems Building secure applications requires significant expertise. Secure platforms and security patterns have been proposed to alleviate this problem. However, correctly applying patterns to use platform features is still highly expertise-dependent. Patterns are informal and there is a gap between them and platform features. We propose the concept of reusable verified design fragments, which package security patterns and platform features and are verified to provide assurance about their security properties. Design fragments can be composed through four primitive tactics. The verification of the composed design against desired security properties is presented in an assurance case. We demonstrate our approach by securing a Continuous Deployment pipeline and show that the tactics are sufficient to compose design fragments into a secure system. Finally, we formally define composition tactics, which are intended to support the development of systems that are secure by construction. security, verification, patterns, composition, assurance",composing patterns to construct secure systems building secure applications requires significant expertise secure platforms and security patterns have been proposed to alleviate this problem however correctly applying patterns to use platform features is still highly expertise dependent patterns are informal and there is a gap between them and platform features we propose the concept of reusable verified design fragments which package security patterns and platform features and are verified to provide assurance about their security properties design fragments can be composed through four primitive tactics the verification of the composed design against desired security properties is presented in an assurance case we demonstrate our approach by securing a continuous deployment pipeline and show that the tactics are sufficient to compose design fragments into a secure system finally we formally define composition tactics which are intended to support the development of systems that are secure by construction security verification patterns composition assurance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
203,1,88,"Requirements to Pervasive System Continuous Deployment Pervasive applications present stringent requirements that make their deployment especially challenging. The unknown and fluctuating environment in which pervasive applications are executed makes traditional approaches not suitable. In addition, the current trend to build applications out of separated components and services makes the deployment process inherently continuous and dynamic. In the last years, we developed several industrial pervasive platforms and applications. From these experiences, we identified ten requirements vital to support the continuous deployment of pervasive systems. In this paper we present these requirements and the associated challenges. Pervasive Computing,  Continuous Deployment,  Dynamism,  Requirements",requirements to pervasive system continuous deployment pervasive applications present stringent requirements that make their deployment especially challenging the unknown and fluctuating environment in which pervasive applications are executed makes traditional approaches not suitable in addition the current trend to build applications out of separated components and services makes the deployment process inherently continuous and dynamic in the last years we developed several industrial pervasive platforms and applications from these experiences we identified ten requirements vital to support the continuous deployment of pervasive systems in this paper we present these requirements and the associated challenges pervasive computing continuous deployment dynamism requirements,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
204,1,148,"Supporting Continuous Integration by Code-churn Based Test Selection Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration. []",supporting continuous integration by code churn based test selection continuous integration promises advantages in large scale software development by enabling software development organizations to deliver new functions faster however implementing continuous integration in large software development organizations is challenging because of organizational social and technical reasons one of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible in our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level the method is based on analysis of correlations between test case failures and source code changes and is evaluated by combining semi structured interviews and workshops with practitioners at ericsson and axis communications in sweden the results show that using measures of precision and recall the test cases can be prioritized the prioritization leads to finding an optimal test suite to execute before the integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
205,1,168,"Towards Post-agile Development Practices Through Productized Development Infrastructure Modern software is developed to meet evolving customer needs in a timely fashion. The need for a rapid time-to-market together with changing requirements has led software intensive companies to utilize agile development, where each iteration aims at producing end-user value and change is embraced. In today's post-agile software development world, there is a need for processes and tools that deliver new software to the end-user as fast as possible. The level of adoption of these continuous software engineering practices depends on the product, customers, and the business domain. In this paper, we investigate the benefits gained from implementing a completely continuous delivery workflow using a domain specific productized development infrastructure through a descriptive single case study. Embracing the continuous delivery mindset throughout the development pipeline allows the case customer company to gain fast insight on new business directions and lends the services to live experimentation which in turn adds to end-user value. Up-to-date feedback cycles between all stakeholders all the way from concept design to end-users are offered. continuous delivery, continuous software engineering, development infrastructure",towards post agile development practices through productized development infrastructure modern software is developed to meet evolving customer needs in a timely fashion the need for a rapid time to market together with changing requirements has led software intensive companies to utilize agile development where each iteration aims at producing end user value and change is embraced in today s post agile software development world there is a need for processes and tools that deliver new software to the end user as fast as possible the level of adoption of these continuous software engineering practices depends on the product customers and the business domain in this paper we investigate the benefits gained from implementing a completely continuous delivery workflow using a domain specific productized development infrastructure through a descriptive single case study embracing the continuous delivery mindset throughout the development pipeline allows the case customer company to gain fast insight on new business directions and lends the services to live experimentation which in turn adds to end user value up to date feedback cycles between all stakeholders all the way from concept design to end users are offered continuous delivery continuous software engineering development infrastructure,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
206,1,127,"Automated testing in the continuous delivery pipeline: A case study of an online company Companies running an online business need to be able to frequently push new features and bug fixes from development into production. Successful high-performance online companies deliver code changes often several times per day. Their continuous delivery model supports the business needs of the online world. At the same time, however, such practices increase the risk of introducing quality issues and unwanted side effects. Rigorous test automation is therefore a key success factor for continuous delivery. In this paper we describe how automated testing is used in the continuous delivery pipeline of an Austrian online business company. The paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline. automated testing, continuous integration, continuous delivery, continusous deployment, dev ops",automated testing in the continuous delivery pipeline a case study of an online company companies running an online business need to be able to frequently push new features and bug fixes from development into production successful high performance online companies deliver code changes often several times per day their continuous delivery model supports the business needs of the online world at the same time however such practices increase the risk of introducing quality issues and unwanted side effects rigorous test automation is therefore a key success factor for continuous delivery in this paper we describe how automated testing is used in the continuous delivery pipeline of an austrian online business company the paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline automated testing continuous integration continuous delivery continusous deployment dev ops,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
207,1,105,"Social testing: A framework to support adoption of continuous delivery by small medium enterprises Continuous delivery (CD) represents a challenge for software test teams, because of the continuous introduction of new features and feedback from customers. We consider testing in a framework where users are encouraged to report defects through social or other incentive schemes. Using an enterprise dataset, we address the question of which types of defects can best be found in the field, allowing in-house test resources to be refocused. Validation of these touch points ultimately interweaves both customer and business needs. The proposed framework is one which can help small to medium software businesses, which typically have limited resources to test and release software via CD. []",social testing a framework to support adoption of continuous delivery by small medium enterprises continuous delivery cd represents a challenge for software test teams because of the continuous introduction of new features and feedback from customers we consider testing in a framework where users are encouraged to report defects through social or other incentive schemes using an enterprise dataset we address the question of which types of defects can best be found in the field allowing in house test resources to be refocused validation of these touch points ultimately interweaves both customer and business needs the proposed framework is one which can help small to medium software businesses which typically have limited resources to test and release software via cd,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
208,1,95,"DevOps: A Definition and Perceived Adoption Impediments As the interest in DevOps continues to grow, there is an increasing need for software organizations to understand how to adopt it successfully. This study has as objective to clarify the concept and provide insight into existing challenges of adopting DevOps. First, the existing literature is reviewed. A definition of DevOps is then formed based on the literature by breaking down the concept into its defining characteristics. We interview 13 subjects in a software company adopting DevOps and, finally, we present 11 impediments for the company's DevOps adoption that were identified based on the interviews. Cloud Service,  Cultural Aspect,  Service Failure,  Software Process Improvement,  Continuous Delivery",devops a definition and perceived adoption impediments as the interest in devops continues to grow there is an increasing need for software organizations to understand how to adopt it successfully this study has as objective to clarify the concept and provide insight into existing challenges of adopting devops first the existing literature is reviewed a definition of devops is then formed based on the literature by breaking down the concept into its defining characteristics we interview 13 subjects in a software company adopting devops and finally we present 11 impediments for the company s devops adoption that were identified based on the interviews cloud service cultural aspect service failure software process improvement continuous delivery,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
209,1,160,"Rondo: A Tool Suite for Continuous Deployment in Dynamic Environments Driven by the emergence of new computing environments, dynamically evolving software systems makes it impossible for developers to deploy software with human-centric processes. Instead, there is an increasing need for automation tools that continuously deploy software into execution, in order to push updates or adapt existing software regarding contextual and business changes. Existing solutions fall short on providing fault-tolerant, reproducible deployments that can scale on heterogeneous environments. In this paper we present Rondo, a tool suite that enables continuous deployment for dynamic, service-oriented applications. At the center of these tools, we propose a deterministic and idem potent deployment process. We provide with Rondo a deployment manager that implements this process and capable of conducting deployments and continuously adapting applications according to the changes in the current target platform. The tool suite also includes a domain-specific language for describing deployment requests. We validate our approach in multiple projects, for provisioning the platform as well as for installing applications and continuous reconfigurations. Dynamism, Continuous Deployment, Service-Oriented Computing",rondo a tool suite for continuous deployment in dynamic environments driven by the emergence of new computing environments dynamically evolving software systems makes it impossible for developers to deploy software with human centric processes instead there is an increasing need for automation tools that continuously deploy software into execution in order to push updates or adapt existing software regarding contextual and business changes existing solutions fall short on providing fault tolerant reproducible deployments that can scale on heterogeneous environments in this paper we present rondo a tool suite that enables continuous deployment for dynamic service oriented applications at the center of these tools we propose a deterministic and idem potent deployment process we provide with rondo a deployment manager that implements this process and capable of conducting deployments and continuously adapting applications according to the changes in the current target platform the tool suite also includes a domain specific language for describing deployment requests we validate our approach in multiple projects for provisioning the platform as well as for installing applications and continuous reconfigurations dynamism continuous deployment service oriented computing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
210,1,70,"DevOps: Making It Easy to Do the Right Thing Wotif Group used DevOps principles to recover from the downward spiral of manual release activity that many IT departments face. Its approach involved the idea of ""making it easy to do the right thing."" By defining the right thing (deployment standards) for development and operations teams and making it easy to adopt, Wotif drastically improved the average release cycle time. This article is part of a theme issue on DevOps. software release, software delivery, software release management and delivery, Internet, e-commerce, DevOps, continuous delivery, continuous deployment, Wotif Group:software development, software engineering",devops making it easy to do the right thing wotif group used devops principles to recover from the downward spiral of manual release activity that many it departments face its approach involved the idea of making it easy to do the right thing by defining the right thing deployment standards for development and operations teams and making it easy to adopt wotif drastically improved the average release cycle time this article is part of a theme issue on devops software release software delivery software release management and delivery internet e commerce devops continuous delivery continuous deployment wotif group software development software engineering,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
211,1,191,"Continuous deployment at Facebook and OANDA Continuous deployment is the software engineering practice of deploying many small incremental software updates into production, leading to a continuous stream of 10s, 100s, or even 1,000s of deployments per day. High-profile Internet firms such as Amazon, Etsy, Facebook, Flickr, Google, and Netflix have embraced continuous deployment. However, the practice has not been covered in textbooks and no scientific publication has presented an analysis of continuous deployment. In this paper, we describe the continuous deployment practices at two very different firms: Facebook and OANDA. We show that continuous deployment does not inhibit productivity or quality even in the face of substantial engineering team and code size growth. To the best of our knowledge, this is the first study to show it is possible to scale the size of an engineering team by 20X and the size of the code base by 50X without negatively impacting developer productivity or software quality. Our experience suggests that top-level management support of continuous deployment is necessary, and that given a choice, developers prefer faster deployment. We identify elements we feel make continuous deployment viable and present observations from operating in a continuous deployment environment. []",continuous deployment at facebook and oanda continuous deployment is the software engineering practice of deploying many small incremental software updates into production leading to a continuous stream of 10s 100s or even 1 000s of deployments per day high profile internet firms such as amazon etsy facebook flickr google and netflix have embraced continuous deployment however the practice has not been covered in textbooks and no scientific publication has presented an analysis of continuous deployment in this paper we describe the continuous deployment practices at two very different firms facebook and oanda we show that continuous deployment does not inhibit productivity or quality even in the face of substantial engineering team and code size growth to the best of our knowledge this is the first study to show it is possible to scale the size of an engineering team by 20x and the size of the code base by 50x without negatively impacting developer productivity or software quality our experience suggests that top level management support of continuous deployment is necessary and that given a choice developers prefer faster deployment we identify elements we feel make continuous deployment viable and present observations from operating in a continuous deployment environment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
212,1,150,"Towards DevOps in the Embedded Systems Domain: Why is It So Hard? DevOps is a predominant phenomenon in the web domain. Its two core principles emphasize collaboration between software development and operations, and the use of agile principles to manage deployment environments and their configurations. DevOps techniques, such as collaboration and behaviour-driven monitoring, have been used by web companies to facilitate continuous deployment of new functionality to customers. The techniques may also offer opportunities for continuous product improvement when adopted in the embedded systems domain. However, certain characteristics of embedded software development present obstacles for DevOps adoption, and as yet, there is no empirical evidence of its adoption in the embedded systems domain. In this study, we present the challenges for DevOps adoption in embedded systems using a multiple-case study approach with four companies. The contribution of this paper is to introduce the concept of DevOps adoption in the embedded systems domain and then to identify key challenges for the DevOps adoption. DevOps, continuous deployment, embedded systems",towards devops in the embedded systems domain why is it so hard devops is a predominant phenomenon in the web domain its two core principles emphasize collaboration between software development and operations and the use of agile principles to manage deployment environments and their configurations devops techniques such as collaboration and behaviour driven monitoring have been used by web companies to facilitate continuous deployment of new functionality to customers the techniques may also offer opportunities for continuous product improvement when adopted in the embedded systems domain however certain characteristics of embedded software development present obstacles for devops adoption and as yet there is no empirical evidence of its adoption in the embedded systems domain in this study we present the challenges for devops adoption in embedded systems using a multiple case study approach with four companies the contribution of this paper is to introduce the concept of devops adoption in the embedded systems domain and then to identify key challenges for the devops adoption devops continuous deployment embedded systems,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
213,1,242,"Techniques for improving regression testing in continuous integration development environments In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost-effective. In an initial pre-submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost-effective. In a subsequent post-submit phase of testing, where dependent modules as well as changed modules are tested, we use test case prioritization techniques to ensure that failures are reported more quickly. In both cases, the techniques we utilize are novel, involving algorithms that are relatively inexpensive and do not rely on code coverage information -- two requirements for conducting testing cost-effectively in this context. To evaluate our approach, we conducted an empirical study on a large data set from Google that we make publicly available. The results of our study show that our selection and prioritization techniques can each lead to cost-effectiveness improvements in the continuous integration process. Regression Testing,  Test Case Prioritization,  Continuous Integration,  Regression Test Selection",techniques for improving regression testing in continuous integration development environments in continuous integration development environments software engineers frequently integrate new or changed code with the mainline codebase this can reduce the amount of code rework that is needed as systems evolve and speed up development time while continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable in this work we present algorithms that make continuous integration processes more cost effective in an initial pre submit phase of testing developers specify modules to be tested and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost effective in a subsequent post submit phase of testing where dependent modules as well as changed modules are tested we use test case prioritization techniques to ensure that failures are reported more quickly in both cases the techniques we utilize are novel involving algorithms that are relatively inexpensive and do not rely on code coverage information two requirements for conducting testing cost effectively in this context to evaluate our approach we conducted an empirical study on a large data set from google that we make publicly available the results of our study show that our selection and prioritization techniques can each lead to cost effectiveness improvements in the continuous integration process regression testing test case prioritization continuous integration regression test selection,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
214,1,219,"CiCUTS: Combining System Execution Modeling Tools with Continuous Integration Environments System execution modeling (SEM) tools provide an effective means to evaluate the quality of service (QoS) of enterprise distributed real-time and embedded (DRE) systems. SEM tools facilitate testing and resolving performance issues throughout the entire development life-cycle, rather than waiting until final system integration. SEM tools have not historically focused on effective testing. New techniques are therefore needed to help bridge the gap between the early integration capabilities of SEM tools and testing so developers can focus on resolving strategic integration and performance issues, as opposed to wrestling with tedious and error-prone low-level testing concerns. This paper provides two contributions to research on using SEM tools to address enterprise DRE system integration challenges. First, we evaluate several approaches for combining continuous integration environments with SEM tools and describe CiCUTS, which combines the CUTS SEM tool with the CruiseControl.NET continuous integration environment. Second, we present a case study that shows how CiCUTS helps reduce the time and effort required to manage and execute integration tests that evaluate QoS metrics for a representative DRE system from the domain of shipboard computing. The results of our case study show that CiCUTS helps developers and testers ensure the performance of an example enterprise DRE system is within its QoS specifications throughout development, instead of waiting until system integration time to evaluate QoS. continuous integration, integration testing, serialized phasing, system execution modeling",cicuts combining system execution modeling tools with continuous integration environments system execution modeling sem tools provide an effective means to evaluate the quality of service qos of enterprise distributed real time and embedded dre systems sem tools facilitate testing and resolving performance issues throughout the entire development life cycle rather than waiting until final system integration sem tools have not historically focused on effective testing new techniques are therefore needed to help bridge the gap between the early integration capabilities of sem tools and testing so developers can focus on resolving strategic integration and performance issues as opposed to wrestling with tedious and error prone low level testing concerns this paper provides two contributions to research on using sem tools to address enterprise dre system integration challenges first we evaluate several approaches for combining continuous integration environments with sem tools and describe cicuts which combines the cuts sem tool with the cruisecontrol net continuous integration environment second we present a case study that shows how cicuts helps reduce the time and effort required to manage and execute integration tests that evaluate qos metrics for a representative dre system from the domain of shipboard computing the results of our case study show that cicuts helps developers and testers ensure the performance of an example enterprise dre system is within its qos specifications throughout development instead of waiting until system integration time to evaluate qos continuous integration integration testing serialized phasing system execution modeling,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
215,1,181,"Surrogate: A Simulation Apparatus for Continuous Integration Testing in Service Oriented Architecture CIT (continuous integration testing) has been widely studied in the testing research field in order to start some levels of integration test as early as possible. One challenge of CIT lies in how to simulate the behavior of those unavailable components. Existing methods like stud and mock fail to provide the advanced component simulation capabilities required by CIT from perspectives like diversified program artifacts, behavior transitivity, and configurability. This paper proposes a new simulation apparatus, namely surrogate, to address this problem. The surrogate generator generates platform specific code skeleton from definition of the component to be simulated. The generated code communicates with surrogate engine and returns simulated platform specific behaviors. The surrogate engine simulates component behaviors including both output and possible invocation on dependent components. Moreover, it provides platform independent interfaces and configuration model. Early implementations of surrogate generator and surrogate engine are introduced in detail. To validate the value of surrogate technology in CIT, a case study has been carried out with careful analysis. The result shows that this technology really helps identify some bugs at early stage of development. Continuous integration testing, service oriented archi-tecture, simulation",surrogate a simulation apparatus for continuous integration testing in service oriented architecture cit continuous integration testing has been widely studied in the testing research field in order to start some levels of integration test as early as possible one challenge of cit lies in how to simulate the behavior of those unavailable components existing methods like stud and mock fail to provide the advanced component simulation capabilities required by cit from perspectives like diversified program artifacts behavior transitivity and configurability this paper proposes a new simulation apparatus namely surrogate to address this problem the surrogate generator generates platform specific code skeleton from definition of the component to be simulated the generated code communicates with surrogate engine and returns simulated platform specific behaviors the surrogate engine simulates component behaviors including both output and possible invocation on dependent components moreover it provides platform independent interfaces and configuration model early implementations of surrogate generator and surrogate engine are introduced in detail to validate the value of surrogate technology in cit a case study has been carried out with careful analysis the result shows that this technology really helps identify some bugs at early stage of development continuous integration testing service oriented archi tecture simulation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
216,1,217,"CONTINUOUS AUTOMATED TESTING OF SDR SOFTWARE Software testing is vital for the success of any SDR engineering organization, given the complexity and reliability requirements of radio communications. At Vanu, Inc. all waveforms under development are tested 24 hours a day by an automated system. The test system has been running continuously since 2002. In that time it has evolved in sophistication and become an integral part of the company's software engineering methodology. As soon as an engineer checks a new software version into the code repository, the automated test system checks it out, compiles versions of all derived and inter-operating software, and commences testing. Test results are reported continuously to all interested members of the engineering team via web-based reports and an online chat room. Since Vanu waveforms are entirely implemented on general purpose processors using standard operating systems, the tests can run on standard servers without loss of fidelity. A radio channel simulator enables end-to-end testing of communication among multiple servers connected by Ethernet. Since our radio heads exchange digital samples with the baseband processing server via Ethernet, inserting the simulator rather than actual radio hardware is fully transparent to the software under test. This paper describes the architecture of the test system and the design of its major components. Synergies with aspects of the Vanu, Inc. SDR design approach are highlighted. []",continuous automated testing of sdr software software testing is vital for the success of any sdr engineering organization given the complexity and reliability requirements of radio communications at vanu inc all waveforms under development are tested 24 hours a day by an automated system the test system has been running continuously since 2002 in that time it has evolved in sophistication and become an integral part of the company s software engineering methodology as soon as an engineer checks a new software version into the code repository the automated test system checks it out compiles versions of all derived and inter operating software and commences testing test results are reported continuously to all interested members of the engineering team via web based reports and an online chat room since vanu waveforms are entirely implemented on general purpose processors using standard operating systems the tests can run on standard servers without loss of fidelity a radio channel simulator enables end to end testing of communication among multiple servers connected by ethernet since our radio heads exchange digital samples with the baseband processing server via ethernet inserting the simulator rather than actual radio hardware is fully transparent to the software under test this paper describes the architecture of the test system and the design of its major components synergies with aspects of the vanu inc sdr design approach are highlighted,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
217,1,100,"Architectural tactics to support rapid and agile stability The essence of stability in software development is the ability to produce quality software with infrastructure that will meet long-term business goals. The essence of rapid and agile development is the ability to deliver capabilities quickly based on customer priorities. Stability often requires cross-functional analysis and infrastructure support that will build foundational technology for the capabilities to stand on, which takes time and resources. But today's organizations must attend to both agility and enduring design. This article presents three tactics that support rapid and agile stability: aligning feature-based development and system decomposition, creating an architectural runway, and using matrix teams. []",architectural tactics to support rapid and agile stability the essence of stability in software development is the ability to produce quality software with infrastructure that will meet long term business goals the essence of rapid and agile development is the ability to deliver capabilities quickly based on customer priorities stability often requires cross functional analysis and infrastructure support that will build foundational technology for the capabilities to stand on which takes time and resources but today s organizations must attend to both agility and enduring design this article presents three tactics that support rapid and agile stability aligning feature based development and system decomposition creating an architectural runway and using matrix teams,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
218,1,70,"Scaling Continuous Integration Of all the Extreme Programming practices, continuous integration is one of the least controversial -- the benefits of an integrated, streamlined build process is something that software developers immediately recognise. However, as a project scales up in size and complexity, continuous integration can become increasingly hard to practice successfully. By focussing on the problems associated with a growing project, this paper describes a variety of strategies for successfully scaling continuous integration. Integration Process,  Unit Test,  Acceptance Test,  Integration Server,  Code Base",scaling continuous integration of all the extreme programming practices continuous integration is one of the least controversial the benefits of an integrated streamlined build process is something that software developers immediately recognise however as a project scales up in size and complexity continuous integration can become increasingly hard to practice successfully by focussing on the problems associated with a growing project this paper describes a variety of strategies for successfully scaling continuous integration integration process unit test acceptance test integration server code base,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
219,1,106,"Transforming a Six Month Release Cycle to Continuous Flow This paper tells the story of how the team managed to speed up delivery whilst raising quality at the same time, conject used to release complete modules of their Internet platform once every six months. Customers were forced to wait for months to get access to new features. Once they arrived, the big bang releases disrupted the user experience due to bugs lurking in the new software. Despite what seemed to be an impossible task, the team successfully transformed their software development process to deliver a stable and continuous flow of small releases. Now features are delivered to customers more quickly and with much higher quality. []",transforming a six month release cycle to continuous flow this paper tells the story of how the team managed to speed up delivery whilst raising quality at the same time conject used to release complete modules of their internet platform once every six months customers were forced to wait for months to get access to new features once they arrived the big bang releases disrupted the user experience due to bugs lurking in the new software despite what seemed to be an impossible task the team successfully transformed their software development process to deliver a stable and continuous flow of small releases now features are delivered to customers more quickly and with much higher quality,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
220,1,64,"Development and Deployment at Facebook Internet companies such as Facebook operate in a ""perpetual development"" mindset. This means that the website continues to undergo development with no predefined final objective, and that new developments are deployed so that users can enjoy them as soon as they're ready. To support this, Facebook uses both technical approaches such as peer review and extensive automated testing, and a culture of personal responsibility. perpetual development, devops, web development",development and deployment at facebook internet companies such as facebook operate in a perpetual development mindset this means that the website continues to undergo development with no predefined final objective and that new developments are deployed so that users can enjoy them as soon as they re ready to support this facebook uses both technical approaches such as peer review and extensive automated testing and a culture of personal responsibility perpetual development devops web development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
221,1,82,"Enterprise Continuous Integration Using Binary Dependencies Continuous Integration (CI) is a well-established practice which allows us as developers to experience fewer development conflicts and achieve rapid feedback on progress. CI by itself though becomes hard to scale as projects get large or have independent deliverables. Enterprise Continuous Integration (ECI) is an extension to CI that helps us regain the benefits of CI when working with separately developed, yet interdependent modules. We show how to develop an ECI process based upon binary dependencies, giving examples using existing .NET tools. Continuous integration,  scalability,  tools and techniques,  .NET",enterprise continuous integration using binary dependencies continuous integration ci is a well established practice which allows us as developers to experience fewer development conflicts and achieve rapid feedback on progress ci by itself though becomes hard to scale as projects get large or have independent deliverables enterprise continuous integration eci is an extension to ci that helps us regain the benefits of ci when working with separately developed yet interdependent modules we show how to develop an eci process based upon binary dependencies giving examples using existing net tools continuous integration scalability tools and techniques net,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
222,1,143,"Toward Agile Architecture: Insights from 15 Years of ATAM Data Agile teams strive to balance short-term feature development with longer-term quality concerns. These evolutionary approaches often hit a ""complexity wall""' from the cumulative effects of unplanned changes, resulting in unreliable, poorly performing software. So, the agile community is refocusing on approaches to address architectural concerns. Researchers analyzed quality attribute concerns from 15 years of Architecture Trade-Off Analysis Method data, gathered from 31 projects. Modifiability was the dominant concern across all project types; performance, availability, and interoperability also received considerable attention. For IT projects, a relatively new quality-deployability-emerged as a key concern. The study results provide insights for agile teams allocating architecture-related tasks to iterations. For example, teams can use these results to create checklists for release planning or retrospectives to help assess whether to address a given quality to support future needs. This article is part of a special issue on Software Architecture. architecture evaluation, agile, technical debt, incremental development, modifiability, availability, interoperability, deployability, software development, software engineering",toward agile architecture insights from 15 years of atam data agile teams strive to balance short term feature development with longer term quality concerns these evolutionary approaches often hit a complexity wall from the cumulative effects of unplanned changes resulting in unreliable poorly performing software so the agile community is refocusing on approaches to address architectural concerns researchers analyzed quality attribute concerns from 15 years of architecture trade off analysis method data gathered from 31 projects modifiability was the dominant concern across all project types performance availability and interoperability also received considerable attention for it projects a relatively new quality deployability emerged as a key concern the study results provide insights for agile teams allocating architecture related tasks to iterations for example teams can use these results to create checklists for release planning or retrospectives to help assess whether to address a given quality to support future needs this article is part of a special issue on software architecture architecture evaluation agile technical debt incremental development modifiability availability interoperability deployability software development software engineering,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0.0
223,1,201,"Stakeholder Perceptions of the Adoption of Continuous Integration -- A Case Study Continuous integration is an important support mechanism for fast delivery of new features. However, its adoption in industry has often been problematic, partly due to social challenges. However, there is little knowledge of the exact nature of the challenges, and how different stakeholders perceive the need for and adoption of continuous integration. In this paper, we describe how the introduction of continuous integration was perceived by different stakeholders in a R&D program at Ericsson. The case provided a rare opportunity to study the adoption of continuous integration in a large distributed organization. We interviewed 27 stakeholders and found differing perceptions of continuous integration: how suitable it is for the organization, how adoption should be organized, and whether it is possible to achieve sufficient quality through automated testing. These differences of perception were mainly consequences of the geographic distribution. Based on the case study, we propose three guidelines. First, understand that the product architecture has a significant effect on the adoption. However, do not let architectural problems keep you from implementing continuous integration. Second, give the team members sufficient time to overcome the initial learning phase in the adoption. Third, avoid centralizing competencies to individual sites, and invest in cross-site communication. continuous integration, adoption, case study",stakeholder perceptions of the adoption of continuous integration a case study continuous integration is an important support mechanism for fast delivery of new features however its adoption in industry has often been problematic partly due to social challenges however there is little knowledge of the exact nature of the challenges and how different stakeholders perceive the need for and adoption of continuous integration in this paper we describe how the introduction of continuous integration was perceived by different stakeholders in a r d program at ericsson the case provided a rare opportunity to study the adoption of continuous integration in a large distributed organization we interviewed 27 stakeholders and found differing perceptions of continuous integration how suitable it is for the organization how adoption should be organized and whether it is possible to achieve sufficient quality through automated testing these differences of perception were mainly consequences of the geographic distribution based on the case study we propose three guidelines first understand that the product architecture has a significant effect on the adoption however do not let architectural problems keep you from implementing continuous integration second give the team members sufficient time to overcome the initial learning phase in the adoption third avoid centralizing competencies to individual sites and invest in cross site communication continuous integration adoption case study,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
224,1,210,"Synthesizing Continuous Deployment Practices Used in Software Development Continuous deployment speeds up the process of existing agile methods, such as Scrum, and Extreme Programming (XP) through the automatic deployment of software changes to end-users upon passing of automated tests. Continuous deployment has become an emerging software engineering process amongst numerous software companies, such as Facebook, Github, Netflix, and Rally Software. A systematic analysis of software practices used in continuous deployment can facilitate a better understanding of continuous deployment as a software engineering process. Such analysis can also help software practitioners in having a shared vocabulary of practices and in choosing the software practices that they can use to implement continuous deployment. The goal of this paper is to aid software practitioners in implementing continuous deployment through a systematic analysis of software practices that are used by software companies. We studied the continuous deployment practices of 19 software companies by performing a qualitative analysis of Internet artifacts and by conducting follow-up inquiries. In total, we found 11 software practices that are used by 19 software companies. We also found that in terms of use, eight of the 11 software practices are common across 14 software companies. We observe that continuous deployment necessitates the consistent use of sound software engineering practices such as automated testing, automated deployment, and code review. agile, continuous deployment, continuous delivery, industry practices, internet artifacts, follow-up inquiries",synthesizing continuous deployment practices used in software development continuous deployment speeds up the process of existing agile methods such as scrum and extreme programming xp through the automatic deployment of software changes to end users upon passing of automated tests continuous deployment has become an emerging software engineering process amongst numerous software companies such as facebook github netflix and rally software a systematic analysis of software practices used in continuous deployment can facilitate a better understanding of continuous deployment as a software engineering process such analysis can also help software practitioners in having a shared vocabulary of practices and in choosing the software practices that they can use to implement continuous deployment the goal of this paper is to aid software practitioners in implementing continuous deployment through a systematic analysis of software practices that are used by software companies we studied the continuous deployment practices of 19 software companies by performing a qualitative analysis of internet artifacts and by conducting follow up inquiries in total we found 11 software practices that are used by 19 software companies we also found that in terms of use eight of the 11 software practices are common across 14 software companies we observe that continuous deployment necessitates the consistent use of sound software engineering practices such as automated testing automated deployment and code review agile continuous deployment continuous delivery industry practices internet artifacts follow up inquiries,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
225,1,221,"Transitioning Towards Continuous Delivery in the B2B Domain: A Case Study Delivering value to customers in real-time requires companies to utilize real-time deployment of software to expose features to users faster, and to shorten the feedback loop. This allows for faster reaction and helps to ensure that the development is focused on features providing real value. Continuous delivery is a development practice where the software functionality is deployed continuously to customer environment. Although this practice has been established in some domains such as B2C mobile software, the B2B domain imposes specific challenges. This article presents a case study that is conducted in a medium-sized software company operating in the B2B domain. The objective of this study is to analyze the challenges and benefits of continuous delivery in this domain. The results suggest that technical challenges are only one part of the challenges a company encounters in this transition. The company must also address challenges related to the customer and procedures. The core challenges are caused by having multiple customers with diverse environments and unique properties, whose business depends on the software product. Some customers require to perform manual acceptance testing, while some are reluctant towards new versions. By utilizing continuous delivery, it is possible for the case company to shorten the feedback cycles, increase the reliability of new versions, and reduce the amount of resources required for deploying and testing new releases. Continuous delivery,  Continuous deployment,  Development process,  B2B,  Case study",transitioning towards continuous delivery in the b2b domain a case study delivering value to customers in real time requires companies to utilize real time deployment of software to expose features to users faster and to shorten the feedback loop this allows for faster reaction and helps to ensure that the development is focused on features providing real value continuous delivery is a development practice where the software functionality is deployed continuously to customer environment although this practice has been established in some domains such as b2c mobile software the b2b domain imposes specific challenges this article presents a case study that is conducted in a medium sized software company operating in the b2b domain the objective of this study is to analyze the challenges and benefits of continuous delivery in this domain the results suggest that technical challenges are only one part of the challenges a company encounters in this transition the company must also address challenges related to the customer and procedures the core challenges are caused by having multiple customers with diverse environments and unique properties whose business depends on the software product some customers require to perform manual acceptance testing while some are reluctant towards new versions by utilizing continuous delivery it is possible for the case company to shorten the feedback cycles increase the reliability of new versions and reduce the amount of resources required for deploying and testing new releases continuous delivery continuous deployment development process b2b case study,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
226,1,289,"Ultimate architecture enforcement custom checks enforced at code-commit time Creating a software architecture is a critical task in the development of software systems. However, the architecture discussed and carefully created is often not entirely followed in the implementation. Unless the architecture is communicated effectively to all developers, divergence between the intended architecture (created by the architect) and the actual architecture (found in the source code) tends to gradually increase. Static analysis tools, which are often used to check coding conventions and best practices, can help. However, the common use of static analysis tools for architecture enforcement has two limitations. One is the fact that design rules specific to a software architecture are not known and hence not enforced by the tool. The other limitation is more of a practical issue: static analysis tools are often integrated to the IDE or to a continuous integration environment; they report violations but the developers may choose to ignore them. This paper reports a successful experience where we addressed these two limitations for a large codebase comprising over 50 Java applications. Using a free open source tool called checkstyle and its Java API, we implemented custom checks for design constraints specified by the architecture of our software systems. In addition, we created a script that executes automatically on the Subversion software configuration management server prior to any code commit operation. This script runs the custom checks and denies the commit operation in case a violation is found. When that happens, the developer gets a clear error message explaining the problem. The architecture team is also notified and can proactively contact the developer to address any lack of understanding of the architecture. This experience report provides technical details of our architecture enforcement approach and recommendations to employ this or similar solutions more effectively. architecture conformance,  checkstyle.,  software architecture,  architecture enforcement,  java,  static analysis",ultimate architecture enforcement custom checks enforced at code commit time creating a software architecture is a critical task in the development of software systems however the architecture discussed and carefully created is often not entirely followed in the implementation unless the architecture is communicated effectively to all developers divergence between the intended architecture created by the architect and the actual architecture found in the source code tends to gradually increase static analysis tools which are often used to check coding conventions and best practices can help however the common use of static analysis tools for architecture enforcement has two limitations one is the fact that design rules specific to a software architecture are not known and hence not enforced by the tool the other limitation is more of a practical issue static analysis tools are often integrated to the ide or to a continuous integration environment they report violations but the developers may choose to ignore them this paper reports a successful experience where we addressed these two limitations for a large codebase comprising over 50 java applications using a free open source tool called checkstyle and its java api we implemented custom checks for design constraints specified by the architecture of our software systems in addition we created a script that executes automatically on the subversion software configuration management server prior to any code commit operation this script runs the custom checks and denies the commit operation in case a violation is found when that happens the developer gets a clear error message explaining the problem the architecture team is also notified and can proactively contact the developer to address any lack of understanding of the architecture this experience report provides technical details of our architecture enforcement approach and recommendations to employ this or similar solutions more effectively architecture conformance checkstyle software architecture architecture enforcement java static analysis,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
227,1,170,"Factors impacting rapid releases: An industrial case study Context: Software release teams try to reduce the time needed for the transit of features or bug fixes from the development environment to the production, crossing all the quality gates. However, little is known about the factors that influence the time-to-production and how they might be controlled in order to speed up the release cycles. Goal: This paper examines step by step the release process of an industrial software organization aiming to identify factors that have a significant impact on the lead time and outcomes of the software releases. Method: Over 14 months of release data have been analyzed (246 releases from the isolated source code branches to the production environment). Results: We discuss three dimensions under which a series of factors could be addressed: technical, organizational, and interactional. We present our findings in terms of implications for release process improvements. Conclusions: Our analyzes reveal that testing is the most time consuming activities (86%) along with the need for more congruence among teams, especially in the context of parallel development. parallel development,  rapid release,  release cycles,  release management,  empirical software engineering,  lead time,  software release,  software quality,  software process",factors impacting rapid releases an industrial case study context software release teams try to reduce the time needed for the transit of features or bug fixes from the development environment to the production crossing all the quality gates however little is known about the factors that influence the time to production and how they might be controlled in order to speed up the release cycles goal this paper examines step by step the release process of an industrial software organization aiming to identify factors that have a significant impact on the lead time and outcomes of the software releases method over 14 months of release data have been analyzed 246 releases from the isolated source code branches to the production environment results we discuss three dimensions under which a series of factors could be addressed technical organizational and interactional we present our findings in terms of implications for release process improvements conclusions our analyzes reveal that testing is the most time consuming activities 86 along with the need for more congruence among teams especially in the context of parallel development parallel development rapid release release cycles release management empirical software engineering lead time software release software quality software process,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
228,1,232,"A Unified Test Framework for Continuous Integration Testing of SOA Solutions The quality of service oriented architecture (SOA) solutions is becoming more and more important along with the increasing adoption of SOA. Continuous integration testing (CIT) is an effective technology to discover bugs as early as possible. However, the diversity of programming models used in an SOA solution and the distribution nature of an SOA solution pose new challenges for CIT. Existing testing frameworks more focus on the integration testing of applications developed by a single programming model. In this paper, a unified test framework is proposed to overcome these limitations and enable the CIT of SOA solutions across the whole development lifecycle. This framework is designed following the model driven architecture (MDA). The information of an executable test case is separated into two layers: the behavior layer and the configuration layer. The behavior layer represents the test logic of a test case and is platform independent. The configuration layer contains the platform specific information and is configurable for different programming models. An extensible and pluggable test execution engine is specially designed to execute the integration test cases. A global test case identifier instrumentation approach is used to merge the distributed test case execution traces captured by ITCAM - an IBM integrated management tool. A verification approach supporting Boolean expression and back-end service interaction verification is proposed to verify the test execution result. Initial experiments have shown the effectiveness of this unified test framework. continuous integration testing, service oriented architecture",a unified test framework for continuous integration testing of soa solutions the quality of service oriented architecture soa solutions is becoming more and more important along with the increasing adoption of soa continuous integration testing cit is an effective technology to discover bugs as early as possible however the diversity of programming models used in an soa solution and the distribution nature of an soa solution pose new challenges for cit existing testing frameworks more focus on the integration testing of applications developed by a single programming model in this paper a unified test framework is proposed to overcome these limitations and enable the cit of soa solutions across the whole development lifecycle this framework is designed following the model driven architecture mda the information of an executable test case is separated into two layers the behavior layer and the configuration layer the behavior layer represents the test logic of a test case and is platform independent the configuration layer contains the platform specific information and is configurable for different programming models an extensible and pluggable test execution engine is specially designed to execute the integration test cases a global test case identifier instrumentation approach is used to merge the distributed test case execution traces captured by itcam an ibm integrated management tool a verification approach supporting boolean expression and back end service interaction verification is proposed to verify the test execution result initial experiments have shown the effectiveness of this unified test framework continuous integration testing service oriented architecture,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
229,1,182,"Using continuous integration and automated test techniques for a robust C4ISR system We have used CI (continuous integration) and various software testing techniques to achieve a robust C4ISR (command, control, communications, computers, intelligence, surveillance, and reconnaissance) multi-platform system. Because of rapid changes in the C4ISR domain and in the software technology, frequent critical design adjustments and in turn vast code modifications or additions become inevitable. Defect fixes might also incur code changes. These unavoidable code modifications may put a big risk in the reliability of a mission critical system. Also, in order to stay competitive in the C4ISR market, a company must make recurring releases without sacrificing quality. We have designed and implemented an XML driven automated test framework that enabled us developing numerous high quality tests rapidly. While using CI with automated software test techniques, we have aimed at speeding up the delivery of high quality and robust software by decreasing integration procedure, which is one of the main bottleneck points in the industry. This work describes how we have used CI and software test techniques in a large-scaled, multi-platform, multi-language, distributed C4ISR project and what the benefits of such a system are. continuous integration, software test techniques, mission critical systems, robustness, XML driven automated test framework, C4ISR systems, multi-platform testing",using continuous integration and automated test techniques for a robust c4isr system we have used ci continuous integration and various software testing techniques to achieve a robust c4isr command control communications computers intelligence surveillance and reconnaissance multi platform system because of rapid changes in the c4isr domain and in the software technology frequent critical design adjustments and in turn vast code modifications or additions become inevitable defect fixes might also incur code changes these unavoidable code modifications may put a big risk in the reliability of a mission critical system also in order to stay competitive in the c4isr market a company must make recurring releases without sacrificing quality we have designed and implemented an xml driven automated test framework that enabled us developing numerous high quality tests rapidly while using ci with automated software test techniques we have aimed at speeding up the delivery of high quality and robust software by decreasing integration procedure which is one of the main bottleneck points in the industry this work describes how we have used ci and software test techniques in a large scaled multi platform multi language distributed c4isr project and what the benefits of such a system are continuous integration software test techniques mission critical systems robustness xml driven automated test framework c4isr systems multi platform testing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
230,1,143,"Test Automation Framework for Implementing Continuous Integration Manual testing is a laborious and time consuming process. In addition, it may not be effective in finding certain defects. Therefore, we introduce an effective framework for automated testing to help solve such problems. The proposed framework helps automate the distribution, execution, and results analysis of test cases. The workflow of tests and test environments are graphically expressed as tables. Many software development and testing practices can be automated and greatly simplified by using this framework. It can also be used to create a Continuous Integration (CI) system by incorporating the automated build tools or CI servers. This paper provides best practices on automated CI solutions using the proposed framework to provide developers and/or testers with a better idea of progress and code quality throughout the project lifecycle so that they can direct their time and expertise to more important, challenging issues. Test Automation Framework, STAF, Fit, FitNesse, Continuous Integration",test automation framework for implementing continuous integration manual testing is a laborious and time consuming process in addition it may not be effective in finding certain defects therefore we introduce an effective framework for automated testing to help solve such problems the proposed framework helps automate the distribution execution and results analysis of test cases the workflow of tests and test environments are graphically expressed as tables many software development and testing practices can be automated and greatly simplified by using this framework it can also be used to create a continuous integration ci system by incorporating the automated build tools or ci servers this paper provides best practices on automated ci solutions using the proposed framework to provide developers and or testers with a better idea of progress and code quality throughout the project lifecycle so that they can direct their time and expertise to more important challenging issues test automation framework staf fit fitnesse continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
231,1,150,"Hitting the Wall: What to Do When High Performing Scrum Teams Overwhelm Operations and Infrastructure All-at-once Scrum implementations require total commitment to change, high level management support and aggressive removal of impediments. Several company-wide implementations are known to the authors but none of them had to deal with the complexity of a large, mission-critical, enterprise software product. Pegasystems develops software to manage, automate and optimize a broad array of business processes. In July of 2009 the company deployed over 20 Scrum teams in the U.S. and India within two months. Complexity of languages, frameworks, and tools led to an architecture where continuous integration support for software development teams was impossible without a major upgrade in infrastructure and operations. A virtual Scrum team was formed to avoid ""hitting the wall"" before this impediment impacted the first Scrum release of the software. Here we describe how a Scrum team engineered a continuous integration environment for hundreds of software developers on two continents within a few weeks. []",hitting the wall what to do when high performing scrum teams overwhelm operations and infrastructure all at once scrum implementations require total commitment to change high level management support and aggressive removal of impediments several company wide implementations are known to the authors but none of them had to deal with the complexity of a large mission critical enterprise software product pegasystems develops software to manage automate and optimize a broad array of business processes in july of 2009 the company deployed over 20 scrum teams in the u s and india within two months complexity of languages frameworks and tools led to an architecture where continuous integration support for software development teams was impossible without a major upgrade in infrastructure and operations a virtual scrum team was formed to avoid hitting the wall before this impediment impacted the first scrum release of the software here we describe how a scrum team engineered a continuous integration environment for hundreds of software developers on two continents within a few weeks,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
232,1,247,"Continuous SCRUM: Agile management of SAAS products Hosted software-as-a-service products provide an opportunity to provide consumers with continuous deployment of new features, as opposed to scheduled version upgrades as is the norm for products installed on-premise. In order to exploit this opportunity, a SaaS provider needs to adopt an agile process that is capable of releasing new features rapidly. The SCRUM [5,6] process is ideally suited for this purpose: However, when SCRUM has been used for agile development of an installed product, parallel, overlapping 'sprints' are executed by separate teams, each dealing with short, medium, and longer-term enhancements to the product[3]; with the result that version upgrades are therefore easier to manage. In contrast, in the case of a SAAS product, version upgrades are no longer a constraint, so we can do better. In this paper we describe 'Continuous SCRUM', a variant of Type-C SCRUM, augmented with engineering best practices, in a manner ideally suited for managing SAAS products. In our approach, bug-fixes, minor enhancements, as well as major features are released continuously, on a weekly basis by a single team, in contrast to ""Meta-SCRUM"" [3]. We also present field data from our experience with using Continuous SCRUM for a hosted platform-as-a-service product for more than two years. Our experience reinforces other recent evidence [11] that rapid, smaller releases are often preferable to infrequent, larger ones. Continuous SCRUM provides a mechanism to achieve and sustain a rapid release cycle, for SAAS products as well as, we believe, for custom applications developed in-house. agile process, configuration management,  SCRUM,  SAAS,  PAAS,  release management,  continuous deployment",continuous scrum agile management of saas products hosted software as a service products provide an opportunity to provide consumers with continuous deployment of new features as opposed to scheduled version upgrades as is the norm for products installed on premise in order to exploit this opportunity a saas provider needs to adopt an agile process that is capable of releasing new features rapidly the scrum 5 6 process is ideally suited for this purpose however when scrum has been used for agile development of an installed product parallel overlapping sprints are executed by separate teams each dealing with short medium and longer term enhancements to the product 3 with the result that version upgrades are therefore easier to manage in contrast in the case of a saas product version upgrades are no longer a constraint so we can do better in this paper we describe continuous scrum a variant of type c scrum augmented with engineering best practices in a manner ideally suited for managing saas products in our approach bug fixes minor enhancements as well as major features are released continuously on a weekly basis by a single team in contrast to meta scrum 3 we also present field data from our experience with using continuous scrum for a hosted platform as a service product for more than two years our experience reinforces other recent evidence 11 that rapid smaller releases are often preferable to infrequent larger ones continuous scrum provides a mechanism to achieve and sustain a rapid release cycle for saas products as well as we believe for custom applications developed in house agile process configuration management scrum saas paas release management continuous deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
233,1,164,"Integrating Early V&V Support to a GSE Tool Integration Platform The ever-growing market pressure and complex products demand high quality work and effectiveness from software practitioners. This relates also for the methods and tools they use for the development of software-intensive systems. Validation and verification (V&V) are the cornerstones of the overall quality of a system. By performing efficient V&V activities to detect defects during the early phases of development, the developers are able to save time and effort required for fixing them. Tool support is available for all types of V&V activities, especially testing, model checking, syntactic verification, and inspection. In distributed development the role of tools is even more relevant than in single-site development, and tool integration is often imperative for ensuring the effectiveness of work. In this paper, we discuss how a tool integration framework was extended to support early V&V activities via continuous integrations. We find that integrating early V& V supporting tools is feasible and useful, and makes a tool integration framework even more beneficial. Verfication and Validation, tool integration, V&V tools, global software integration, continuous integration",integrating early v v support to a gse tool integration platform the ever growing market pressure and complex products demand high quality work and effectiveness from software practitioners this relates also for the methods and tools they use for the development of software intensive systems validation and verification v v are the cornerstones of the overall quality of a system by performing efficient v v activities to detect defects during the early phases of development the developers are able to save time and effort required for fixing them tool support is available for all types of v v activities especially testing model checking syntactic verification and inspection in distributed development the role of tools is even more relevant than in single site development and tool integration is often imperative for ensuring the effectiveness of work in this paper we discuss how a tool integration framework was extended to support early v v activities via continuous integrations we find that integrating early v v supporting tools is feasible and useful and makes a tool integration framework even more beneficial verfication and validation tool integration v v tools global software integration continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0
234,1,104,"How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization. Continuous integration, software process integration, test case prioritization, fault localization",how well do test case prioritization techniques support statistical fault localization in continuous integration a tight integration of test case prioritization techniques and fault localization techniques may both expose failures faster and locate faults more effectively statistical fault localization techniques use the execution information collected during testing to locate faults executing a small fraction of a prioritized test suite reduces the cost of testing and yet the subsequent fault localization may suffer this paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization among many interesting empirical results we find that coverage based and random techniques can be more effective than distribution based techniques in supporting statistical fault localization continuous integration software process integration test case prioritization fault localization,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0.0
235,1,121,"Ambient Awareness of Build Status in Collocated Software Teams We describe the evaluation of a build awareness system that assists agile software development teams to understand current build status and who is responsible for any build breakages. The system uses ambient awareness technologies, providing a separate, easily perceived communication channel distinct from standard team workflow. Multiple system configurations and behaviours were evaluated. An evaluation of the system showed that, while there was no significant change in the proportion of build breakages, the overall number of builds increased substantially, and the duration of broken builds decreased. Team members also reported an increased sense of awareness of, and responsibility for, broken builds and some noted the system dramatically changed their perception of the build process making them more cognisant of broken builds. software teams,  continuous integration,  ambient awareness,  build processes,  status information",ambient awareness of build status in collocated software teams we describe the evaluation of a build awareness system that assists agile software development teams to understand current build status and who is responsible for any build breakages the system uses ambient awareness technologies providing a separate easily perceived communication channel distinct from standard team workflow multiple system configurations and behaviours were evaluated an evaluation of the system showed that while there was no significant change in the proportion of build breakages the overall number of builds increased substantially and the duration of broken builds decreased team members also reported an increased sense of awareness of and responsibility for broken builds and some noted the system dramatically changed their perception of the build process making them more cognisant of broken builds software teams continuous integration ambient awareness build processes status information,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
236,1,144,"A Technique for Agile and Automatic Interaction Testing for Product Lines Product line developers must ensure that existing and new features work in all products. Adding to or changing a product line might break some of its features. In this paper, we present a technique for automatic and agile interaction testing for product lines. The technique enables developers to know if features work together with other features in a product line, and it blends well into a process of continuous integration. The technique is evaluated with two industrial applications, testing a product line of safety devices and the Eclipse IDEs. The first case shows how existing test suites are applied to the products of a 2-wise covering array to identify two interaction faults. The second case shows how over 400,000 test executions are performed on the products of a 2-wise covering array using over 40,000 existing automatic tests to identify potential interactions faults. Product Lines,  Testing,  Agile,  Continuous Integration , Automatic,  Combinatorial Interaction Testing",a technique for agile and automatic interaction testing for product lines product line developers must ensure that existing and new features work in all products adding to or changing a product line might break some of its features in this paper we present a technique for automatic and agile interaction testing for product lines the technique enables developers to know if features work together with other features in a product line and it blends well into a process of continuous integration the technique is evaluated with two industrial applications testing a product line of safety devices and the eclipse ides the first case shows how existing test suites are applied to the products of a 2 wise covering array to identify two interaction faults the second case shows how over 400 000 test executions are performed on the products of a 2 wise covering array using over 40 000 existing automatic tests to identify potential interactions faults product lines testing agile continuous integration automatic combinatorial interaction testing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0.0
237,1,137,"Technical Dependency Challenges in Large-Scale Agile Software Development This qualitative study investigates challenges associated with technical dependencies and their communication. Such challenges frequently occur when agile practices are scaled to large-scale software development. The use of thematic analysis on semi-structured interviews revealed five challenges: planning, task prioritization, knowledge sharing, code quality, and integration. More importantly, these challenges interact with one another and can lead to a domino effect or vicious circle. If an organization struggles with one challenge, it is likely that the other challenges become problematic as well. This situation can have a significant impact on process and product quality. Our recommendations focus on improving planning and knowledge sharing (with practices such as scrum-of-scrums, continuous integration, open space technology) to break the vicious circle, and to reestablish effective communication across teams, which will then enable large-scale companies to achieve the benefits of large-scale agility. Technical dependencies,  Large-scale agile,  Cross-Functional Teams (XFT),  Qualitative research",technical dependency challenges in large scale agile software development this qualitative study investigates challenges associated with technical dependencies and their communication such challenges frequently occur when agile practices are scaled to large scale software development the use of thematic analysis on semi structured interviews revealed five challenges planning task prioritization knowledge sharing code quality and integration more importantly these challenges interact with one another and can lead to a domino effect or vicious circle if an organization struggles with one challenge it is likely that the other challenges become problematic as well this situation can have a significant impact on process and product quality our recommendations focus on improving planning and knowledge sharing with practices such as scrum of scrums continuous integration open space technology to break the vicious circle and to reestablish effective communication across teams which will then enable large scale companies to achieve the benefits of large scale agility technical dependencies large scale agile cross functional teams xft qualitative research,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
238,1,170,"Patterns for Continuous Integration Builds in Cross-Platform Agile Software Development Cross-platform software development poses challenges to agile development teams in practicing continuous integration (CI) builds not only because such builds take a longer time to complete and are more likely to fail, but also because builds of different lengths and scopes must be available depending on the working circumstances. To deal with this situation, three aspects of build automation in CI-the structuring of source code modules, the management of intermediate and final build artifacts, and the execution of builds-must be re-considered to account for the cross-platform characteristics. This paper discovers and documents a collection of ten patterns of CI builds for use in developing cross-platform software in the three aspects re-considered. These patterns are distilled from known uses of builds in existing software and from our experience in building commercial and open-source cross-platform software. As illustrated with an example adapted from the development of a real-world commercial cross-platform software product, the patterns can be effectively applied to solve many commonly encountered problems in applying CI for agile cross-platform software development. continuous integration,  software build,  cross-platform software,  pattern,  pattern language,  agile development",patterns for continuous integration builds in cross platform agile software development cross platform software development poses challenges to agile development teams in practicing continuous integration ci builds not only because such builds take a longer time to complete and are more likely to fail but also because builds of different lengths and scopes must be available depending on the working circumstances to deal with this situation three aspects of build automation in ci the structuring of source code modules the management of intermediate and final build artifacts and the execution of builds must be re considered to account for the cross platform characteristics this paper discovers and documents a collection of ten patterns of ci builds for use in developing cross platform software in the three aspects re considered these patterns are distilled from known uses of builds in existing software and from our experience in building commercial and open source cross platform software as illustrated with an example adapted from the development of a real world commercial cross platform software product the patterns can be effectively applied to solve many commonly encountered problems in applying ci for agile cross platform software development continuous integration software build cross platform software pattern pattern language agile development,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0.0
239,1,73,"Rapid Releases and Patch Backouts: A Software Analytics Approach Release engineering deals with decisions that impact the daily lives of developers, testers, and users and thus contribute to a product's success. Although gut feeling is important in such decisions, it's increasingly important to leverage existing data, such as bug reports, source code changes, code reviews, and test results, both to support decisions and to help evaluate current practices. The exploration of software engineering data to obtain insightful information is called software analytics. release engineering, rapid releases, software analytics, bug reopening, software engineering, software development, Mozilla, Firefox, Web browsers",rapid releases and patch backouts a software analytics approach release engineering deals with decisions that impact the daily lives of developers testers and users and thus contribute to a product s success although gut feeling is important in such decisions it s increasingly important to leverage existing data such as bug reports source code changes code reviews and test results both to support decisions and to help evaluate current practices the exploration of software engineering data to obtain insightful information is called software analytics release engineering rapid releases software analytics bug reopening software engineering software development mozilla firefox web browsers,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
240,1,155,"Elaboration on an integrated architecture and requirement practice: Prototyping with quality attribute focus This experience report builds on an earlier study in which we interviewed eight project teams that were using iterative incremental lifecycles. In the study, we captured the practices the teams felt contributed to rapid delivery. We identified a mix of Agile and architecture practices that teams apply to rapidly field software and minimize disruption and delay. In this paper, we elaborate one practice from the study, prototyping with quality attribute focus. We compared two experiences in prototyping focused on quality attribute considerations applied on Scrum projects. We observe through interviews that feature development and prototyping practice spans multiple levels: feature development/sprint, release planning, and portfolio planning. We also observe other factors including rapid trade-off analysis, flexible architecture, and adoption of a set of enabling prototyping guidelines. The analysis of the observations sheds light on several aspects of the practice that enable the team to respond quickly and efficiently when prototype feedback suggests architectural change. agile software development, architecture, quality attribute, prototyping, release planning, requirements, software development practices, architecture trade-off",elaboration on an integrated architecture and requirement practice prototyping with quality attribute focus this experience report builds on an earlier study in which we interviewed eight project teams that were using iterative incremental lifecycles in the study we captured the practices the teams felt contributed to rapid delivery we identified a mix of agile and architecture practices that teams apply to rapidly field software and minimize disruption and delay in this paper we elaborate one practice from the study prototyping with quality attribute focus we compared two experiences in prototyping focused on quality attribute considerations applied on scrum projects we observe through interviews that feature development and prototyping practice spans multiple levels feature development sprint release planning and portfolio planning we also observe other factors including rapid trade off analysis flexible architecture and adoption of a set of enabling prototyping guidelines the analysis of the observations sheds light on several aspects of the practice that enable the team to respond quickly and efficiently when prototype feedback suggests architectural change agile software development architecture quality attribute prototyping release planning requirements software development practices architecture trade off,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
241,1,100,"Security of public continuous integration services Continuous Integration (CI) and Free, Libre and Open Source Software (FLOSS) are both associated with agile software development. Contradictingly, FLOSS projects have difficulties to use CI and software forges still lack support for CI. Two factors hamper widespread use of CI in FLOSS development: Cost of the computational resources and security risks of public CI services. Through security analysis of public CI services, this paper identifies possible attack vectors. To eliminate one class of attack vectors, the paper describes a concept that encapsulates a part of the CI system via virtualization. The concept is implemented as a proof of concept. Security,  Performance",security of public continuous integration services continuous integration ci and free libre and open source software floss are both associated with agile software development contradictingly floss projects have difficulties to use ci and software forges still lack support for ci two factors hamper widespread use of ci in floss development cost of the computational resources and security risks of public ci services through security analysis of public ci services this paper identifies possible attack vectors to eliminate one class of attack vectors the paper describes a concept that encapsulates a part of the ci system via virtualization the concept is implemented as a proof of concept security performance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
242,1,278,"Practical experience with test-driven development during commissioning of the multi-star AO system ARGOS Commissioning time for an instrument at an observatory is precious, especially the night time. Whenever astronomers come up with a software feature request or point out a software defect, the software engineers have the task to find a solution and implement it as fast as possible. In this project phase, the software engineers work under time pressure and stress to deliver a functional instrument control software (ICS). The shortness of development time during commissioning is a constraint for software engineering teams and applies to the ARGOS project as well. The goal of the ARGOS (Advanced Rayleigh guided Ground layer adaptive Optics System) project is the upgrade of the Large Binocular Telescope (LBT) with an adaptive optics (AO) system consisting of six Rayleigh laser guide stars and wavefront sensors. For developing the ICS, we used the technique Test- Driven Development (TDD) whose main rule demands that the programmer writes test code before production code. Thereby, TDD can yield a software system, that grows without defects and eases maintenance. Having applied TDD in a calm and relaxed environment like office and laboratory, the ARGOS team has profited from the benefits of TDD. Before the commissioning, we were worried that the time pressure in that tough project phase would force us to drop TDD because we would spend more time writing test code than it would be worth. Despite this concern at the beginning, we could keep TDD most of the time also in this project phase This report describes the practical application and performance of TDD including its benefits, limitations and problems during the ARGOS commissioning. Furthermore, it covers our experience with pair programming and continuous integration at the telescope. Test-Driven Development,  TDD,  commissioning,  continuous integration,  pair programming,  software engineering,  testing,  instrument control software,  distributed software system,  adaptive optics,  LBT",practical experience with test driven development during commissioning of the multi star ao system argos commissioning time for an instrument at an observatory is precious especially the night time whenever astronomers come up with a software feature request or point out a software defect the software engineers have the task to find a solution and implement it as fast as possible in this project phase the software engineers work under time pressure and stress to deliver a functional instrument control software ics the shortness of development time during commissioning is a constraint for software engineering teams and applies to the argos project as well the goal of the argos advanced rayleigh guided ground layer adaptive optics system project is the upgrade of the large binocular telescope lbt with an adaptive optics ao system consisting of six rayleigh laser guide stars and wavefront sensors for developing the ics we used the technique test driven development tdd whose main rule demands that the programmer writes test code before production code thereby tdd can yield a software system that grows without defects and eases maintenance having applied tdd in a calm and relaxed environment like office and laboratory the argos team has profited from the benefits of tdd before the commissioning we were worried that the time pressure in that tough project phase would force us to drop tdd because we would spend more time writing test code than it would be worth despite this concern at the beginning we could keep tdd most of the time also in this project phase this report describes the practical application and performance of tdd including its benefits limitations and problems during the argos commissioning furthermore it covers our experience with pair programming and continuous integration at the telescope test driven development tdd commissioning continuous integration pair programming software engineering testing instrument control software distributed software system adaptive optics lbt,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0.0
243,1,205,"Identifying and understanding header file hotspots in C/C++ build processes Software developers rely on a fast build system to incrementally compile their source code changes and produce modified deliverables for testing and deployment. Header files, which tend to trigger slow rebuild processes, are most problematic if they also change frequently during the development process, and hence, need to be rebuilt often. In this paper, we propose an approach that analyzes the build dependency graph (i.e., the data structure used to determine the minimal list of commands that must be executed when a source code file is modified), and the change history of a software system to pinpoint header file hotspots---header files that change frequently and trigger long rebuild processes. Through a case study on the GLib, PostgreSQL, Qt, and Ruby systems, we show that our approach identifies header file hotspots that, if improved, will provide greater improvement to the total future build cost of a system than just focusing on the files that trigger the slowest rebuild processes, change the most frequently, or are used the most throughout the codebase. Furthermore, regression models built using architectural and code properties of source files can explain 32--57 % of these hotspots, identifying subsystems that are particularly hotspot-prone and would benefit the most from architectural refinement. Build systems, Performance analysis, Mining software repositories",identifying and understanding header file hotspots in c c build processes software developers rely on a fast build system to incrementally compile their source code changes and produce modified deliverables for testing and deployment header files which tend to trigger slow rebuild processes are most problematic if they also change frequently during the development process and hence need to be rebuilt often in this paper we propose an approach that analyzes the build dependency graph i e the data structure used to determine the minimal list of commands that must be executed when a source code file is modified and the change history of a software system to pinpoint header file hotspots header files that change frequently and trigger long rebuild processes through a case study on the glib postgresql qt and ruby systems we show that our approach identifies header file hotspots that if improved will provide greater improvement to the total future build cost of a system than just focusing on the files that trigger the slowest rebuild processes change the most frequently or are used the most throughout the codebase furthermore regression models built using architectural and code properties of source files can explain 32 57 of these hotspots identifying subsystems that are particularly hotspot prone and would benefit the most from architectural refinement build systems performance analysis mining software repositories,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
244,1,208,"SQA-Profiles: Rule-based activity profiles for Continuous Integration environments Continuous Integration (CI) environments cope with the repeated integration of source code changes and provide rapid feedback about the status of a software project. However, as the integration cycles become shorter, the amount of data increases, and the effort to find information in CI environments becomes substantial. In modern CI environments, the selection of measurements (e.g., build status, quality metrics) listed in a dashboard does only change with the intervention of a stakeholder (e.g., a project manager). In this paper, we want to address the shortcoming of static views with so-called Software Quality Assessment (SQA) profiles. SQA-Profiles are defined as rule-sets and enable a dynamic composition of CI dashboards based on stakeholder activities in tools of a CI environment (e.g., version control system). We present a set of SQA-Profiles for project management committee (PMC) members: Bandleader, Integrator, Gatekeeper, and Onlooker. For this, we mined the commit and issue management activities of PMC members from 20 Apache projects. We implemented a framework to evaluate the performance of our rule-based SQA-Profiles in comparison to a machine learning approach. The results showed that project-independent SQA-Profiles can be used to automatically extract the profiles of PMC members with a precision of 0.92 and a recall of 0.78. []",sqa profiles rule based activity profiles for continuous integration environments continuous integration ci environments cope with the repeated integration of source code changes and provide rapid feedback about the status of a software project however as the integration cycles become shorter the amount of data increases and the effort to find information in ci environments becomes substantial in modern ci environments the selection of measurements e g build status quality metrics listed in a dashboard does only change with the intervention of a stakeholder e g a project manager in this paper we want to address the shortcoming of static views with so called software quality assessment sqa profiles sqa profiles are defined as rule sets and enable a dynamic composition of ci dashboards based on stakeholder activities in tools of a ci environment e g version control system we present a set of sqa profiles for project management committee pmc members bandleader integrator gatekeeper and onlooker for this we mined the commit and issue management activities of pmc members from 20 apache projects we implemented a framework to evaluate the performance of our rule based sqa profiles in comparison to a machine learning approach the results showed that project independent sqa profiles can be used to automatically extract the profiles of pmc members with a precision of 0 92 and a recall of 0 78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
245,1,165,"Mixed Data-Parallel Scheduling for Distributed Continuous Integration In this paper, we consider the problem of scheduling a special kind of mixed data-parallel applications arising in the context of Continuous Integration. Continuous integration (CI) is a software engineering technique, which consists in re-building and testing interdependent software components as soon as developers modify them. The CI tool is able to provide quick feedback to the developers, which allows them to fix the bug soon after it has been introduced. The CI process can be described as a DAG where nodes represent package build tasks, and edges represent dependencies among these packages, build tasks themselves can in turn be run in parallel. Thus, CI can be viewed as a mixed data-parallel application. A crucial point for a successful CI process is its ability to provide quick feedback. Thus, make span minimization is the main goal. Our contribution is twofold. First we provide and analyze a large dataset corresponding to a build DAG. Second, we compare the performance of several scheduling heuristics on this dataset. DAG Scheduling, mixed parallelism, Continuous Integration",mixed data parallel scheduling for distributed continuous integration in this paper we consider the problem of scheduling a special kind of mixed data parallel applications arising in the context of continuous integration continuous integration ci is a software engineering technique which consists in re building and testing interdependent software components as soon as developers modify them the ci tool is able to provide quick feedback to the developers which allows them to fix the bug soon after it has been introduced the ci process can be described as a dag where nodes represent package build tasks and edges represent dependencies among these packages build tasks themselves can in turn be run in parallel thus ci can be viewed as a mixed data parallel application a crucial point for a successful ci process is its ability to provide quick feedback thus make span minimization is the main goal our contribution is twofold first we provide and analyze a large dataset corresponding to a build dag second we compare the performance of several scheduling heuristics on this dataset dag scheduling mixed parallelism continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
246,1,198,"BuildBot: Robotic Monitoring of Agile Software Development Teams In this paper, we describe BuildBot, a robotic interface developed to assist with the continuous integration process utilized by co-located agile software development teams. BuildBot's physical nature allows us to engage the agile software development team members through vision, hearing and touch. In this way, BuildBot becomes an active part of the development process by bringing together human-robot interaction, human group dynamics and software engineering concepts through a number of interaction modalities. In this paper we describe the design and implementation of the BuildBot prototype, a robotic interface that can sense virtual stimuli, in this case the state of a software build, and react accordingly in a physical way via vision, sound and touch. We present an early evaluation comparing BuildBot to two other tools used by an agile team to monitor the continuous integration process. We also show preliminary results indicating that BuildBot may be more noticeable to the developers and contribute to a fun and lighthearted atmosphere. We argue that by increasing awareness of the state of the software build, BuildBot can assist in the self-supervision of agile software engineering teams and can help the team achieve its goals in a more engaging and sociable manner. []",buildbot robotic monitoring of agile software development teams in this paper we describe buildbot a robotic interface developed to assist with the continuous integration process utilized by co located agile software development teams buildbot s physical nature allows us to engage the agile software development team members through vision hearing and touch in this way buildbot becomes an active part of the development process by bringing together human robot interaction human group dynamics and software engineering concepts through a number of interaction modalities in this paper we describe the design and implementation of the buildbot prototype a robotic interface that can sense virtual stimuli in this case the state of a software build and react accordingly in a physical way via vision sound and touch we present an early evaluation comparing buildbot to two other tools used by an agile team to monitor the continuous integration process we also show preliminary results indicating that buildbot may be more noticeable to the developers and contribute to a fun and lighthearted atmosphere we argue that by increasing awareness of the state of the software build buildbot can assist in the self supervision of agile software engineering teams and can help the team achieve its goals in a more engaging and sociable manner,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
247,1,70,"Backtracking Incremental Continuous Integration Failing integration builds are show stoppers. Development activity is stalled because developers have to wait with integrating new changes until the problem is fixed and a successful build has been run. We show how backtracking can be used to mitigate the impact of build failures in the context of component-based software development. This way, even in the face of failure, development may continue and a working version is always available. software configuration management, build management, software maintenance",backtracking incremental continuous integration failing integration builds are show stoppers development activity is stalled because developers have to wait with integrating new changes until the problem is fixed and a successful build has been run we show how backtracking can be used to mitigate the impact of build failures in the context of component based software development this way even in the face of failure development may continue and a working version is always available software configuration management build management software maintenance,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
248,1,163,"Using Continuous Integration of Code and Content to Teach Software Engineering with Limited Resources Previous courses addressing the gap between student and professional programming practice have either isolated small groups' development in such a way that larger scale difficulties that motivate many professional practices do not arise, or have required significant additional staffing that would be expensive to provide in a large cohort core undergraduate software engineering course. We describe the first iteration of a course that enabled 73 students to work together to improve a large common legacy code base using professional practices and tools, staffed only by two lecturers and two undergraduate students employed as part-time tutors. The course relies on continuous integration and automated metrics, that coalesce frequently updated information in a manner that is visible to students and can be monitored by a small number of staff. The course is supported by a just-in-time teaching programme of thirty-two technical topics. We describe the constraints that determined the design of the course, and quantitative and qualitative data from the first iteration of the course. Continuous Integration,  Software Engineering,  Studio Course,  Resource Constraints,  Experience Report",using continuous integration of code and content to teach software engineering with limited resources previous courses addressing the gap between student and professional programming practice have either isolated small groups development in such a way that larger scale difficulties that motivate many professional practices do not arise or have required significant additional staffing that would be expensive to provide in a large cohort core undergraduate software engineering course we describe the first iteration of a course that enabled 73 students to work together to improve a large common legacy code base using professional practices and tools staffed only by two lecturers and two undergraduate students employed as part time tutors the course relies on continuous integration and automated metrics that coalesce frequently updated information in a manner that is visible to students and can be monitored by a small number of staff the course is supported by a just in time teaching programme of thirty two technical topics we describe the constraints that determined the design of the course and quantitative and qualitative data from the first iteration of the course continuous integration software engineering studio course resource constraints experience report,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
249,1,115,"uBuild: Automated Testing and Performance Evaluation of Embedded Linux Systems This paper describes uBuild, a novel tool designed to support the automated execution of repeatable and controlled tests of embedded Linux systems. This is useful for continuous integration purposes, and to evaluate the impact of various design and implementation options on the system's performance. uBuild allows the designer to build the embedded system image from scratch, by compiling all the needed software from the source code and by even building the needed cross-compilation toolchain if required. It provides deterministic control on the configuration options used to build the cross-compilation toolchain, the Linux kernel, the system libraries, and all the programs. In this way, the effects of each option can be tested and evaluated in isolation. Embedded Systems,  Continuous Testing,  Performance Evaluation",ubuild automated testing and performance evaluation of embedded linux systems this paper describes ubuild a novel tool designed to support the automated execution of repeatable and controlled tests of embedded linux systems this is useful for continuous integration purposes and to evaluate the impact of various design and implementation options on the system s performance ubuild allows the designer to build the embedded system image from scratch by compiling all the needed software from the source code and by even building the needed cross compilation toolchain if required it provides deterministic control on the configuration options used to build the cross compilation toolchain the linux kernel the system libraries and all the programs in this way the effects of each option can be tested and evaluated in isolation embedded systems continuous testing performance evaluation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0.0
250,1,119,"Software Product Measurement and Analysis in a Continuous Integration Environment This paper describes a framework for a software internal quality measurement program with automatic metrics extraction. This framework was successfully implemented in an Industrial Software Factory. That was possible through the implementation of a proposed Continuous Integration CI) environment to periodically analyze source codes and extract metrics. These metrics were consolidated in a Data Warehouse by allowing On-line Analytical Processing (OLAP) and Key Performance Indicator (KPI) analysis with high-performance and user-friendly interface. The measurement program followed GQ(I)M paradigm for metrics selection to ensure that collected metrics are relevant from the Software Factory goals perspective. Finally, the Measurement and Analysis Process Area of the Capability Maturity Model integration - CMMi was used for measurement and analysis planning and implementation. Software Measurement, Software Metrics, Goal-Driven Measurement, Software Product Quality, Continuous Integration",software product measurement and analysis in a continuous integration environment this paper describes a framework for a software internal quality measurement program with automatic metrics extraction this framework was successfully implemented in an industrial software factory that was possible through the implementation of a proposed continuous integration ci environment to periodically analyze source codes and extract metrics these metrics were consolidated in a data warehouse by allowing on line analytical processing olap and key performance indicator kpi analysis with high performance and user friendly interface the measurement program followed gq i m paradigm for metrics selection to ensure that collected metrics are relevant from the software factory goals perspective finally the measurement and analysis process area of the capability maturity model integration cmmi was used for measurement and analysis planning and implementation software measurement software metrics goal driven measurement software product quality continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0.0
251,1,118,"Continuous Delivery? Easy! Just Change Everything (Well, Maybe It Is Not That Easy) Rally Software transitioned from shipping code every eight-weeks, with time-boxed Scrum sprints, to a model of continuous delivery with Kanban. The team encountered complex challenges with their build systems, automated test suites, customer enablement, and internal communication. But there was light at the end of the tunnel - greater control and flexibility over feature releases, incremental delivery of value, lower risks, fewer defects, easier on-boarding of new developers, less off-hours work, and a considerable up tick in confidence. This experience report describes the journey to continuous delivery with the aim that others can learn from our mistakes and get their teams deploying more frequently. We will describe and contrast this transition from the business (product management) and engineering perspectives. []",continuous delivery easy just change everything well maybe it is not that easy rally software transitioned from shipping code every eight weeks with time boxed scrum sprints to a model of continuous delivery with kanban the team encountered complex challenges with their build systems automated test suites customer enablement and internal communication but there was light at the end of the tunnel greater control and flexibility over feature releases incremental delivery of value lower risks fewer defects easier on boarding of new developers less off hours work and a considerable up tick in confidence this experience report describes the journey to continuous delivery with the aim that others can learn from our mistakes and get their teams deploying more frequently we will describe and contrast this transition from the business product management and engineering perspectives,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
252,1,149,"Making Software Integration Really Continuous The earlier merge conflicts are detected the easier it is to resolve them. A recommended practice is for developers to frequently integrate so that they detect conflicts earlier. However, manual integrations are cumbersome and disrupt programming flow, so developers commonly defer them; besides, manual integrations do not help to detect conflicts with uncommitted code of co-workers. Consequently, conflicts grow over time thus making resolution harder at late stages. We present a solution that continuously integrates in the background uncommitted and committed changes to support automatic detection of conflicts emerging during programming. To do so, we designed a novel merge algorithm that is O(N) complex, and implemented it inside an IDE, thus promoting a metaphor of continuous merging, similar to continuous compilation. Evidence from controlled experiments shows that our solution helps developers to become aware of and resolve conflicts earlier than when they use a mainstream version control system. software merging,  version control,  continuous integration,  conflict detection,  continuous merging",making software integration really continuous the earlier merge conflicts are detected the easier it is to resolve them a recommended practice is for developers to frequently integrate so that they detect conflicts earlier however manual integrations are cumbersome and disrupt programming flow so developers commonly defer them besides manual integrations do not help to detect conflicts with uncommitted code of co workers consequently conflicts grow over time thus making resolution harder at late stages we present a solution that continuously integrates in the background uncommitted and committed changes to support automatic detection of conflicts emerging during programming to do so we designed a novel merge algorithm that is o n complex and implemented it inside an ide thus promoting a metaphor of continuous merging similar to continuous compilation evidence from controlled experiments shows that our solution helps developers to become aware of and resolve conflicts earlier than when they use a mainstream version control system software merging version control continuous integration conflict detection continuous merging,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0.0
253,1,153,"Implementing Continuous Integration Software in an Established Computational Chemistry Software Package Continuous integration is the software engineering principle of rapid and automated development and testing. We identify several key points of continuous integration and demonstrate how they relate to the needs of computational science projects by discussing the implementation and relevance of these principles to AMBER, a large and widely used molecular dynamics software package. The use of a continuous integration server has both improved collaboration and communication between AMBER developers, who are globally distributed, as well as making failure and benchmark information that would be time consuming for individual developers to obtain by themselves, available in real time. Continuous integration servers currently available are aimed at the software engineering community and can be difficult to adapt to the needs of computational science projects, however as demonstrated in this paper the effort payoff can be rapid since uncommon errors are found and contributions from geographically separated researchers are unified into one easily-accessible web-based interface. []",implementing continuous integration software in an established computational chemistry software package continuous integration is the software engineering principle of rapid and automated development and testing we identify several key points of continuous integration and demonstrate how they relate to the needs of computational science projects by discussing the implementation and relevance of these principles to amber a large and widely used molecular dynamics software package the use of a continuous integration server has both improved collaboration and communication between amber developers who are globally distributed as well as making failure and benchmark information that would be time consuming for individual developers to obtain by themselves available in real time continuous integration servers currently available are aimed at the software engineering community and can be difficult to adapt to the needs of computational science projects however as demonstrated in this paper the effort payoff can be rapid since uncommon errors are found and contributions from geographically separated researchers are unified into one easily accessible web based interface,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
254,1,115,"Implementation of continuous integration and automated testing in software development of smart grid scheduling support system When smart grid scheduling support system (D5000 system) was developed, the development team ran across tough issue that the system is difficult for integration and becomes unstable after integration due to the complexity. To resolve the problem, the author made research and introduced continuous integration and automated testing approach on D5000 system development. This paper provides the concept and advantages of continuous integration, and analyzes the necessity for continuous integration. It also describes automated testing for quality improvement with code static analytics, automated unit testing, and automated function testing; This paper gives a case study to deploy continuous integration and automated testing on D5000 system development which resolves quality and integration issues effectively and efficiently. smart grid, continuous integration, automated testing",implementation of continuous integration and automated testing in software development of smart grid scheduling support system when smart grid scheduling support system d5000 system was developed the development team ran across tough issue that the system is difficult for integration and becomes unstable after integration due to the complexity to resolve the problem the author made research and introduced continuous integration and automated testing approach on d5000 system development this paper provides the concept and advantages of continuous integration and analyzes the necessity for continuous integration it also describes automated testing for quality improvement with code static analytics automated unit testing and automated function testing this paper gives a case study to deploy continuous integration and automated testing on d5000 system development which resolves quality and integration issues effectively and efficiently smart grid continuous integration automated testing,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0.0
255,1,144,"Hitting the Target: Practices for Moving Toward Innovation Experiment Systems The benefits and barriers that software development companies face when moving beyond agile development practices are identified in a multiple-case study in five Finnish companies. The practices that companies need to adopt when moving towards innovation experiment systems are recognised. The background of the study is the Stairway to Heaven (StH) model that describes the path that many software development companies take when advancing their development practices. The development practices in each case are investigated and analysed in relation to the StH model. At first the results of the analysis strengthened the validity of the StH model as a path taken by software development companies to advance their development practices. Based on the findings, the StH model was extended with a set of additional practices and their adoption levels for each step of the model. The extended model was validated in five case companies. Software development,  Agile development,  Feedback loops,  Innovation experiment systems,  Continuous deployment",hitting the target practices for moving toward innovation experiment systems the benefits and barriers that software development companies face when moving beyond agile development practices are identified in a multiple case study in five finnish companies the practices that companies need to adopt when moving towards innovation experiment systems are recognised the background of the study is the stairway to heaven sth model that describes the path that many software development companies take when advancing their development practices the development practices in each case are investigated and analysed in relation to the sth model at first the results of the analysis strengthened the validity of the sth model as a path taken by software development companies to advance their development practices based on the findings the sth model was extended with a set of additional practices and their adoption levels for each step of the model the extended model was validated in five case companies software development agile development feedback loops innovation experiment systems continuous deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
256,1,103,"Automated software integration flows in industry: a multiple-case study There is a steadily increasing interest in the agile practice of continuous integration. Consequently, there is great diversity in how it is interpreted and implemented, and a need to study, document and analyze how automated software integration flows are implemented in the industry today. In this paper we study five separate cases, using a descriptive model developed to address the variation points in continuous integration practice discovered in literature. Each case is discussed and evaluated individually, whereupon six guidelines for the design and implementation of automated software integration are presented. Furthermore, the descriptive model used to document the cases is evaluated and evolved. software integration,  Agile software development,  continuous integration,  automation,  methodologies",automated software integration flows in industry a multiple case study there is a steadily increasing interest in the agile practice of continuous integration consequently there is great diversity in how it is interpreted and implemented and a need to study document and analyze how automated software integration flows are implemented in the industry today in this paper we study five separate cases using a descriptive model developed to address the variation points in continuous integration practice discovered in literature each case is discussed and evaluated individually whereupon six guidelines for the design and implementation of automated software integration are presented furthermore the descriptive model used to document the cases is evaluated and evolved software integration agile software development continuous integration automation methodologies,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
257,1,169,"Climbing the ""Stairway to Heaven"" -- A Mulitiple-Case Study Exploring Barriers in the Transition from Agile Development towards Continuous Deployment of Software Agile software development is well-known for its focus on close customer collaboration and customer feedback. In emphasizing flexibility, efficiency and speed, agile practices have lead to a paradigm shift in how software is developed. However, while agile practices have succeeded in involving the customer in the development cycle, there is an urgent need to learn from customer usage of software also after delivering and deployment of the software product. The concept of continuous deployment, i.e. the ability to deliver software functionality frequently to customers and subsequently, the ability to continuously learn from real-time customer usage of software, has become attractive to companies realizing the potential in having even shorter feedback loops. However, the transition towards continuous deployment involves a number of barriers. This paper presents a multiple-case study in which we explore barriers associated with the transition towards continuous deployment. Based on interviews at four different software development companies we present key barriers in this transition as well as actions that need to be taken to address these. agile software development, customer collaboration, continuous integration, continuous deployment",climbing the stairway to heaven a mulitiple case study exploring barriers in the transition from agile development towards continuous deployment of software agile software development is well known for its focus on close customer collaboration and customer feedback in emphasizing flexibility efficiency and speed agile practices have lead to a paradigm shift in how software is developed however while agile practices have succeeded in involving the customer in the development cycle there is an urgent need to learn from customer usage of software also after delivering and deployment of the software product the concept of continuous deployment i e the ability to deliver software functionality frequently to customers and subsequently the ability to continuously learn from real time customer usage of software has become attractive to companies realizing the potential in having even shorter feedback loops however the transition towards continuous deployment involves a number of barriers this paper presents a multiple case study in which we explore barriers associated with the transition towards continuous deployment based on interviews at four different software development companies we present key barriers in this transition as well as actions that need to be taken to address these agile software development customer collaboration continuous integration continuous deployment,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
258,1,169,"The Practice and Future of Release Engineering: A Roundtable with Three Release Engineers Release engineering focuses on building a pipeline that transforms source code into an integrated, compiled, packaged, tested, and signed product that's ready for release. The pipeline's input is the source code developers write to create a product or modify an existing one. Enterprises running large-scale websites and delivering mobile applications with millions of users must rely on a robust release pipeline to ensure they can deliver and update their products to new and existing customers, at the required release cadence. This special issue provides an overview of research and practitioner experience. This article aims to give you insight into the state of the practice and the challenges release engineers face. It features highlights from interviews with Boris Debic, a privacy engineer (and former release engineer); Chuck Rossi, a release-engineering manager; and Kim Moir, a release engineer. We asked each of them the same questions covering topics such as release-engineering metrics, continuous delivery's benefits and limitations, the required job skills, the required changes in education, and recommendations for future research. []",the practice and future of release engineering a roundtable with three release engineers release engineering focuses on building a pipeline that transforms source code into an integrated compiled packaged tested and signed product that s ready for release the pipeline s input is the source code developers write to create a product or modify an existing one enterprises running large scale websites and delivering mobile applications with millions of users must rely on a robust release pipeline to ensure they can deliver and update their products to new and existing customers at the required release cadence this special issue provides an overview of research and practitioner experience this article aims to give you insight into the state of the practice and the challenges release engineers face it features highlights from interviews with boris debic a privacy engineer and former release engineer chuck rossi a release engineering manager and kim moir a release engineer we asked each of them the same questions covering topics such as release engineering metrics continuous delivery s benefits and limitations the required job skills the required changes in education and recommendations for future research,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.0
259,1,87,"Achieving Reliable High-Frequency Releases in Cloud Environments Continuous delivery and deployment are dramatically shortening release cycles from months to hours. Cloud applications with high-frequency releases often rely heavily on automated tools and cloud infrastructure APIs to deploy new software versions. The authors report on reliability issues and how these tools and APIs contribute to them. They also analyze the trade-offs between using heavily baked and lightly baked virtual-image approaches, on the basis of experiments with Amazon Web Service OpsWorks APIs and the Chef configuration management tool. Finally, they propose error-handling practices for continuous-delivery facilities. release engineering, continuous delivery, continuous deployment, DevOps, system administration, software engineering",achieving reliable high frequency releases in cloud environments continuous delivery and deployment are dramatically shortening release cycles from months to hours cloud applications with high frequency releases often rely heavily on automated tools and cloud infrastructure apis to deploy new software versions the authors report on reliability issues and how these tools and apis contribute to them they also analyze the trade offs between using heavily baked and lightly baked virtual image approaches on the basis of experiments with amazon web service opsworks apis and the chef configuration management tool finally they propose error handling practices for continuous delivery facilities release engineering continuous delivery continuous deployment devops system administration software engineering,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
260,1,147,"Toward Design Decisions to Enable Deployability: Empirical Study of Three Projects Reaching for the Continuous Delivery Holy Grail There is growing interest in continuous delivery practices to enable rapid and reliable deployment. While practices are important, we suggest architectural design decisions are equally important for projects to achieve goals such continuous integration (CI) build, automated testing and reduced deployment-cycle time. Architectural design decisions that conflict with deploy ability goals can impede the team's ability to achieve the desired state of deployment and may result in substantial technical debt. To explore this assertion, we interviewed three project teams striving to practicing continuous delivery. In this paper, we summarize examples of the deploy ability goals for each project as well as the architectural decisions that they have made to enable deploy ability. We present the deploy ability goals, design decisions, and deploy ability tactics collected and summarize the design tactics derived from the interviews in the form of an initial draft version hierarchical deploy ability tactic tree. deployability, continuous integration, continuous delivery, architecture tactic, test automation",toward design decisions to enable deployability empirical study of three projects reaching for the continuous delivery holy grail there is growing interest in continuous delivery practices to enable rapid and reliable deployment while practices are important we suggest architectural design decisions are equally important for projects to achieve goals such continuous integration ci build automated testing and reduced deployment cycle time architectural design decisions that conflict with deploy ability goals can impede the team s ability to achieve the desired state of deployment and may result in substantial technical debt to explore this assertion we interviewed three project teams striving to practicing continuous delivery in this paper we summarize examples of the deploy ability goals for each project as well as the architectural decisions that they have made to enable deploy ability we present the deploy ability goals design decisions and deploy ability tactics collected and summarize the design tactics derived from the interviews in the form of an initial draft version hierarchical deploy ability tactic tree deployability continuous integration continuous delivery architecture tactic test automation,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
261,1,296,"On the journey to continuous deployment: Technical and social challenges along the way Context Continuous Deployment (CD) is an emerging software development process with organisations such as Facebook, Microsoft, and IBM successfully implementing and using the process. The CD process aims to immediately deploy software to customers as soon as new code is developed, and can result in a number of benefits for organisations, such as: new business opportunities, reduced risk for each release, and prevent development of wasted software. There is little academic literature on the challenges organisations face when adopting the CD process, however there are many anecdotal challenges that organisations have voiced on their online blogs. Objective The aim of this research is to examine the challenges faced by organisations when adopting CD as well as the strategies to mitigate these challenges. Method An explorative case study technique that involves in-depth interviews with software practitioners in an organisation that has adopted CD was conducted to identify these challenges. Results This study found a total of 20 technical and social adoption challenges that organisations may face when adopting the CD process. The results are discussed to gain a deeper understanding of the strategies employed by organisations to mitigate the impacts of these challenges. Conclusion While a number of individual technical and social adoption challenges were uncovered by the case study in this research, most challenges were not faced in isolation. The severity of these challenges were reduced by a number of mitigation strategies employed by the case study organisation. It is concluded that organisations need to be well prepared to handle technical and social adoption challenges with their existing expertise, processes and tools before adopting the CD process. For practitioners, knowing how to address the challenges an organisation may face when adopting the CD process provides a level of awareness that they previously may not have had. Continuous deployment, Agile software development, Lean software development, Challenges and mitigation strategies",on the journey to continuous deployment technical and social challenges along the way context continuous deployment cd is an emerging software development process with organisations such as facebook microsoft and ibm successfully implementing and using the process the cd process aims to immediately deploy software to customers as soon as new code is developed and can result in a number of benefits for organisations such as new business opportunities reduced risk for each release and prevent development of wasted software there is little academic literature on the challenges organisations face when adopting the cd process however there are many anecdotal challenges that organisations have voiced on their online blogs objective the aim of this research is to examine the challenges faced by organisations when adopting cd as well as the strategies to mitigate these challenges method an explorative case study technique that involves in depth interviews with software practitioners in an organisation that has adopted cd was conducted to identify these challenges results this study found a total of 20 technical and social adoption challenges that organisations may face when adopting the cd process the results are discussed to gain a deeper understanding of the strategies employed by organisations to mitigate the impacts of these challenges conclusion while a number of individual technical and social adoption challenges were uncovered by the case study in this research most challenges were not faced in isolation the severity of these challenges were reduced by a number of mitigation strategies employed by the case study organisation it is concluded that organisations need to be well prepared to handle technical and social adoption challenges with their existing expertise processes and tools before adopting the cd process for practitioners knowing how to address the challenges an organisation may face when adopting the cd process provides a level of awareness that they previously may not have had continuous deployment agile software development lean software development challenges and mitigation strategies,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
262,1,138,"Challenges When Adopting Continuous Integration: A Case Study The complexity of software development has increased over the last few years. Customers today demand higher quality and more stable software with shorter delivery time. Software companies strive to improve their processes in order to meet theses challenges. Agile practices have been widely praised for the focus they put on customer collaboration and shorter feedback loops. Companies that have well established agile practices have been trying to improve their processes further by adopting continuous integration - the concept where teams integrate their code several times a day. However, adopting continuous integration is not a trivial task. This paper presents a case study in which we, based on interviews at a major Swedish telecommunication services and equipment provider, assess the challenges of continuous integration. The study found 23 adoption challenges that organisations may face when adopting the continuous integration process. continuous integration,  software,  challenges",challenges when adopting continuous integration a case study the complexity of software development has increased over the last few years customers today demand higher quality and more stable software with shorter delivery time software companies strive to improve their processes in order to meet theses challenges agile practices have been widely praised for the focus they put on customer collaboration and shorter feedback loops companies that have well established agile practices have been trying to improve their processes further by adopting continuous integration the concept where teams integrate their code several times a day however adopting continuous integration is not a trivial task this paper presents a case study in which we based on interviews at a major swedish telecommunication services and equipment provider assess the challenges of continuous integration the study found 23 adoption challenges that organisations may face when adopting the continuous integration process continuous integration software challenges,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
263,1,130,"The highways and country roads to continuous deployment As part of a Finnish research program, researchers interviewed 15 information and communications technology companies to determine the extent to which the companies adopted continuous deployment. They also aimed to find out why continuous deployment is considered beneficial and what the obstacles are to its full adoption. The benefits mentioned the most often were the ability to get faster feedback, the ability to deploy more often to keep customers satisfied, and improved quality and productivity. Despite understanding the benefits, none of the companies adopted a fully automatic deployment pipeline. The companies also had higher continuous-deployment capability than what they practiced. In many cases, they consciously chose to not aim for full continuous deployment. Obstacles to full adoption included domain-imposed restrictions, resistance to change, customer desires, and developers' skill and confidence. continuous deployment, software development, software engineering, thematic analysis, continuous delivery, continuous integration",the highways and country roads to continuous deployment as part of a finnish research program researchers interviewed 15 information and communications technology companies to determine the extent to which the companies adopted continuous deployment they also aimed to find out why continuous deployment is considered beneficial and what the obstacles are to its full adoption the benefits mentioned the most often were the ability to get faster feedback the ability to deploy more often to keep customers satisfied and improved quality and productivity despite understanding the benefits none of the companies adopted a fully automatic deployment pipeline the companies also had higher continuous deployment capability than what they practiced in many cases they consciously chose to not aim for full continuous deployment obstacles to full adoption included domain imposed restrictions resistance to change customer desires and developers skill and confidence continuous deployment software development software engineering thematic analysis continuous delivery continuous integration,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
264,1,200,"Vroom: Faster Build Processes for Java Build processes are too slow. Because most of the build time for Java projects is spent executing tests, researchers have focused on speeding up testing. They've integrated two complementary approaches into a system that seamlessly supports Ant and Maven JUnit build processes. The first approach, unit test virtualization, isolates in-memory dependencies among test cases, which otherwise are isolated inefficiently by restarting the Java Virtual Machine (JVM) before every test. The system supports just-in-time reinitialization of only the small portion of memory needed by the next test, reusing a single JVM. The implementation of this approach is called VMVM (Virtual Machine in the Virtual Machine, pronounced ""vroom vroom""). In addition, simple setup and tear-down resource management methods designed for sequential execution lead to conflicts when the resources are accessed concurrently. So, the second approach, virtualized unit test virtualization, isolates external dependencies such as files and network ports while long-running tests execute in parallel. For this, the system distributes testing jobs in round-robin manner among OS-level virtual machines. The result is, on average, a 51 percent speedup of application build times. The implementation of this approach is called VMVMVM (Virtual Machine in a Virtual Machine on a Virtual Machine ""vroom vroom vroom""). test execution, testing tools, software engineering",vroom faster build processes for java build processes are too slow because most of the build time for java projects is spent executing tests researchers have focused on speeding up testing they ve integrated two complementary approaches into a system that seamlessly supports ant and maven junit build processes the first approach unit test virtualization isolates in memory dependencies among test cases which otherwise are isolated inefficiently by restarting the java virtual machine jvm before every test the system supports just in time reinitialization of only the small portion of memory needed by the next test reusing a single jvm the implementation of this approach is called vmvm virtual machine in the virtual machine pronounced vroom vroom in addition simple setup and tear down resource management methods designed for sequential execution lead to conflicts when the resources are accessed concurrently so the second approach virtualized unit test virtualization isolates external dependencies such as files and network ports while long running tests execute in parallel for this the system distributes testing jobs in round robin manner among os level virtual machines the result is on average a 51 percent speedup of application build times the implementation of this approach is called vmvmvm virtual machine in a virtual machine on a virtual machine vroom vroom vroom test execution testing tools software engineering,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
265,1,279,"SQA-Mashup: A mashup framework for continuous integration Context Continuous Integration (CI) has become an established best practice of modern software development. Its philosophy of regularly integrating the changes of individual developers with the master code base saves the entire development team from descending into Integration Hell, a term coined in the field of extreme programming. In practice, CI is supported by automated tools to cope with this repeated integration of source code through automated builds and testing. One of the main problems, however, is that relevant information about the quality and health of a software system is both scattered across those tools and across multiple views. Objective This paper introduces a quality awareness framework for CI-data and its conceptional model used for the data integration and visualization. The framework called SQA-Mashup makes use of the service-based mashup paradigm and integrates information from the entire CI-toolchain into a single service. Method The research approach followed in our work consists out of (i) a conceptional model for data integration and visualization, (ii) a prototypical framework implementation based on tool requirements derived from literature, and (iii) a controlled user study to evaluate its usefulness. Results The results of the controlled user study showed that SQA-Mashup's single point of access allows users to answer questions regarding the state of a system more quickly (57%) and accurately (21.6%) than with standalone CI-tools. Conclusions The SQA-Mashup framework can serve as one-stop shop for software quality data monitoring in a software development project. It enables easy access to CI-data which otherwise is not integrated but scattered across multiple CI-tools. Our dynamic visualization approach allows for a tailoring of integrated CI-data according to information needs of different stakeholders such as developers or testers. Continuous integration, Controlled user study, Software quality, Tool integration, Information needs",sqa mashup a mashup framework for continuous integration context continuous integration ci has become an established best practice of modern software development its philosophy of regularly integrating the changes of individual developers with the master code base saves the entire development team from descending into integration hell a term coined in the field of extreme programming in practice ci is supported by automated tools to cope with this repeated integration of source code through automated builds and testing one of the main problems however is that relevant information about the quality and health of a software system is both scattered across those tools and across multiple views objective this paper introduces a quality awareness framework for ci data and its conceptional model used for the data integration and visualization the framework called sqa mashup makes use of the service based mashup paradigm and integrates information from the entire ci toolchain into a single service method the research approach followed in our work consists out of i a conceptional model for data integration and visualization ii a prototypical framework implementation based on tool requirements derived from literature and iii a controlled user study to evaluate its usefulness results the results of the controlled user study showed that sqa mashup s single point of access allows users to answer questions regarding the state of a system more quickly 57 and accurately 21 6 than with standalone ci tools conclusions the sqa mashup framework can serve as one stop shop for software quality data monitoring in a software development project it enables easy access to ci data which otherwise is not integrated but scattered across multiple ci tools our dynamic visualization approach allows for a tailoring of integrated ci data according to information needs of different stakeholders such as developers or testers continuous integration controlled user study software quality tool integration information needs,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
266,1,226,"Introduction of continuous delivery in multi-customer project courses Continuous delivery is a set of practices and principles to release software faster and more frequently. While it helps to bridge the gap between developers and operations for software in production, it can also improve the communication between developers and customers in the development phase, i.e. before software is in production. It shortens the feedback cycle and developers ideally use it right from the beginning of a software development project. In this paper we describe the implementation of a customized continuous delivery workflow and its benefits in a multi-customer project course in summer 2013. Our workflow focuses on the ability to deliver software with only a few clicks to the customer in order to obtain feedback as early as possible. This helps developers to validate their understanding about requirements, which is especially helpful in agile projects where requirements might change often. We describe how we integrated this workflow and the role of the release manager into our project-based organization and how we introduced it using different teaching methods. Within three months 90 students worked in 10 different projects with real customers from industry and delivered 490 releases. After the project course we evaluated our approach in an online questionnaire and in personal interviews. Our findings and observations show that participating students understood and applied the concepts and are convinced about the benefits of continuous delivery. Feedback,  User Involvement,  Release Management,  Version Control System,  Continuous Delivery,  Continuous Integration,  DevOps,  Executable Prototypes",introduction of continuous delivery in multi customer project courses continuous delivery is a set of practices and principles to release software faster and more frequently while it helps to bridge the gap between developers and operations for software in production it can also improve the communication between developers and customers in the development phase i e before software is in production it shortens the feedback cycle and developers ideally use it right from the beginning of a software development project in this paper we describe the implementation of a customized continuous delivery workflow and its benefits in a multi customer project course in summer 2013 our workflow focuses on the ability to deliver software with only a few clicks to the customer in order to obtain feedback as early as possible this helps developers to validate their understanding about requirements which is especially helpful in agile projects where requirements might change often we describe how we integrated this workflow and the role of the release manager into our project based organization and how we introduced it using different teaching methods within three months 90 students worked in 10 different projects with real customers from industry and delivered 490 releases after the project course we evaluated our approach in an online questionnaire and in personal interviews our findings and observations show that participating students understood and applied the concepts and are convinced about the benefits of continuous delivery feedback user involvement release management version control system continuous delivery continuous integration devops executable prototypes,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0
