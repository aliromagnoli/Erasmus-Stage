# -*- coding: utf-8 -*-
"""Methods.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13MaxwXJGiO-BXfKZnUojFKuwgV4bozad

This file contains methods for the "Data Import" file. 
In order to be correctly used, this file needs to be inside a Drive folder with the .py extension.
"""

from nltk.tokenize import regexp_tokenize
import re
import pandas as pd

from imblearn.over_sampling import RandomOverSampler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler


from sklearn.calibration import CalibratedClassifierCV

from sklearn.metrics import accuracy_score, recall_score, precision_score, fbeta_score, confusion_matrix, ConfusionMatrixDisplay, classification_report
import numpy as np

import math

"""# Data Import

##Import of data
"""

def print_value_counts(l, df): 
  """
  Given a list of column names "l" and a dataset "df", 
  it prints the value counts for each specified variable.
  """
  for item in l: 
    print(df[item].value_counts(), "\n")


def rename_columns(names, df): 
  """
  Given a dictionary "names" like {"old_name" : "new_name"} and a dataset "df", 
  it renames the column names specified as "old_name" with new names specified as "new_name".
  """
  for key in names: 
    df.rename(columns={key : names[key]}, inplace=True)

"""## Column preprocessing

### Text preprocessing
"""

def listToString(s): 
  """
  Given a list "s", it returns "s" as a string.
  """
  str1 = "" 
  for ele in s: 
      str1 = str1 + " " + ele
  return str1


def split_on_word(text):
  """
  Given a list or string "text", it uses a regular expression tokenizer 
  (that keeps apostrophes) to return a tokenized list of lists 
  (one list for each sentence: [[word, word], [word, word, ..., word], ...]).
  """
  if type(text) is list:
      return [regexp_tokenize(sentence, pattern="\w+(?:[-']\w+)*") for sentence in text]
  else:
      return regexp_tokenize(text, pattern="\w+(?:[-']\w+)*")


def count_words(df, column):
  """
  Given a dataset "df" and a column name "column", 
  it returns the dataset "df" with a new column containing the number of words in "column".
  """
  df = df.copy()
  col_name = "n_words_in_" + column
  df[col_name] = df[column].apply(lambda x: len(split_on_word(x)))
  return df


def clean_text(df, col):
  """
  Given a dataset "df" and a column name "col", 
  it modifies the column "col", keeping only alpha-numeric characters and 
  replacing all white space with a single space, and then return "df"
  """
  df = df.copy()
  
  #[^A-Za-z0-9]+: regex to match a string of characters that are not a letters or numbers
  return df[col].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', str(x).lower()))\
                .apply(lambda x: re.sub('\s+', ' ', x).strip())

"""### Features preprocessing"""

def from_list_of_values_to_columns(col, df, print=False): 
  """
  Given a column name "col" and a dataset "df", 
  it converts a column containing lists of values to a binary column for each value.
  """
  df = df.copy()

  #obtaining the unique values 

  df[col] = df[col].apply(eval)

  col_dict = {} 
  for i in df[col]: #obtain value_count in a dictionary
    for j in i:
        if j not in col_dict:
            col_dict[j] = 1 #new column
        else:
            col_dict[j] += 1 #update column count

  series = pd.Series([x for _list in df[col] for x in _list]) #reducing its dimensions from 2 to 1 

  if print == True:
    print(series.value_counts()) #display value count

  #creating new binary columns 

  bool_dict = {} #create boolean dict (the binary value for every colum in col_dict and for every row in the df)
  for i, item in enumerate(col_dict.keys()): 
    bool_dict[item] = df[col].apply(lambda x: item in x)

  return pd.DataFrame(bool_dict).astype(int)

"""## Update `col_names`"""

def update_col_names(col_names, col, name_df, sub_features): 
  """
  Given a dataset to update "col_names", a column name "col", 
  a dataset name "name_df" and a list of values "sub_features", 
  it update the dataset "col_names" with "sub_features"
  """

  #useful trasnformation for assigning a list to a dataframe cell
  l = col_names.index[col_names["feature"] == col].tolist()
  col_names.at[l[0], name_df] = sub_features

  return col_names

"""## Topic search"""

def find_documents_about_topic(df, column, new_column, l): 
  """
  Given a dataset "df", a column name "column", a new column name "new_column" and a list of strings "l", 
  it modifies "df" adding a binary column that specify for every row if "column" contains at least 1 of the strings in "l".
  """

  x = df[column][df[column].str.contains('|'.join(l))] #rows in df[column] that contains at least 1 item of "l"
  
  df[new_column] = 0
  df.loc[df.index.isin(x.index), new_column] = 1 #assigning 1 to the corresponding rows of x in df

  print("Number of documents that", new_column, ":", len(x))

  return df

"""# Traditional ML approach"""

def flatten_words(l, get_unique=False):
  """
  Given a list "l" containing strings, 
  it returns the flatten version of the list, 
  maintaining only the unique strings if get_unique=True.
  """
  qa = [s.split() for s in l]
  if get_unique:
      return sorted(list(set([w for sent in qa for w in sent])))
  else:
      return [w for sent in qa for w in sent]

      
def final_ml_preprocessing(train, test, seed, sampling=None): 
  """
  Given a training set "train" and a test set "test", 
  it returns X_train, y_train, X_test, y_test, after applying
  a weighting scheme on the text and scaling the data.
  """
  

  #splitting in X and y for both train and test
  X_train = train.loc[:, train.columns != 'label']
  y_train = train["label"]
  X_test = test.loc[:, test.columns != 'label']
  y_test = test["label"]

  if sampling is not None:
    #oversampling on training set
    ros = RandomOverSampler(sampling_strategy=sampling, random_state=seed)
    #resampling X, y
    X_train, y_train = ros.fit_resample(X_train, y_train)

  #resetting indexes for concat()
  X_train = X_train.reset_index(drop=True) 
  X_test = X_test.reset_index(drop=True)
  y_train = y_train.reset_index(drop=True)
  y_test = y_test.reset_index(drop=True)

  #tf-idf
  all_text = X_train["text_clean"].values.tolist()# + X_test["text_clean"].values.tolist()
  vocab = flatten_words(all_text, get_unique=True)
  tfidf = TfidfVectorizer(stop_words='english', vocabulary=vocab)
  training_matrix = tfidf.fit_transform(X_train["text_clean"])
  test_matrix = tfidf.fit_transform(X_test["text_clean"])

  # print("Training matrix:", training_matrix.shape)
  # print("Test matrix:", test_matrix.shape)

  X_train = pd.concat([X_train, pd.DataFrame(training_matrix.todense())], axis=1) #add training_matrix to X_train
  X_test = pd.concat([X_test, pd.DataFrame(test_matrix.todense())], axis=1)

  #scaling data
  scaler = MinMaxScaler()
  features = list(set(X_train.columns) - set(["text", "text_clean", "Label"])) #all columns except text, text_clean and Label
  X_train = scaler.fit_transform(X_train[features].values)
  y_train = y_train.values
  X_test = scaler.transform(X_test[features].values)
  
  return X_train, y_train, X_test, y_test


def ml_training(pred, model, seed, row, X_train, y_train, X_test, y_test, res):
  """
  Given a dataset "pred" to store the predictions in, a model function "model",
  a int "seed", a dictionary "row" with all the information to update "pred", 
  "X_train", "y_train" and "X_test", it trains the model, computes the predictions 
  on the test set and updates "pred".
  """

 
  if row["model"] == "RF":
    m = model(class_weight="balanced", random_state=seed, n_estimators=50, max_depth=8) #criterion{“gini”, “entropy”, “log_loss”}, default=”gini”
  elif row["model"] == "SVM":
    m = model(class_weight="balanced", random_state=seed) #, C=0.5, max_iter=2000)
    m = CalibratedClassifierCV(m)
  else: 
    m = model(class_weight="balanced", random_state=seed)


  m = m.fit(X_train, y_train)

  #model predictions on the test set

  pred_class_test = m.predict(X_test) #predicted classes
  pred_test = m.predict_proba(X_test) #predicted probabilities
  pred_test = pred_test[:,1]

  pred_row = row.copy()
  pred_row["target"] = y_test
  pred_row["pred"] = pd.DataFrame(pred_test)
  pred = pred.append(pred_row, ignore_index=True) #pred update

  #evaluation metrics

  print("\n", row["model"], "RESULTS:\n")
  res = evalmetrics(res, y_test, pred_class_test, m.classes_, row)

  return pred, res

"""# Evaluation metrics

## Set-based metrics
"""

def evalmetrics(res, y_test, pred_test, labels, new_row): 

  """
  Given a dataset "res" to store the metrics in, a dataset "y_test" with the 
  target labels, a dataset "pred_test" with the predicted labels, a list "labels" 
  of the unique values of the labels and a dictionary "new_row" with all the 
  information to update "res", it returns the dataset "res" updated.
  """

  #accuracy
  accuracy = accuracy_score(y_test, pred_test)
  #precision
  precision = precision_score(y_test, pred_test, average='binary', zero_division=0)
  #recall
  recall = recall_score(y_test, pred_test, average='binary', zero_division=0)
  #f-2 score
  f2 = fbeta_score(y_test, pred_test, average='binary', beta=2, zero_division=0)
  #f-3 score
  f3 = fbeta_score(y_test, pred_test, average='binary', beta=3, zero_division=0)

  #confusion matrix
  cm = confusion_matrix(y_test, pred_test, labels=labels)
  #specificity or true negative rate
  FP = cm.sum(axis=0) - np.diag(cm)  
  FN = cm.sum(axis=1) - np.diag(cm)
  TP = np.diag(cm)
  TN = cm.sum() - (FP + FN + TP)
  TNR = TN/(TN+FP)

  #evaluation metrics
  report = classification_report(y_test, pred_test, output_dict=True, zero_division=0)
  print(pd.DataFrame(report).transpose().round(decimals=3))

  new_row.update({'accuracy' : accuracy*100,
                  'precision' : precision*100,
                  'recall' : recall*100,
                  'true_negative_rate' : TNR[1]*100,
                  'f2_score' : f2*100,
                  'f3_score' : f3*100})
  res = res.append(new_row, ignore_index=True)

  return res


def compute_avg_std(res): 
  """
  Given a dataset "res" that stores the metrics, it returns the avg and std for 
  every metics stored in "res".
  """

  
  avg_res = pd.DataFrame() 

  for k in res["df"].unique(): 
    for j in res["model"].unique(): 
      condition = (res["df"] == k) & (res["model"] == j)

      new_row = {"df": k, "model":j}
      avg_res = avg_res.append(new_row, ignore_index=True)
      new_condition = (avg_res["df"] == k) & (avg_res["model"] == j)

      for column in res.columns[res.columns.get_loc("accuracy"):]: #every column from accuracy column

        avg_column = column + "_avg"
        std_column = column + "_std"
        
        avg_res.loc[new_condition, avg_column] = round(res.loc[condition, column].mean(), 2)
        avg_res.loc[new_condition, std_column] = round(res.loc[condition, column].std(), 2)

  return avg_res

"""## Rank-based metrics"""

def sigmoid(x):
  return 1 / (1 + math.exp(-x))


def arrange_predictions_and_targets(pred, j): 

  #create dataframe that contain predictions and targets
  temp = pd.DataFrame()
  temp["prediction_value"] = pred["pred"][j]
  temp["target"] = pred["target"].values.tolist()[j]

  #sort in descending order respect to the predictions
  temp.sort_values(by=['prediction_value'], inplace=True, ascending=False, ignore_index=True) 

  #compute the sigmoid function over the predictions (in order to obtain values in [0, 1])
  for index, value in enumerate(temp["prediction_value"]):
    temp.loc[index, "sigmoid_value"] = sigmoid(value)

  #convert predictions from real value to 0 or 1
  temp.loc[temp["sigmoid_value"] >= 0.5, "prediction"] = 1
  temp.loc[temp["sigmoid_value"] < 0.5, "prediction"] = 0
  temp.drop(['prediction_value', "sigmoid_value"], inplace=True, axis=1)

  #cast dataframe type to int
  temp = temp.astype(int)

  return temp

def getTpTnFpFn(df): 

  TP = len(df.loc[df[["target", "prediction"]].eq(1).all(1)])
  TN = len(df.loc[df[["target", "prediction"]].eq(0).all(1)])
  FP = len(df.loc[(df["target"] == 0) & (df["prediction"] == 1)]) 
  FN = df.shape[0] - (TP + TN + FP)

  return TP, TN, FP, FN

def get_rank_at_k(df, total_relevant_docs, K=95): 
  
  relevant_docs = 0
  
  for j in range(len(df)): 
    
    if df.loc[j, "target"] == 1: 
      relevant_docs = relevant_docs + 1
    
    recall_k = relevant_docs / total_relevant_docs
    
    if recall_k >= K/100: 
      return j

  print("Error")
  return 0

def update_results(pred_df, df, model, avg_res, K=95): 

  #compute TP, TN, FP, FN
  TP, TN, FP, FN = getTpTnFpFn(pred_df)
  retrieved_docs = TP + FP
  relevant_docs = TP + FN
  not_relevant_docs = TN + FP

  #EVALUATION MEASURES AT 95% RECALL

  #selection of the rows where recall@95
  last_positive_rank = get_rank_at_k(pred_df, relevant_docs) 
  pred_df_95 = pred_df[:last_positive_rank+1]
  pred_df_5 = pred_df[last_positive_rank+1:]
  TP_95, TN_95, FP_95, FN_95 = getTpTnFpFn(pred_df_95)
  TP_5, TN_5, FP_5, FN_5 = getTpTnFpFn(pred_df_5)

  #TRUE NEGATIVE RATE AT 95% RECALL
  if not_relevant_docs==0: 
    print("Number of not relevant docs equals to 0")
    TNR_k = 0
  else:
    TNR_k = TN_5/not_relevant_docs

  #PRECISION@95
  if retrieved_docs==0: 
    print("Number of retrieved docs equals to 0")
    precision_k = 0
  else:
    precision_k = TP_95/retrieved_docs 

  #WSS@95
  N = len(pred_df)
  WSS_k = ((N - last_positive_rank)/N) - ((100-K)/100)

  #update results dataset
  condition = (avg_res["df"] == df) & (avg_res["model"] == model)
  avg_res.loc[condition, "true_negative_rate@95"] = round(TNR_k*100, 2)
  avg_res.loc[condition, "precision@95"] = round(precision_k*100, 2)
  avg_res.loc[condition, "wss@95"] = round(WSS_k*100, 2)

  # print("N:", N)
  # print("Last positive rank:", last_positive_rank)
  # print()

  return avg_res